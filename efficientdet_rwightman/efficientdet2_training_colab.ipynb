{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"efficientdet2_training_colab.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","authorship_tag":"ABX9TyNqlBL0CSGs1Zmoe7sZG4N+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AiQPodddRTaD","colab_type":"code","colab":{}},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yf22UTuLusak","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVVpPWVd1FNA","colab_type":"code","colab":{}},"source":["gwd_name = 'global-wheat-detection.zip'\n","gwd_path = '/content/drive/My Drive/Colab Notebooks/' + gwd_name\n","!cp '{gwd_path}' .\n","!unzip -q '{gwd_name}'\n","!rm '{gwd_name}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"978lsjD2vQAF","colab_type":"code","colab":{}},"source":["effdet_name = 'efficientdet-pytorch-b0473e5.zip'\n","effdet_path = '/content/drive/My Drive/Colab Notebooks/' + effdet_name\n","!cp '{effdet_path}' .\n","!unzip -q '{effdet_name}'\n","!rm '{effdet_name}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKf0HvPfJnM1","colab_type":"code","colab":{}},"source":["weight_name = 'tf_efficientdet_d5-ef44aea8.pth'\n","weight_path = '/content/drive/My Drive/Colab Notebooks/efficientdetweights/' + weight_name\n","!cp '{weight_path}' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sH0SqqVYcP1","colab_type":"code","colab":{}},"source":["resume_name = 'last-checkpoint.bin'\n","resume_path = '/content/drive/My Drive/Colab Notebooks/efficientdetweights/cutout(s64n64)_1024_fold0/' + resume_name\n","!cp '{resume_path}' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlvKV6g3Kcdn","colab_type":"code","colab":{}},"source":["!pip install --no-deps numpy==1.17.5\n","!pip install --no-deps timm\n","!pip install --no-deps albumentations==0.4.6\n","!pip install --no-deps omegaconf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SDekTiLTg1WA","colab_type":"code","colab":{}},"source":["%%writefile setup.sh\n","\n","export CUDA_HOME=/usr/local/cuda-10.1\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPuaO9L9g2GF","colab_type":"code","colab":{}},"source":["!sh setup.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoBHduIadAI7","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.insert(0, '/content/efficientdet-pytorch-master')\n","import torch\n","import os\n","from datetime import datetime\n","import time\n","import random\n","import cv2\n","import pandas as pd\n","import numpy as np\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","import apex.amp as amp\n","from albumentations.pytorch.transforms import ToTensorV2\n","from sklearn.model_selection import StratifiedKFold\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from glob import glob\n","\n","SEED = 42\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rk6cmtd8uSSc","colab_type":"code","colab":{}},"source":["# Global Constant\n","CURR_DIR = '/content/'\n","# Global Parameters \n","img_size_global = 1024\n","epoch_global = 50\n","batch_global = 2\n","folder_global = 'cutout(s64n64)_1024_fold0-2'\n","resume_global = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooPWtFNg1FYk","colab_type":"code","colab":{}},"source":["marking = pd.read_csv('/content/train.csv')\n","\n","bboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n","for i, column in enumerate(['x', 'y', 'w', 'h']):\n","    marking[column] = bboxs[:,i]\n","marking.drop(columns=['bbox'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QRuzNXcTIju","colab_type":"code","colab":{}},"source":["\"\"\"\n","wh_min = 10\n","wh_max = 512\n","\n","marking.drop(labels=marking[marking['w'] < wh_min].index, axis=0, inplace=True)\n","marking.drop(labels=marking[marking['h'] < wh_min].index, axis=0, inplace=True)\n","marking.drop(labels=marking[marking['w'] > wh_max].index, axis=0, inplace=True)\n","marking.drop(labels=marking[marking['h'] > wh_max].index, axis=0, inplace=True)\n","\"\"\";"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nf5SRRzXG28q","colab_type":"code","colab":{}},"source":["skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","df_folds = marking[['image_id']].copy()\n","df_folds.loc[:, 'bbox_count'] = 1\n","df_folds = df_folds.groupby('image_id').count()\n","df_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\n","df_folds.loc[:, 'stratify_group'] = np.char.add(\n","    df_folds['source'].values.astype(str),\n","    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n",")\n","df_folds.loc[:, 'fold'] = 0\n","\n","for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n","    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhDBKu06HY7m","colab_type":"code","colab":{}},"source":["class TrainGlobalConfig:\n","    num_workers = 4\n","    batch_size = batch_global\n","    n_epochs = epoch_global\n","    lr = 0.0002\n","    mixed_precision = True\n","    accumulate = 16\n","\n","    resume = resume_global\n","    resume_path = '/content/last-checkpoint.bin'\n","\n","    folder = folder_global\n","\n","    # -------------------\n","    verbose = True\n","    verbose_step = 1\n","    # -------------------\n","\n","    # --------------------\n","    step_scheduler = False  # do scheduler.step after optimizer.step\n","    validation_scheduler = True  # do scheduler.step after validation stage loss\n","\n","#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n","#     scheduler_params = dict(\n","#         max_lr=0.001,\n","#         epochs=n_epochs,\n","#         steps_per_epoch=int(len(train_dataset) / batch_size),\n","#         pct_start=0.1,\n","#         anneal_strategy='cos', \n","#         final_div_factor=10**5\n","#     )\n","    \n","    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n","    scheduler_params = dict(\n","        mode='min',\n","        factor=0.5,\n","        patience=1,\n","        verbose=True, \n","        threshold=0.0001,\n","        threshold_mode='abs',\n","        cooldown=0, \n","        min_lr=1e-8,\n","        eps=1e-08\n","    )\n","    # --------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YvX1nmqG3Gf","colab_type":"code","colab":{}},"source":["def get_train_transforms():\n","    return A.Compose(\n","        [\n","            A.OneOf([\n","                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n","                                     val_shift_limit=0.2, p=0.9),\n","                A.RandomBrightnessContrast(brightness_limit=0.2, \n","                                           contrast_limit=0.2, p=0.9),\n","            ],p=0.9),\n","            A.ToGray(p=0.01),\n","            A.Cutout(num_holes=64, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n","            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n","            A.RandomRotate90(p=1.0),\n","            A.HorizontalFlip(p=0.5),\n","            A.Resize(height=img_size_global, width=img_size_global, p=1.0),\n","            ToTensorV2(p=1.0),\n","        ], \n","        p=1.0, \n","        bbox_params=A.BboxParams(\n","            format='pascal_voc',\n","            min_area=0, \n","            min_visibility=0,\n","            label_fields=['labels']\n","        )\n","    )\n","\n","def get_valid_transforms():\n","    return A.Compose(\n","        [\n","            A.Resize(height=img_size_global, width=img_size_global, p=1.0),\n","            ToTensorV2(p=1.0),\n","        ], \n","        p=1.0, \n","        bbox_params=A.BboxParams(\n","            format='pascal_voc',\n","            min_area=0, \n","            min_visibility=0,\n","            label_fields=['labels']\n","        )\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GE__dAlG3EQ","colab_type":"code","colab":{}},"source":["TRAIN_ROOT_PATH = '/content/train'\n","\n","class DatasetRetriever(Dataset):\n","\n","    def __init__(self, marking, image_ids, transforms=None, test=False):\n","        super().__init__()\n","\n","        self.image_ids = image_ids\n","        self.marking = marking\n","        self.transforms = transforms\n","        self.test = test\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.image_ids[index]\n","        \n","        # (1) Apply cutmix augmentation\n","        if self.test or random.random() > 0.5:\n","            image, boxes = self.load_image_and_boxes(index)\n","        else:\n","            image, boxes = self.load_cutmix_image_and_boxes(index)\n","        \n","        # (2) Apply albumentation\n","        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = torch.tensor([index])\n","\n","        if self.transforms:\n","            for i in range(10):\n","                sample = self.transforms(**{\n","                    'image': image,\n","                    'bboxes': target['boxes'],\n","                    'labels': labels\n","                })\n","                if len(sample['bboxes']) > 0:\n","                    image = sample['image']\n","                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n","                    break\n","\n","        # (3) Apply random erasing augmentation\n","        #if not self.test and random.random() <= 0.5:\n","            #image = self._bbox_erasing(image, target['boxes'])  \n","\n","        return image, target, image_id\n","\n","    def __len__(self) -> int:\n","        return self.image_ids.shape[0]\n","\n","    def load_image_and_boxes(self, index):\n","        image_id = self.image_ids[index]\n","        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        records = self.marking[self.marking['image_id'] == image_id]\n","        boxes = records[['x', 'y', 'w', 'h']].values\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","        return image, boxes\n","\n","    def load_cutmix_image_and_boxes(self, index, imsize=1024):\n","        \"\"\" \n","        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n","        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n","        \"\"\"\n","        w, h = imsize, imsize\n","        s = imsize // 2\n","    \n","        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n","        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n","\n","        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n","        result_boxes = []\n","\n","        for i, index in enumerate(indexes):\n","            image, boxes = self.load_image_and_boxes(index)\n","            if i == 0:\n","                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n","                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n","            elif i == 1:  # top right\n","                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n","                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n","            elif i == 2:  # bottom left\n","                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n","                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n","            elif i == 3:  # bottom right\n","                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n","                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n","            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n","            padw = x1a - x1b\n","            padh = y1a - y1b\n","\n","            boxes[:, 0] += padw\n","            boxes[:, 1] += padh\n","            boxes[:, 2] += padw\n","            boxes[:, 3] += padh\n","\n","            result_boxes.append(boxes)\n","\n","        result_boxes = np.concatenate(result_boxes, 0)\n","        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n","        result_boxes = result_boxes.astype(np.int32)\n","        result_boxes = result_boxes[np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)]\n","        return result_image, result_boxes\n","\n","    def _bbox_erasing(self, image, boxes, p=0.5, sl=0.02, sh=0.2, r1=0.3, r2=1/0.3):\n","        for box in boxes:\n","            if random.random() > p:\n","                continue\n","            W, H = box[3]-box[1], box[2]-box[0]\n","            S = W*H\n","            while True:\n","                Se, re = random.uniform(sl,sh)*S, random.uniform(r1,r2)\n","                We, He = int(np.trunc(np.sqrt(Se/re))), int(np.trunc(np.sqrt(Se*re)))\n","                if We > W or He > H:\n","                    continue\n","                xe, ye = random.randrange(int(W-We+1)), random.randrange(int(H-He+1))\n","                xe, ye = int(xe + box[1]), int(ye + box[0])\n","                image[:, ye:ye+He, xe:xe+We] = torch.rand(3, He, We) # x: col, y: row\n","                break\n","        return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-UTVx7ZG3Ar","colab_type":"code","colab":{}},"source":["fold_number = 0\n","\n","train_dataset = DatasetRetriever(\n","    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n","    marking=marking,\n","    transforms=get_train_transforms(),\n","    test=False,\n",")\n","\n","validation_dataset = DatasetRetriever(\n","    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n","    marking=marking,\n","    transforms=get_valid_transforms(),\n","    test=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBU6IphcHPLC","colab_type":"code","colab":{}},"source":["image, target, image_id = train_dataset[1]\n","boxes = target['boxes'].cpu().numpy().astype(np.int32)\n","\n","numpy_image = image.permute(1,2,0).cpu().numpy()\n","\n","fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","\n","for box in boxes:\n","    cv2.rectangle(numpy_image, (box[1], box[0]), (box[3],  box[2]), (1, 0, 0), 2)\n","\n","ax.set_axis_off()\n","ax.imshow(numpy_image);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrO_33uZHYy2","colab_type":"code","colab":{}},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2526oxAiHY_E","colab_type":"code","colab":{}},"source":["import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","class Fitter:\n","    \n","    def __init__(self, model, device, config):\n","        self.model = model\n","        self.device = device\n","        self.config = config\n","\n","        self.base_dir = f'./{config.folder}'\n","        if not os.path.exists(self.base_dir):\n","            os.makedirs(self.base_dir)\n","        self.log_path = f'{self.base_dir}/log.txt'\n","\n","        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n","        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n","        self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level='O1', verbosity=0)\n","\n","        if self.config.resume:\n","            self.load(self.config.resume_path)\n","        else:\n","            self.epoch = 0\n","            self.best_summary_loss = 10**5\n","            \"\"\"\n","            param_optimizer = list(self.model.named_parameters())\n","            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","            optimizer_grouped_parameters = [\n","                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","            ]\n","            \"\"\"\n","            \n","        self.log(f'Fitter prepared. Device is {self.device}')\n","\n","    def fit(self, train_loader, validation_loader):\n","        epoch_range = range(self.epoch, self.config.n_epochs)\n","        for e in epoch_range:\n","            if self.config.verbose:\n","                lr = self.optimizer.param_groups[0]['lr']\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR: {lr}')\n","\n","            t = time.time()\n","            summary_loss = self.train_one_epoch(train_loader)\n","            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n","            self.save(f'{self.base_dir}/last-checkpoint.bin')\n","\n","            t = time.time()\n","            summary_loss = self.validation(validation_loader)\n","            self.log(f'[RESULT]: Valid. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n","\n","            if summary_loss.avg < self.best_summary_loss:\n","                self.best_summary_loss = summary_loss.avg\n","                self.model.eval()\n","                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n","                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-1]:\n","                    os.remove(path)\n","\n","            if self.config.validation_scheduler:\n","                self.scheduler.step(metrics=summary_loss.avg)\n","\n","            self.epoch += 1\n","\n","    def validation(self, val_loader):\n","        self.model.eval()\n","        summary_loss = AverageMeter()\n","        t = time.time()\n","        for step, (images, targets, image_ids) in enumerate(val_loader):\n","            \"\"\"\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    print(\n","                        f'Val Step {step}/{len(val_loader)}, ' + \\\n","                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}', end='\\r'\n","                    )\n","            \"\"\"\n","            with torch.no_grad():\n","                images = torch.stack(images)\n","                batch_size = images.shape[0]\n","                images = images.to(self.device).float()\n","                boxes = [target['boxes'].to(self.device).float() for target in targets]\n","                labels = [target['labels'].to(self.device).float() for target in targets]\n","                img_scales = torch.ones([batch_size,]).to(self.device).float()\n","                img_sizes = torch.stack((torch.ones([batch_size,])*images.shape[2],\n","                                         torch.ones([batch_size,])*images.shape[3])).permute(1,0)\n","                img_sizes = img_sizes.to(self.device).int()\n","\n","                #loss, _, _ = self.model(images, boxes, labels)\n","                outputs = self.model(images, {'bbox':boxes,\n","                                              'cls':labels,\n","                                              'img_scale':img_scales,\n","                                              'img_size':img_sizes})\n","                loss = outputs['loss']\n","                summary_loss.update(loss.detach().item(), batch_size)\n","\n","        return summary_loss\n","\n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        summary_loss = AverageMeter()\n","        #t = time.time()\n","        for step, (images, targets, image_ids) in enumerate(train_loader):\n","            \"\"\"\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    print(\n","                        f'Train Step {step}/{len(train_loader)}, ' + \\\n","                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}', end='\\r'\n","                    )\n","            \"\"\"\n","            images = torch.stack(images)\n","            images = images.to(self.device).float()\n","            batch_size = images.shape[0]\n","            boxes = [target['boxes'].to(self.device).float() for target in targets]\n","            labels = [target['labels'].to(self.device).float() for target in targets]\n","            \n","            self.optimizer.zero_grad()\n","\n","            outputs = self.model(images, {'bbox':boxes, 'cls':labels})\n","            loss = outputs['loss']\n","\n","            #loss, _, _ = self.model(images, boxes, labels)\n","            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","\n","            summary_loss.update(loss.detach().item(), batch_size)\n","            self.optimizer.step()\n","\n","            \"\"\"\n","            if (step+1) % self.config.accumulate == 0:\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","            \"\"\"\n","\n","            if self.config.step_scheduler:\n","                self.scheduler.step()\n","\n","        return summary_loss\n","    \n","    def save(self, path):\n","        self.model.eval()\n","        torch.save({\n","            'model_state_dict': self.model.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'scheduler_state_dict': self.scheduler.state_dict(),\n","            'amp_state_dict': amp.state_dict(),\n","            'best_summary_loss': self.best_summary_loss,\n","            'epoch': self.epoch,\n","        }, path)\n","\n","    def load(self, path):\n","        checkpoint = torch.load(path)\n","        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        amp.load_state_dict(checkpoint['amp_state_dict'])\n","        self.best_summary_loss = checkpoint['best_summary_loss']\n","        self.epoch = checkpoint['epoch'] + 1\n","        \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)\n","        with open(self.log_path, 'a+') as logger:\n","            logger.write(f'{message}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nl2X_XjuHY59","colab_type":"code","colab":{}},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","def run_training():\n","    device = torch.device('cuda:0')\n","    net.to(device)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=TrainGlobalConfig.batch_size,\n","        sampler=RandomSampler(train_dataset),\n","        pin_memory=False,\n","        drop_last=True,\n","        num_workers=TrainGlobalConfig.num_workers,\n","        collate_fn=collate_fn,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        validation_dataset, \n","        batch_size=TrainGlobalConfig.batch_size,\n","        num_workers=TrainGlobalConfig.num_workers,\n","        shuffle=False,\n","        sampler=SequentialSampler(validation_dataset),\n","        pin_memory=False,\n","        collate_fn=collate_fn,\n","    )\n","\n","    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n","    fitter.fit(train_loader, val_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfRlFEObHY2X","colab_type":"code","colab":{}},"source":["from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n","from effdet.efficientdet import HeadNet\n","def get_net():\n","    config = get_efficientdet_config('tf_efficientdet_d5')\n","    net = EfficientDet(config, pretrained_backbone=False)\n","    checkpoint = torch.load('/content/tf_efficientdet_d5-ef44aea8.pth')\n","    net.load_state_dict(checkpoint)\n","    config.num_classes = 1\n","    config.image_size = img_size_global\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n","    return DetBenchTrain(net, config)\n","\n","net = get_net()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hy9iBDj_IllV","colab_type":"code","colab":{}},"source":["run_training()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXFQcoG0kWE7","colab_type":"code","colab":{}},"source":["import shutil\n","weight_org = '/content/drive/My Drive/Colab Notebooks/efficientdetweights/'\n","new_weight = weight_org + TrainGlobalConfig.folder\n","if os.path.exists(new_weight):\n","    shutil.rmtree(new_weight)\n","curr_weight = '/content/' + TrainGlobalConfig.folder\n","!cp -r '{curr_weight}' '{weight_org}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtigXSkIl15i","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}