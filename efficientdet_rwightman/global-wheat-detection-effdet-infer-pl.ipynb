{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Forked from belows:\n",
    "* [[Inference] EfficientDet](https://www.kaggle.com/shonenkov/inference-efficientdet)\n",
    "* [[WBF over TTA][Single Model] EfficientDet](https://www.kaggle.com/shonenkov/bayesian-optimization-wbf-efficientdet)\n",
    "\n",
    "## Licenses\n",
    "| Repository | License |\n",
    "| :--- | :--- |\n",
    "| efficientdet-pytorch | Apache License 2.0 |\n",
    "| weighted-boxes-fusion | MIT License |\n",
    "| timm | Apache License 2.0 |\n",
    "| omegaconf | BSD 3-Clause \"New\" or \"Revised\" License |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps '../input/packageefficientnet/timm-0.1.30-py3-none-any.whl' > /dev/null\n",
    "!pip install --no-deps '../input/packageefficientnet/pycocotools-2.0.1-cp37-cp37m-linux_x86_64.whl' > /dev/null\n",
    "!pip install --no-deps '../input/packageefficientnet/omegaconf-2.0.0-py3-none-any.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import apex.amp as amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../input/efficientdetrwightman')\n",
    "sys.path.insert(0, '../input/weightedboxesfusion')\n",
    "\n",
    "from ensemble_boxes import *\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchPredict\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "IMG_SIZE_ORG = 1024\n",
    "img_size_global = 1024\n",
    "batch_global = 1\n",
    "epoch_global = 5\n",
    "folder_global = 'weights'\n",
    "weight_global = '../input/efficientdetweights/1024-fold0-50.bin'\n",
    "test_test = True"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=img_size_global, width=img_size_global, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_test:\n",
    "    DATA_ROOT_PATH = '../input/global-wheat-detection/test'\n",
    "else:\n",
    "    DATA_ROOT_PATH = '../input/global-wheat-detection/train'\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "        return image, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetRetriever(\n",
    "    image_ids=np.array([path.split('/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n",
    "    transforms=get_valid_transforms()\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size=img_size_global\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchPredict(net, config)\n",
    "    net.eval();\n",
    "    return net.cuda()\n",
    "\n",
    "net = load_net(weight_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare test time augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWheatTTA:\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    image_size = img_size_global\n",
    "\n",
    "    def augment(self, image):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TTAHorizontalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return image.flip(1)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(2)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores):\n",
    "        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n",
    "        return boxes, scores\n",
    "\n",
    "class TTAVerticalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return image.flip(2)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(3)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores):\n",
    "        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n",
    "        return boxes, scores\n",
    "    \n",
    "class TTARotate90(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return torch.rot90(image, 1, (1, 2))\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return torch.rot90(images, 1, (2, 3))\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores):\n",
    "        res_boxes = boxes.copy()\n",
    "        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n",
    "        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n",
    "        return res_boxes, scores\n",
    "    \n",
    "class TTAColorFlip(BaseWheatTTA):\n",
    "    def augment(self, image):\n",
    "        return image.flip(0)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(1)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores):\n",
    "        return boxes, scores\n",
    "    \n",
    "class TTACrop(BaseWheatTTA):\n",
    "    def __init__(self, size, loc, boundary=0.5):\n",
    "        self.size = size\n",
    "        self.pad = (img_size_global - self.size) // 2\n",
    "        self.size = img_size_global - (2*self.pad)\n",
    "        self.boundary = boundary\n",
    "        self.loc = loc\n",
    "    \n",
    "    def augment(self, image):\n",
    "        if self.loc == 0: # center\n",
    "            image = image[:, self.pad:self.pad+self.size, self.pad:self.pad+self.size]\n",
    "        elif self.loc == 1: # top-left\n",
    "            image = image[:, 0:self.size, 0:self.size]\n",
    "        elif self.loc == 2: # top-right\n",
    "            image = image[:, 0:self.size, (self.pad*2):(self.pad*2+self.size)]\n",
    "        elif self.loc == 3: # bottom-left\n",
    "            image = image[:, (self.pad*2):(self.pad*2+self.size), 0:self.size]\n",
    "        else: # bottom-right\n",
    "            image = image[:, (self.pad*2):(self.pad*2+self.size), (self.pad*2):(self.pad*2+self.size)]\n",
    "        \n",
    "        image = cv2.resize(image.permute(1,2,0).cpu().numpy(), (img_size_global,img_size_global), interpolation=cv2.INTER_LINEAR)\n",
    "        return torch.from_numpy(image).permute(2,0,1)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        images = torch.stack([self.augment(image) for image in images])\n",
    "        return images.cuda()\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores):\n",
    "        if scores is None:\n",
    "            scores = np.random.rand(boxes.shape[0])\n",
    "        \n",
    "        box_score = np.concatenate((boxes, np.expand_dims(scores, axis=1)), axis=1)\n",
    "        if self.loc == 0 or self.loc == 2 or self.loc == 4: # min x1\n",
    "            box_score = box_score[np.min(box_score[:,0:1], axis=1) > 0+self.boundary]\n",
    "        if self.loc == 0 or self.loc == 3 or self.loc == 4: # min y1\n",
    "            box_score = box_score[np.min(box_score[:,1:2], axis=1) > 0+self.boundary]\n",
    "        if self.loc == 0 or self.loc == 1 or self.loc == 3: # max x2\n",
    "            box_score = box_score[np.max(box_score[:,2:3], axis=1) < img_size_global-1-self.boundary]\n",
    "        if self.loc == 0 or self.loc == 1 or self.loc == 2: # max y2\n",
    "            box_score = box_score[np.max(box_score[:,3:4], axis=1) < img_size_global-1-self.boundary]\n",
    "        \n",
    "        #box_score = box_score[np.min(box_score[:,0:4], axis=1) > 0+self.boundary]\n",
    "        #box_score = box_score[np.max(box_score[:,0:4], axis=1) < img_size_global-1-self.boundary]\n",
    "        \n",
    "        boxes, scores = box_score[:,0:4], box_score[:,4]\n",
    "        boxes = (boxes*(self.size/img_size_global))# + self.pad\n",
    "        \n",
    "        if self.loc == 0:\n",
    "            boxes += self.pad\n",
    "        else:\n",
    "            if self.loc == 2 or self.loc == 4:\n",
    "                boxes[:,0] += (2*self.pad)\n",
    "                boxes[:,2] += (2*self.pad)\n",
    "            if self.loc == 3 or self.loc == 4:\n",
    "                boxes[:,1] += (2*self.pad)\n",
    "                boxes[:,3] += (2*self.pad)\n",
    "        return boxes, scores\n",
    "    \n",
    "class TTACenterReduce(BaseWheatTTA):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.pad = (img_size_global - self.size) // 2\n",
    "        self.size = img_size_global - (2*self.pad)\n",
    "        #self.boundary = 0.5\n",
    "    \n",
    "    def augment(self, image):\n",
    "        image_out = torch.zeros(image.shape)\n",
    "        image = cv2.resize(image.permute(1,2,0).cpu().numpy(), (self.size,self.size), interpolation=cv2.INTER_LINEAR)\n",
    "        image = torch.from_numpy(image).permute(2,0,1)        \n",
    "        image_out[:, self.pad:self.pad+self.size, self.pad:self.pad+self.size] = image\n",
    "        return image_out\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        images = torch.stack([self.augment(image) for image in images])\n",
    "        return images.cuda()\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores):\n",
    "        if scores is None:\n",
    "            scores = np.random.rand(boxes.shape[0])\n",
    "        boxes = ((boxes - self.pad)*img_size_global/self.size).clip(min=0, max=img_size_global)\n",
    "\n",
    "        box_score = np.concatenate((boxes, np.expand_dims(scores, axis=1)), axis=1)\n",
    "        box_score = box_score[np.min(box_score[:,2:4]-box_score[:,0:2], axis=1) > 0]\n",
    "        box_score = box_score[np.max(box_score[:,2:4]-box_score[:,0:2], axis=1) <= img_size_global]\n",
    "        \n",
    "        boxes, scores = box_score[:,0:4], box_score[:,4]        \n",
    "        return boxes, scores\n",
    "\n",
    "class TTACompose(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def augment(self, image):\n",
    "        for transform in self.transforms:\n",
    "            image = transform.augment(image)\n",
    "        return image\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        for transform in self.transforms:\n",
    "            images = transform.batch_augment(images)\n",
    "        return images\n",
    "    \n",
    "    def prepare_boxes(self, boxes):\n",
    "        result_boxes = boxes.copy()\n",
    "        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n",
    "        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n",
    "        return result_boxes\n",
    "    \n",
    "    def deaugment_boxes(self, boxes, scores=None):\n",
    "        for transform in self.transforms[::-1]:\n",
    "            boxes, scores = transform.deaugment_boxes(boxes, scores)\n",
    "        return self.prepare_boxes(boxes), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_det(index, det, score_threshold=0.25):\n",
    "    boxes = det[index].detach().cpu().numpy()[:,:4]    \n",
    "    scores = det[index].detach().cpu().numpy()[:,4]\n",
    "    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "    boxes = (boxes).clip(min=0, max=img_size_global).astype(int)\n",
    "    indexes = np.where(scores>score_threshold)\n",
    "    boxes = boxes[indexes]\n",
    "    scores = scores[indexes]\n",
    "\n",
    "    return boxes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test codes for TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transform = TTACompose([\n",
    "    #TTARotate90(),\n",
    "    #TTAVerticalFlip(),\n",
    "    #TTAResize(),\n",
    "])\n",
    "transform2 = TTACompose([\n",
    "    #TTAHorizontalFlip(),\n",
    "    #TTAVerticalFlip(),\n",
    "    #TTARotate90(),\n",
    "    TTACrop(size=800, loc=4, boundary=0.5),\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 8))\n",
    "\n",
    "# original\n",
    "image, image_id = dataset[1]\n",
    "numpy_image = image.permute(1,2,0).cpu().numpy().copy()\n",
    "ax[0].imshow(numpy_image);\n",
    "ax[0].set_title('original')\n",
    "\n",
    "# tta\n",
    "tta_image = transform.augment(image)\n",
    "print(tta_image.shape)\n",
    "tta_image_numpy = tta_image.permute(1,2,0).cpu().numpy().copy()\n",
    "tta_image_size = torch.tensor([[tta_image.shape[1], tta_image.shape[2]]])\n",
    "\n",
    "det = net(tta_image.unsqueeze(0).float().cuda(), torch.tensor([1]).float().cuda(), img_size=tta_image_size.float().cuda())\n",
    "boxes, scores = process_det(0, det)\n",
    "del det\n",
    "gc.collect()\n",
    "for box in boxes:\n",
    "    cv2.rectangle(tta_image_numpy, (box[0].astype(int), box[1].astype(int)),\n",
    "                                   (box[2].astype(int)-1, box[3].astype(int)-1), (1, 0, 0), 2)\n",
    "ax[1].imshow(tta_image_numpy);\n",
    "ax[1].set_title('original with bbox')\n",
    "\n",
    "# tta2\n",
    "tta_image = transform2.augment(image)\n",
    "print(tta_image.shape)\n",
    "tta_image_numpy = tta_image.permute(1,2,0).cpu().numpy().copy()\n",
    "tta_image_size = torch.tensor([[tta_image.shape[1], tta_image.shape[2]]])\n",
    "\n",
    "det = net(tta_image.unsqueeze(0).float().cuda(), torch.tensor([1]).float().cuda(), img_size=tta_image_size.float().cuda())\n",
    "boxes, scores = process_det(0, det)\n",
    "del det\n",
    "gc.collect()\n",
    "for box in boxes:\n",
    "    cv2.rectangle(tta_image_numpy, (box[0].astype(int), box[1].astype(int)),\n",
    "                                   (box[2].astype(int)-1,  box[3].astype(int)-1), (1, 0, 0), 2)\n",
    "ax[2].imshow(tta_image_numpy);\n",
    "ax[2].set_title('TTA2')\n",
    "\n",
    "# deaugment\n",
    "boxes, _ = transform2.deaugment_boxes(boxes)\n",
    "for box in boxes:\n",
    "    cv2.rectangle(numpy_image, (box[0].astype(int), box[1].astype(int)),\n",
    "                               (box[2].astype(int)-1, box[3].astype(int)-1), (1, 0, 0), 2)\n",
    "ax[3].imshow(numpy_image);\n",
    "ax[3].set_title('deaugment TTA2');\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make TTA combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.TTAHorizontalFlip object at 0x7f57c79ad710>, <__main__.TTAVerticalFlip object at 0x7f57c50d5f10>, <__main__.TTARotate90 object at 0x7f57c50d5e50>]\n",
      "[<__main__.TTAHorizontalFlip object at 0x7f57c79ad710>, <__main__.TTAVerticalFlip object at 0x7f57c50d5f10>]\n",
      "[<__main__.TTAHorizontalFlip object at 0x7f57c79ad710>, <__main__.TTARotate90 object at 0x7f57c50d5e50>]\n",
      "[<__main__.TTAHorizontalFlip object at 0x7f57c79ad710>]\n",
      "[<__main__.TTAVerticalFlip object at 0x7f57c50d5f10>, <__main__.TTARotate90 object at 0x7f57c50d5e50>]\n",
      "[<__main__.TTAVerticalFlip object at 0x7f57c50d5f10>]\n",
      "[<__main__.TTARotate90 object at 0x7f57c50d5e50>]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "tta_transforms = []\n",
    "for tta_combination in product([TTAHorizontalFlip(), None], \n",
    "                               [TTAVerticalFlip(), None],\n",
    "                               [TTARotate90(), None]):\n",
    "                               #[TTACrop(size=400, loc=1), TTACrop(size=400, loc=2), TTACrop(size=400, loc=3), TTACrop(size=400, loc=4), None]):\n",
    "    print([tta_transform for tta_transform in tta_combination if tta_transform])\n",
    "    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make prediction with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tta_predictions(images, score_threshold=0.25):\n",
    "    with torch.no_grad():\n",
    "        images = torch.stack(images).float().cuda()\n",
    "        predictions = []\n",
    "        for tta_transform in tta_transforms:\n",
    "            result = []\n",
    "            #images_batch = tta_transform.batch_augment(images.clone())\n",
    "            img_sizes = torch.stack((torch.ones([2,])*images.shape[2],\n",
    "                                     torch.ones([2,])*images.shape[3])).permute(1,0)\n",
    "            img_sizes = img_sizes.int().cuda()\n",
    "            det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda(), img_size=img_sizes)\n",
    "\n",
    "            for i in range(images.shape[0]):\n",
    "                boxes = det[i].detach().cpu().numpy()[:,:4]\n",
    "                scores = det[i].detach().cpu().numpy()[:,4]\n",
    "                indexes = np.where(scores > score_threshold)[0]\n",
    "                boxes = boxes[indexes]\n",
    "                scores = scores[indexes]\n",
    "                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "                boxes, scores = tta_transform.deaugment_boxes(boxes.copy(), scores.copy())\n",
    "                result.append({\n",
    "                    'boxes': boxes,\n",
    "                    'scores': scores,\n",
    "                })\n",
    "            predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "def run_wbf(predictions, image_index, image_size=img_size_global, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/image_size).tolist() for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n",
    "    \n",
    "    #for prediction in predictions:\n",
    "        #print(prediction[image_index]['scores'].shape)\n",
    "    #print('-'*15)\n",
    "    \n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]).astype(int).tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test codes for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Show sample result of TTA image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_idx = 0 # 0-9\n",
    "for j, (images, image_ids) in enumerate(data_loader):\n",
    "    if sample_idx//2 == j:\n",
    "        break\n",
    "i = sample_idx%2\n",
    "\n",
    "predictions = make_tta_predictions(images)\n",
    "sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "boxes = boxes.astype(np.int32).clip(min=0, max=img_size_global)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample, (box[0], box[1]), (box[2]-1, box[3]-1), (1, 0, 0), 2)\n",
    "    \n",
    "ax.imshow(sample);\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Predict and format for final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "#results = []\n",
    "results_1 = pd.DataFrame(data=None, columns=['image_id','x','y','w','h'])\n",
    "bbox_count = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "for images, image_ids in tqdm(data_loader):\n",
    "    predictions = make_tta_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "        boxes = (boxes*IMG_SIZE_ORG/img_size_global).astype(np.int32).clip(min=0, max=IMG_SIZE_ORG)\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = pd.DataFrame(data=None, columns=['image_id','x','y','w','h'], index=range(bbox_count,bbox_count+boxes.shape[0]))\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'image_id'] = image_id\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'x'] = boxes[:,0]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'y'] = boxes[:,1]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'w'] = boxes[:,2]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'h'] = boxes[:,3]\n",
    "        bbox_count += boxes.shape[0]\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "        results.append(result)\n",
    "        \"\"\"\n",
    "        results_1 = pd.merge(results_1, result, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 4\n",
    "    batch_size = batch_global\n",
    "    n_epochs = epoch_global\n",
    "    lr = 0.0002\n",
    "    mixed_precision = True\n",
    "    accumulate = 16\n",
    "\n",
    "    resume = True\n",
    "    resume_path = weight_global\n",
    "\n",
    "    folder = folder_global\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True  # do scheduler.step after validation stage loss\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "#         pct_start=0.1,\n",
    "#         anneal_strategy='cos', \n",
    "#         final_div_factor=10**5\n",
    "#     )\n",
    "    \n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        verbose=True, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=img_size_global, width=img_size_global, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROOT_PATH = DATA_ROOT_PATH\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, marking, image_ids, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = image_ids\n",
    "        self.marking = marking\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        \n",
    "        # (1) Apply cutmix augmentation\n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "        \n",
    "        # (2) Apply albumentation\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    break\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.marking[self.marking['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRetriever(\n",
    "    image_ids=np.unique(results_1['image_id'].values),\n",
    "    #image_ids=results_1['image_id'].values,\n",
    "    marking=results_1,\n",
    "    transforms=get_train_transforms(),\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        #self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level='O1', verbosity=0)\n",
    "\n",
    "        if self.config.resume:\n",
    "            self.load(self.config.resume_path)\n",
    "        else:\n",
    "            self.epoch = 0\n",
    "            self.best_summary_loss = 10**5\n",
    "            \n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        if os.path.exists(self.base_dir):\n",
    "            shutil.rmtree(self.base_dir)\n",
    "        os.makedirs(self.base_dir)\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "            \n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader):\n",
    "        epoch_range = range(0, self.config.n_epochs)\n",
    "        for e in epoch_range:\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.train_one_epoch(train_loader)\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "            \"\"\"\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-1]:\n",
    "                    os.remove(path)\n",
    "            \"\"\"\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        #t = time.time()\n",
    "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
    "            \"\"\"\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \"\"\"\n",
    "            images = torch.stack(images)\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(images, {'bbox':boxes, 'cls':labels})\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            #with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                #scaled_loss.backward()\n",
    "            loss.backward()\n",
    "\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            #'amp_state_dict': amp.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        #amp.load_state_dict(checkpoint['amp_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        print('load completed')\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "    net.to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "def get_net():\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load('../input/efficientdetweights/tf_efficientdet_d5-ef44aea8.pth')\n",
    "    net.load_state_dict(checkpoint)\n",
    "    config.num_classes = 1\n",
    "    config.image_size = img_size_global\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "    return DetBenchTrain(net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load completed\n",
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2020-08-04T06:40:01.974242\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 38, summary_loss: 0.19584, time: 7.91803\n",
      "\n",
      "2020-08-04T06:40:10.531170\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 39, summary_loss: 0.19089, time: 8.09142\n",
      "\n",
      "2020-08-04T06:40:19.551874\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 40, summary_loss: 0.18560, time: 8.00396\n",
      "\n",
      "2020-08-04T06:40:28.599920\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 41, summary_loss: 0.18084, time: 7.69959\n",
      "\n",
      "2020-08-04T06:40:37.337584\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 42, summary_loss: 0.17694, time: 7.75099\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "net = load_net('./weights/last-checkpoint.bin')\n",
    "#results = []\n",
    "results_2 = pd.DataFrame(data=None, columns=['image_id','x','y','w','h'])\n",
    "bbox_count = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "for images, image_ids in tqdm(data_loader):\n",
    "    predictions = make_tta_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "        boxes = (boxes*IMG_SIZE_ORG/img_size_global).astype(np.int32).clip(min=0, max=IMG_SIZE_ORG)\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = pd.DataFrame(data=None, columns=['image_id','x','y','w','h'], index=range(bbox_count,bbox_count+boxes.shape[0]))\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'image_id'] = image_id\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'x'] = boxes[:,0]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'y'] = boxes[:,1]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'w'] = boxes[:,2]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'h'] = boxes[:,3]\n",
    "        bbox_count += boxes.shape[0]\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "        results.append(result)\n",
    "        \"\"\"\n",
    "        results_2 = pd.merge(results_2, result, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 4\n",
    "    batch_size = batch_global\n",
    "    n_epochs = epoch_global\n",
    "    lr = 0.0002\n",
    "    mixed_precision = True\n",
    "    accumulate = 16\n",
    "\n",
    "    resume = True\n",
    "    resume_path = './weights/last-checkpoint.bin'\n",
    "\n",
    "    folder = folder_global\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True  # do scheduler.step after validation stage loss\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "#         pct_start=0.1,\n",
    "#         anneal_strategy='cos', \n",
    "#         final_div_factor=10**5\n",
    "#     )\n",
    "    \n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        verbose=True, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )\n",
    "    # --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRetriever(\n",
    "    image_ids=np.unique(results_2['image_id'].values),\n",
    "    #image_ids=results_1['image_id'].values,\n",
    "    marking=results_2,\n",
    "    transforms=get_train_transforms(),\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load completed\n",
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2020-08-04T06:41:01.913991\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 43, summary_loss: 0.18357, time: 7.59892\n",
      "\n",
      "2020-08-04T06:41:10.137663\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 44, summary_loss: 0.17881, time: 8.05535\n",
      "\n",
      "2020-08-04T06:41:19.371089\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 45, summary_loss: 0.17370, time: 7.80852\n",
      "\n",
      "2020-08-04T06:41:28.085892\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 46, summary_loss: 0.16874, time: 7.73465\n",
      "\n",
      "2020-08-04T06:41:36.683501\n",
      "LR: 1.5625e-06\n",
      "[RESULT]: Train. Epoch: 47, summary_loss: 0.16704, time: 7.69386\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "net = load_net('./weights/last-checkpoint.bin')\n",
    "\n",
    "results = []\n",
    "#results_1 = pd.DataFrame(data=None, columns=['image_id','x','y','w','h'])\n",
    "bbox_count = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "for images, image_ids in tqdm(data_loader):\n",
    "    predictions = make_tta_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "        boxes = (boxes*IMG_SIZE_ORG/img_size_global).astype(np.int32).clip(min=0, max=IMG_SIZE_ORG)\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \"\"\"\n",
    "        result = pd.DataFrame(data=None, columns=['image_id','x','y','w','h'], index=range(bbox_count,bbox_count+boxes.shape[0]))\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'image_id'] = image_id\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'x'] = boxes[:,0]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'y'] = boxes[:,1]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'w'] = boxes[:,2]\n",
    "        result.loc[bbox_count:bbox_count+boxes.shape[0], 'h'] = boxes[:,3]\n",
    "        bbox_count += boxes.shape[0]\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        #results_1 = pd.merge(results_1, result, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>0.9641 888 49 107 95 0.9579 729 885 100 92 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>0.9642 771 828 166 162 0.9234 909 123 113 98 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.9324 874 290 149 141 0.9005 362 153 102 99 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53f253011</td>\n",
       "      <td>0.9248 20 597 119 143 0.9219 232 839 116 96 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cb8d261a3</td>\n",
       "      <td>0.8637 434 116 109 86 0.8608 23 862 81 151 0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  2fd875eaa  0.9641 888 49 107 95 0.9579 729 885 100 92 0.9...\n",
       "1  cc3532ff6  0.9642 771 828 166 162 0.9234 909 123 113 98 0...\n",
       "2  51b3e36ab  0.9324 874 290 149 141 0.9005 362 153 102 99 0...\n",
       "3  53f253011  0.9248 20 597 119 143 0.9219 232 839 116 96 0....\n",
       "4  cb8d261a3  0.8637 434 116 109 86 0.8608 23 862 81 151 0.8..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df.to_csv('submission.csv', index=False)\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
