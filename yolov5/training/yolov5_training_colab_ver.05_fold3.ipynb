{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolov5_training_colab_ver.05_fold3.ipynb의 사본","provenance":[{"file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","timestamp":1594664809100}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","authorship_tag":"ABX9TyN/su0Z4/sDfyXgJob8aRIk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AiQPodddRTaD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594621505692,"user_tz":-540,"elapsed":762,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"a1f840d4-c262-48f5-9510-2ad778b8bbbb"},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","\"\"\""],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'\\nfunction ClickConnect(){\\n    console.log(\"Clicked on connect button\"); \\n    document.querySelector(\"#ok\").click()\\n}\\nsetInterval(ClickConnect,60000)\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"Yf22UTuLusak","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1594621508241,"user_tz":-540,"elapsed":3253,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"b1b62b32-99a2-407d-fb97-3f65fe5dcbfc"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mon Jul 13 06:25:05 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OoBHduIadAI7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594621508242,"user_tz":-540,"elapsed":3235,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["import numpy as np\n","import pandas as pd\n","import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlvKV6g3Kcdn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"executionInfo":{"status":"ok","timestamp":1594621513764,"user_tz":-540,"elapsed":8735,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"06ee998c-b6a2-4c8e-e995-14a0572cddaa"},"source":["!pip install -U PyYAML"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting PyYAML\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\r\u001b[K     |█▏                              | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 4.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=f6bf59381911d2a9095c4d2ce7a64d944148f4c32133c7591ced9b76fbc70a90\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built PyYAML\n","Installing collected packages: PyYAML\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SDekTiLTg1WA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594621513765,"user_tz":-540,"elapsed":8717,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"45151b59-23a5-40ed-e543-a3947b90d0ef"},"source":["%%writefile setup.sh\n","\n","export CUDA_HOME=/usr/local/cuda-10.1\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Writing setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wPuaO9L9g2GF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594621925570,"user_tz":-540,"elapsed":420484,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"3742bf84-baee-4fd3-f50b-e3209c856a6c"},"source":["!sh setup.sh"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Cloning into 'apex'...\n","remote: Enumerating objects: 80, done.\u001b[K\n","remote: Counting objects: 100% (80/80), done.\u001b[K\n","remote: Compressing objects: 100% (61/61), done.\u001b[K\n","remote: Total 7335 (delta 40), reused 42 (delta 19), pack-reused 7255\u001b[K\n","Receiving objects: 100% (7335/7335), 13.88 MiB | 26.27 MiB/s, done.\n","Resolving deltas: 100% (4941/4941), done.\n","/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-t5tafojk\n","Created temporary directory: /tmp/pip-req-tracker-erlxzrnm\n","Created requirements tracker '/tmp/pip-req-tracker-erlxzrnm'\n","Created temporary directory: /tmp/pip-install-1y_rv6x4\n","Processing ./apex\n","  Created temporary directory: /tmp/pip-req-build-enrziaqt\n","  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-erlxzrnm'\n","    Running setup.py (path:/tmp/pip-req-build-enrziaqt/setup.py) egg_info for package from file:///content/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-enrziaqt/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-enrziaqt/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-enrziaqt/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-enrziaqt/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-enrziaqt/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-enrziaqt/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-enrziaqt/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-enrziaqt has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n","  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-erlxzrnm'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-ekq9bx0c\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-enrziaqt/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-enrziaqt/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ekq9bx0c/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    /tmp/pip-req-build-enrziaqt/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda-10.1/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    creating /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-ekq9bx0c/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-enrziaqt\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-erlxzrnm'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"665tCOfRpIIx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594621925825,"user_tz":-540,"elapsed":420720,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["import torch.random\n","import random\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqnYi7N2Kfjy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594621944372,"user_tz":-540,"elapsed":439235,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["zip_name = 'split-fold3.zip'\n","zip_path = '/content/drive/My Drive/Colab Notebooks/gwdsplit/' + zip_name\n","!cp \"{zip_path}\" .\n","!unzip -q '{zip_name}'\n","!rm '{zip_name}'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"978lsjD2vQAF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594621950207,"user_tz":-540,"elapsed":445050,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["yolov5_name = 'yolov5.zip'\n","yolov5_path = '/content/drive/My Drive/Colab Notebooks/' + yolov5_name\n","!cp '{yolov5_path}' .\n","!unzip -q '{yolov5_name}'\n","!rm '{yolov5_name}'"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROkeoZzd_ljI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594621952956,"user_tz":-540,"elapsed":447784,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["weight_name = 'yolov5x_coco.pt'\n","weight_path = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + weight_name\n","!cp '{weight_path}' ."],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1l1XG4zejqR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594621952956,"user_tz":-540,"elapsed":447767,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["train_input = '/content/drive/My Drive/Colab Notebooks/yolov5/train.py'\n","data_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/wheat_colab.yaml'\n","cfg_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/yolov5x.yaml'\n","weights_input = '/content/' + weight_name\n","name_input = 'x-b2-e50-fold3'"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-Sb84dpxl6U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594621953479,"user_tz":-540,"elapsed":448271,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"8f95d6fb-bee5-4d48-8cea-bf722ea28141"},"source":["%cd /content/yolov5/"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/yolov5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lYOIA2YaeNgC","colab_type":"text"},"source":["# **train.py**"]},{"cell_type":"code","metadata":{"id":"9UCW99OCx4QA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1594621955693,"user_tz":-540,"elapsed":450466,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"8e7fa402-0c95-47cf-9b1c-9c65dc61af34"},"source":["import argparse\n","\n","import torch.distributed as dist\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.utils.data\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import test  # import test.py to get mAP after each epoch\n","from models.yolo import Model\n","from utils import google_utils\n","from utils.datasets import *\n","from utils.utils import *\n","\n","mixed_precision = True\n","try:  # Mixed precision training https://github.com/NVIDIA/apex\n","    from apex import amp\n","except:\n","    print('Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex')\n","    mixed_precision = False  # not installed\n","\n","wdir = 'weights' + os.sep  # weights dir\n","os.makedirs(wdir, exist_ok=True)\n","last = wdir + 'last.pt'\n","best = wdir + 'best.pt'\n","results_file = 'results.txt'\n","\n","# Hyperparameters\n","hyp = {'lr0': 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","       'momentum': 0.937,  # SGD momentum\n","       'weight_decay': 5e-4,  # optimizer weight decay\n","       'giou': 0.05,  # giou loss gain\n","       'cls': 0.58,  # cls loss gain\n","       'cls_pw': 1.0,  # cls BCELoss positive_weight\n","       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)\n","       'obj_pw': 1.0,  # obj BCELoss positive_weight\n","       'iou_t': 0.20,  # iou training threshold\n","       'anchor_t': 4.0,  # anchor-multiple threshold\n","       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n","       'hsv_h': 0.014,  # image HSV-Hue augmentation (fraction)\n","       'hsv_s': 0.68,  # image HSV-Saturation augmentation (fraction)\n","       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n","       'degrees': 0.0,  # image rotation (+/- deg)\n","       'translate': 0.0,  # image translation (+/- fraction)\n","       'scale': 0.5,  # image scale (+/- gain)\n","       'shear': 0.0}  # image shear (+/- deg)\n","print(hyp)\n","\n","# Overwrite hyp with hyp*.txt (optional)\n","f = glob.glob('hyp*.txt')\n","if f:\n","    print('Using %s' % f[0])\n","    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n","        hyp[k] = v\n","\n","# Print focal loss if gamma > 0\n","if hyp['fl_gamma']:\n","    print('Using FocalLoss(gamma=%g)' % hyp['fl_gamma'])\n","\n","\n","def train(hyp):\n","    epochs = opt.epochs  # 300\n","    batch_size = opt.batch_size  # 64\n","    weights = opt.weights  # initial training weights\n","\n","    # Configure\n","    init_seeds(1)\n","    with open(opt.data) as f:\n","        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n","    train_path = data_dict['train']\n","    test_path = data_dict['val']\n","    nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes\n","\n","    # Remove previous results\n","    for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):\n","        os.remove(f)\n","\n","    # Create model\n","    model = Model(opt.cfg, nc=data_dict['nc']).to(device)\n","\n","    # Image sizes\n","    gs = int(max(model.stride))  # grid size (max stride)\n","    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n","\n","    # Optimizer\n","    nbs = 64  # nominal batch size\n","    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n","    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n","    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n","    for k, v in model.named_parameters():\n","        if v.requires_grad:\n","            if '.bias' in k:\n","                pg2.append(v)  # biases\n","            elif '.weight' in k and '.bn' not in k:\n","                pg1.append(v)  # apply weight decay\n","            else:\n","                pg0.append(v)  # all else\n","\n","    optimizer = optim.Adam(pg0, lr=hyp['lr0']) if opt.adam else \\\n","        optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n","    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n","    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n","    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n","    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n","    print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n","    del pg0, pg1, pg2\n","\n","    # Load Model\n","    google_utils.attempt_download(weights)\n","    start_epoch, best_fitness = 0, 0.0\n","    if weights.endswith('.pt'):  # pytorch format\n","        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","\n","        # load model\n","        try:\n","            ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()\n","                             if model.state_dict()[k].shape == v.shape}  # to FP32, filter\n","            model.load_state_dict(ckpt['model'], strict=False)\n","        except KeyError as e:\n","            s = \"%s is not compatible with %s. This may be due to model differences or %s may be out of date. \" \\\n","                \"Please delete or update %s and try again, or use --weights '' to train from scratch.\" \\\n","                % (opt.weights, opt.cfg, opt.weights, opt.weights)\n","            raise KeyError(s) from e\n","\n","        # load optimizer\n","        if ckpt['optimizer'] is not None:\n","            optimizer.load_state_dict(ckpt['optimizer'])\n","            best_fitness = ckpt['best_fitness']\n","\n","        # load results\n","        if ckpt.get('training_results') is not None:\n","            with open(results_file, 'w') as file:\n","                file.write(ckpt['training_results'])  # write results.txt\n","\n","        # epochs\n","        start_epoch = ckpt['epoch'] + 1\n","        if epochs < start_epoch:\n","            print('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\n","                  (opt.weights, ckpt['epoch'], epochs))\n","            epochs += ckpt['epoch']  # finetune additional epochs\n","\n","        del ckpt\n","\n","    # Mixed precision training https://github.com/NVIDIA/apex\n","    if mixed_precision:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n","\n","\n","    scheduler.last_epoch = start_epoch - 1  # do not move\n","    # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822\n","    # plot_lr_scheduler(optimizer, scheduler, epochs)\n","\n","    # Initialize distributed training\n","    if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n","        dist.init_process_group(backend='nccl',  # distributed backend\n","                                init_method='tcp://127.0.0.1:9999',  # init method\n","                                world_size=1,  # number of nodes\n","                                rank=0)  # node rank\n","        model = torch.nn.parallel.DistributedDataParallel(model)\n","        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","    # Trainloader\n","    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\n","                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect)\n","    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n","    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Correct your labels or your model.' % (mlc, nc, opt.cfg)\n","\n","    # Testloader\n","    testloader = create_dataloader(test_path, imgsz_test, batch_size, gs, opt,\n","                                   hyp=hyp, augment=False, cache=opt.cache_images, rect=True)[0]\n","\n","    # Model parameters\n","    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\n","    model.nc = nc  # attach number of classes to model\n","    model.hyp = hyp  # attach hyperparameters to model\n","    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n","    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n","    model.names = data_dict['names']\n","\n","    # Class frequency\n","    labels = np.concatenate(dataset.labels, 0)\n","    c = torch.tensor(labels[:, 0])  # classes\n","    # cf = torch.bincount(c.long(), minlength=nc) + 1.\n","    # model._initialize_biases(cf.to(device))\n","    if tb_writer:\n","        plot_labels(labels)\n","        tb_writer.add_histogram('classes', c, 0)\n","\n","    # Check anchors\n","    if not opt.noautoanchor:\n","        check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n","\n","    # Exponential moving average\n","    ema = torch_utils.ModelEMA(model)\n","\n","    # Start training\n","    t0 = time.time()\n","    nb = len(dataloader)  # number of batches\n","    n_burn = max(3 * nb, 1e3)  # burn-in iterations, max(3 epochs, 1k iterations)\n","    maps = np.zeros(nc)  # mAP per class\n","    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n","    print('Image sizes %g train, %g test' % (imgsz, imgsz_test))\n","    print('Using %g dataloader workers' % dataloader.num_workers)\n","    print('Starting training for %g epochs...' % epochs)\n","    # torch.autograd.set_detect_anomaly(True)\n","    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n","        model.train()\n","\n","        # Update image weights (optional)\n","        if dataset.image_weights:\n","            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n","            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n","            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n","\n","        # Update mosaic border\n","        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n","        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n","\n","        mloss = torch.zeros(4, device=device)  # mean losses\n","        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n","        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n","        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n","            ni = i + nb * epoch  # number integrated batches (since train start)\n","            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n","\n","            # Burn-in\n","            if ni <= n_burn:\n","                xi = [0, n_burn]  # x interp\n","                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n","                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n","                for j, x in enumerate(optimizer.param_groups):\n","                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n","                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n","                    if 'momentum' in x:\n","                        x['momentum'] = np.interp(ni, xi, [0.9, hyp['momentum']])\n","\n","            # Multi-scale\n","            if opt.multi_scale:\n","                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n","                sf = sz / max(imgs.shape[2:])  # scale factor\n","                if sf != 1:\n","                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n","                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n","\n","            # Forward\n","            pred = model(imgs)\n","\n","            # Loss\n","            loss, loss_items = compute_loss(pred, targets.to(device), model)\n","            if not torch.isfinite(loss):\n","                print('WARNING: non-finite loss, ending training ', loss_items)\n","                return results\n","\n","            # Backward\n","            if mixed_precision:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            # Optimize\n","            if ni % accumulate == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                ema.update(model)\n","\n","            # Print\n","            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n","            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n","            s = ('%10s' * 2 + '%10.4g' * 6) % (\n","                '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n","            pbar.set_description(s)\n","\n","            # Plot\n","            if ni < 3:\n","                f = 'train_batch%g.jpg' % ni  # filename\n","                result = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n","                if tb_writer and result is not None:\n","                    tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n","                    # tb_writer.add_graph(model, imgs)  # add model to tensorboard\n","\n","            # end batch ------------------------------------------------------------------------------------------------\n","\n","        # Scheduler\n","        scheduler.step()\n","\n","        # mAP\n","        ema.update_attr(model)\n","        final_epoch = epoch + 1 == epochs\n","        if not opt.notest or final_epoch:  # Calculate mAP\n","            results, maps, times = test.test(opt.data,\n","                                             batch_size=batch_size,\n","                                             imgsz=imgsz_test,\n","                                             save_json=final_epoch and opt.data.endswith(os.sep + 'coco.yaml'),\n","                                             model=ema.ema,\n","                                             single_cls=opt.single_cls,\n","                                             dataloader=testloader)\n","\n","        # Write\n","        with open(results_file, 'a') as f:\n","            f.write(s + '%10.4g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n","        if len(opt.name) and opt.bucket:\n","            os.system('gsutil cp results.txt gs://%s/results/results%s.txt' % (opt.bucket, opt.name))\n","\n","        # Tensorboard\n","        if tb_writer:\n","            tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n","                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/F1',\n","                    'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n","            for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n","                tb_writer.add_scalar(tag, x, epoch)\n","\n","        # Update best mAP\n","        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n","        if fi > best_fitness:\n","            best_fitness = fi\n","\n","        # Save model\n","        save = (not opt.nosave) or (final_epoch and not opt.evolve)\n","        if save:\n","            with open(results_file, 'r') as f:  # create checkpoint\n","                ckpt = {'epoch': epoch,\n","                        'best_fitness': best_fitness,\n","                        'training_results': f.read(),\n","                        'model': ema.ema,\n","                        'optimizer': None if final_epoch else optimizer.state_dict()}\n","\n","            # Save last, best and delete\n","            torch.save(ckpt, last)\n","            if (best_fitness == fi) and not final_epoch:\n","                torch.save(ckpt, best)\n","            del ckpt\n","\n","        # end epoch ----------------------------------------------------------------------------------------------------\n","    # end training\n","\n","    # Strip optimizers\n","    n = ('_' if len(opt.name) and not opt.name.isnumeric() else '') + opt.name\n","    fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n","    for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n","        if os.path.exists(f1):\n","            os.rename(f1, f2)  # rename\n","            ispt = f2.endswith('.pt')  # is *.pt\n","            strip_optimizer(f2) if ispt else None  # strip optimizer\n","            os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n","\n","    # Finish\n","    if not opt.evolve:\n","        plot_results()  # save as results.png\n","    print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n","    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\n","    torch.cuda.empty_cache()\n","    return results"],"execution_count":13,"outputs":[{"output_type":"stream","text":["{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9csFml3nfP7H","colab_type":"text"},"source":["# **Train Option**"]},{"cell_type":"code","metadata":{"id":"h0uu9IpAyJC1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594664697753,"user_tz":-540,"elapsed":43192504,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"2b04f9b9-0d5e-40b0-9e7f-9fcd16af8377"},"source":["check_git_status()\n","class opt:\n","    epochs=50                #parser.add_argument('--epochs', type=int, default=300)\n","    batch_size=2            #parser.add_argument('--batch-size', type=int, default=16)\n","    cfg=cfg_input           #parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='*.cfg path')\n","    data=data_input         #parser.add_argument('--data', type=str, default='data/coco128.yaml', help='*.data path')\n","    img_size=[1024, 1024]   #parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')\n","    rect=False              #parser.add_argument('--rect', action='store_true', help='rectangular training')\n","    resume=False            #parser.add_argument('--resume', action='store_true', help='resume training from last.pt')\n","    nosave=False            #parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n","    notest=False            #parser.add_argument('--notest', action='store_true', help='only test final epoch')\n","    noautoanchor=False      #parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n","    evolve=False            #parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\n","    bucket=''               #parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n","    cache_images=False      #parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n","    weights=weights_input   #parser.add_argument('--weights', type=str, default='', help='initial weights path')\n","    name=name_input         #parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')\n","    device=''               #parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    adam=False              #parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n","    multi_scale=False       #parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%')\n","    single_cls=False        #parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\n","\n","#parser = argparse.ArgumentParser()\n","#opt = parser.parse_args()\n","\n","opt.weights = last if opt.resume and not opt.weights else opt.weights\n","opt.cfg = check_file(opt.cfg)  # check file\n","opt.data = check_file(opt.data)  # check file\n","print(opt)\n","opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\n","device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n","if device.type == 'cpu':\n","    mixed_precision = False\n","\n","# Train\n","if not opt.evolve:\n","    tb_writer = SummaryWriter(comment=opt.name)\n","    print('Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/')\n","    train(hyp)\n","\n","# Evolve hyperparameters (optional)\n","else:\n","    tb_writer = None\n","    opt.notest, opt.nosave = True, True  # only test/save final epoch\n","    if opt.bucket:\n","        os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n","\n","    for _ in range(10):  # generations to evolve\n","        if os.path.exists('evolve.txt'):  # if evolve.txt exists: select best hyps and mutate\n","            # Select parent(s)\n","            parent = 'single'  # parent selection method: 'single' or 'weighted'\n","            x = np.loadtxt('evolve.txt', ndmin=2)\n","            n = min(5, len(x))  # number of previous results to consider\n","            x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n","            w = fitness(x) - fitness(x).min()  # weights\n","            if parent == 'single' or len(x) == 1:\n","                # x = x[random.randint(0, n - 1)]  # random selection\n","                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n","            elif parent == 'weighted':\n","                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n","\n","            # Mutate\n","            mp, s = 0.9, 0.2  # mutation probability, sigma\n","            npr = np.random\n","            npr.seed(int(time.time()))\n","            g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n","            ng = len(g)\n","            v = np.ones(ng)\n","            while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n","                v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n","            for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n","                hyp[k] = x[i + 7] * v[i]  # mutate\n","\n","        # Clip to limits\n","        keys = ['lr0', 'iou_t', 'momentum', 'weight_decay', 'hsv_s', 'hsv_v', 'translate', 'scale', 'fl_gamma']\n","        limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n","        for k, v in zip(keys, limits):\n","            hyp[k] = np.clip(hyp[k], v[0], v[1])\n","\n","        # Train mutation\n","        results = train(hyp.copy())\n","\n","        # Write mutation results\n","        print_mutation(hyp, results, opt.bucket)\n","\n","        # Plot results\n","        # plot_evolution_results(hyp)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["<class '__main__.opt'>\n","Using CUDA Apex device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB)\n","\n","Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n","  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n","  2                -1  1    315680  models.common.BottleneckCSP             [160, 160, 4]                 \n","  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n","  4                -1  1   3311680  models.common.BottleneckCSP             [320, 320, 12]                \n","  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n","  6                -1  1  13228160  models.common.BottleneckCSP             [640, 640, 12]                \n","  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n","  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n","  9                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1   5435520  models.common.BottleneckCSP             [1280, 640, 4, False]         \n"," 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1   1360960  models.common.BottleneckCSP             [640, 320, 4, False]          \n"," 18                -1  1      5778  torch.nn.modules.conv.Conv2d            [320, 18, 1, 1]               \n"," 19                -2  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n"," 20          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 21                -1  1   5025920  models.common.BottleneckCSP             [640, 640, 4, False]          \n"," 22                -1  1     11538  torch.nn.modules.conv.Conv2d            [640, 18, 1, 1]               \n"," 23                -2  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n"," 24          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 25                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 26                -1  1     23058  torch.nn.modules.conv.Conv2d            [1280, 18, 1, 1]              \n"," 27      [-1, 22, 18]  1         0  models.yolo.Detect                      [1, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n","Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n","\n","Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n"],"name":"stdout"},{"output_type":"stream","text":["Reading image shapes: 100%|██████████| 2699/2699 [00:00<00:00, 13802.52it/s]\n","Caching labels /content/labels/train (2699 found, 0 missing, 0 empty, 0 duplicate, for 2699 images): 100%|██████████| 2699/2699 [00:00<00:00, 4789.78it/s]\n","Reading image shapes: 100%|██████████| 674/674 [00:00<00:00, 14900.15it/s]\n","Caching labels /content/labels/valid (430 found, 0 missing, 0 empty, 0 duplicate, for 674 images):  64%|██████▍   | 430/674 [00:00<00:00, 4291.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saving labels to /content/labels/train.npy for faster future loading\n"],"name":"stdout"},{"output_type":"stream","text":["\rCaching labels /content/labels/valid (674 found, 0 missing, 0 empty, 0 duplicate, for 674 images): 100%|██████████| 674/674 [00:00<00:00, 4332.18it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Analyzing anchors... Best Possible Recall (BPR) = 0.9992\n","Image sizes 1024 train, 1024 test\n","Using 2 dataloader workers\n","Starting training for 50 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      0/49      4.8G   0.07345    0.1851         0    0.2585        34      1024: 100%|██████████| 1350/1350 [12:47<00:00,  1.76it/s]\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [02:07<00:00,  2.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.249       0.949       0.759       0.309\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      1/49     5.28G   0.05578    0.1553         0    0.2111        22      1024: 100%|██████████| 1350/1350 [12:20<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:56<00:00,  2.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.538       0.937       0.906       0.409\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      2/49     5.28G   0.05442    0.1572         0    0.2116       105      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:55<00:00,  2.93it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.465       0.943       0.901       0.436\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      3/49     5.28G    0.0464    0.1505         0    0.1969       154      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.575       0.944        0.92       0.462\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      4/49     5.28G   0.04269    0.1446         0    0.1873        41      1024: 100%|██████████| 1350/1350 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.705       0.945       0.937       0.497\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      5/49     5.28G   0.04012    0.1446         0    0.1847        56      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:52<00:00,  3.00it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.689       0.947       0.938       0.505\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      6/49     5.28G   0.03903    0.1437         0    0.1827       103      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:51<00:00,  3.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.709       0.945        0.94       0.516\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      7/49     5.28G   0.03823    0.1418         0    0.1801       103      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:52<00:00,  2.98it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.713       0.948       0.941       0.514\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      8/49     5.28G   0.03805    0.1408         0    0.1789        35      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.727       0.945        0.94       0.518\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      9/49     5.28G   0.03749    0.1392         0    0.1767        43      1024: 100%|██████████| 1350/1350 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:48<00:00,  3.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.725       0.946       0.943       0.523\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     10/49     5.28G   0.03678    0.1385         0    0.1752        22      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:48<00:00,  3.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04        0.74       0.946       0.944       0.524\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     11/49     5.28G   0.03634     0.139         0    0.1753       134      1024: 100%|██████████| 1350/1350 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.709       0.949       0.943       0.522\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     12/49     5.28G   0.03622    0.1384         0    0.1747        84      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:52<00:00,  2.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.772       0.939        0.94       0.522\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     13/49     5.28G   0.03607    0.1389         0    0.1749        54      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:51<00:00,  3.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.747       0.946       0.943       0.526\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     14/49     5.28G   0.03576    0.1378         0    0.1735        39      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.765       0.942       0.942       0.529\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     15/49     5.28G   0.03534    0.1357         0    0.1711        10      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.755       0.946       0.944       0.531\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     16/49     5.28G   0.03497    0.1346         0    0.1696        25      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.748       0.945       0.944       0.526\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     17/49     5.28G   0.03531    0.1371         0    0.1724        38      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.759       0.945       0.942       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     18/49     5.28G   0.03463    0.1342         0    0.1688         8      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.759       0.946       0.945       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     19/49     5.28G   0.03456    0.1344         0     0.169        24      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.767       0.943       0.944       0.533\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     20/49     5.28G   0.03413    0.1344         0    0.1685        49      1024: 100%|██████████| 1350/1350 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.759       0.946       0.945       0.535\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     21/49     5.28G   0.03414    0.1342         0    0.1683        67      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.769       0.947       0.946       0.536\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     22/49     5.28G   0.03378    0.1324         0    0.1661        46      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.765       0.946       0.945       0.537\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     23/49     5.28G   0.03376    0.1322         0    0.1659        21      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.772       0.944       0.945       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     24/49     5.28G   0.03373    0.1328         0    0.1665        26      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.775       0.944       0.943       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     25/49     5.28G    0.0334    0.1317         0    0.1651        63      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.767       0.945       0.945       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     26/49     5.28G   0.03335    0.1316         0     0.165        57      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.775       0.944       0.945       0.537\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     27/49     5.28G   0.03326    0.1308         0    0.1641       120      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.773       0.945       0.944       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     28/49     5.28G   0.03314     0.131         0    0.1641        47      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.769       0.946       0.945       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     29/49     5.28G   0.03297    0.1297         0    0.1627        37      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.772       0.945       0.945       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     30/49     5.28G   0.03273    0.1288         0    0.1616       145      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.773       0.945       0.945        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     31/49     5.28G   0.03276    0.1289         0    0.1617        89      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:48<00:00,  3.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.774       0.946       0.946        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     32/49     5.28G   0.03269    0.1289         0    0.1616        15      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:48<00:00,  3.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.776       0.944       0.945       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     33/49     5.28G   0.03252    0.1291         0    0.1616        30      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.777       0.944       0.945       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     34/49     5.28G   0.03235    0.1271         0    0.1595        26      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.773       0.944       0.944       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     35/49     5.28G   0.03226    0.1273         0    0.1595       131      1024: 100%|██████████| 1350/1350 [12:19<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:51<00:00,  3.03it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04        0.78       0.943       0.945       0.541\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     36/49     5.28G   0.03222    0.1276         0    0.1598        59      1024: 100%|██████████| 1350/1350 [12:19<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.782       0.943       0.944        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     37/49     5.28G   0.03215    0.1285         0    0.1606        18      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.783       0.944       0.945        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     38/49     5.28G   0.03194    0.1261         0    0.1581        38      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.782       0.943       0.944        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     39/49     5.28G   0.03197    0.1265         0    0.1584        20      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.781       0.942       0.944        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     40/49     5.28G   0.03199    0.1258         0    0.1578        89      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.779       0.942       0.943       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     41/49     5.28G    0.0318    0.1255         0    0.1573       133      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.781       0.941       0.943       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     42/49     5.28G   0.03179    0.1261         0    0.1579        58      1024: 100%|██████████| 1350/1350 [12:19<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.785       0.941       0.943       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     43/49     5.28G    0.0317    0.1253         0     0.157        46      1024: 100%|██████████| 1350/1350 [12:19<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:51<00:00,  3.01it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.783       0.942       0.943       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     44/49     5.28G   0.03156    0.1242         0    0.1557        69      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.786       0.941       0.942       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     45/49     5.28G    0.0315    0.1248         0    0.1563        28      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04        0.79        0.94       0.942       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     46/49     5.28G   0.03141    0.1247         0    0.1561        30      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.785       0.942       0.942       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     47/49     5.28G   0.03148    0.1252         0    0.1567        22      1024: 100%|██████████| 1350/1350 [12:17<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.793        0.94       0.943       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     48/49     5.28G   0.03144    0.1238         0    0.1553        46      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:49<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04        0.79        0.94       0.942       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     49/49     5.28G   0.03131    0.1246         0    0.1559        36      1024: 100%|██████████| 1350/1350 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 337/337 [01:51<00:00,  3.03it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         674    2.96e+04       0.785       0.941       0.943       0.538\n","Optimizer stripped from weights/last_x-b2-e50-fold3.pt\n","Optimizer stripped from weights/best_x-b2-e50-fold3.pt\n","50 epochs completed in 11.863 hours.\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 10 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"HXFQcoG0kWE7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594664705628,"user_tz":-540,"elapsed":43200354,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["import shutil\n","new_weight = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + name_input\n","if os.path.exists(new_weight):\n","  shutil.rmtree(new_weight)\n","os.mkdir(new_weight)\n","\n","weight_last = '/content/yolov5/weights/last_' + name_input + '.pt'\n","weight_best = '/content/yolov5/weights/best_' + name_input + '.pt'\n","\n","!cp '{weight_last}' '{new_weight}'\n","!cp '{weight_best}' '{new_weight}'\n","!cp 'results.png' '{new_weight}'\n","!cp 'labels.png' '{new_weight}'\n","!cp 'test_batch0_gt.jpg' '{new_weight}'\n","!cp 'test_batch0_pred.jpg' '{new_weight}'"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtigXSkIl15i","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594664705630,"user_tz":-540,"elapsed":43200348,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":[""],"execution_count":15,"outputs":[]}]}