{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolov5_training_colab_ver.03.ipynb의 사본","provenance":[{"file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","timestamp":1594294718738}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","authorship_tag":"ABX9TyNPMH1K+5sUr2ndSadV40Z8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AiQPodddRTaD","colab_type":"code","colab":{}},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yf22UTuLusak","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1594276278564,"user_tz":-540,"elapsed":2643,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"abf933d6-6d03-4bbd-f1c3-c4f20ac88fff"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul  9 06:31:18 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OoBHduIadAI7","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlvKV6g3Kcdn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"executionInfo":{"status":"ok","timestamp":1594276289813,"user_tz":-540,"elapsed":6249,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"34b1652b-05b3-4d56-bd8f-a2d1f4a0550b"},"source":["!pip install -U PyYAML"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting PyYAML\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\r\u001b[K     |█▏                              | 10kB 30.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 3.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=2ea41247f9abf39f40bff3639f1fca59208d69d8098f98adfb97a08222a36351\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built PyYAML\n","Installing collected packages: PyYAML\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SDekTiLTg1WA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594276289815,"user_tz":-540,"elapsed":5290,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"24e69923-8e3d-46d5-e32a-67daccfc9f24"},"source":["%%writefile setup.sh\n","\n","export CUDA_HOME=/usr/local/cuda-10.1\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wPuaO9L9g2GF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594276697285,"user_tz":-540,"elapsed":411310,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"267e30e4-f6b0-42a3-f7ef-a091bf0f6087"},"source":["!sh setup.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'apex'...\n","remote: Enumerating objects: 80, done.\u001b[K\n","remote: Counting objects:   1% (1/80)\u001b[K\rremote: Counting objects:   2% (2/80)\u001b[K\rremote: Counting objects:   3% (3/80)\u001b[K\rremote: Counting objects:   5% (4/80)\u001b[K\rremote: Counting objects:   6% (5/80)\u001b[K\rremote: Counting objects:   7% (6/80)\u001b[K\rremote: Counting objects:   8% (7/80)\u001b[K\rremote: Counting objects:  10% (8/80)\u001b[K\rremote: Counting objects:  11% (9/80)\u001b[K\rremote: Counting objects:  12% (10/80)\u001b[K\rremote: Counting objects:  13% (11/80)\u001b[K\rremote: Counting objects:  15% (12/80)\u001b[K\rremote: Counting objects:  16% (13/80)\u001b[K\rremote: Counting objects:  17% (14/80)\u001b[K\rremote: Counting objects:  18% (15/80)\u001b[K\rremote: Counting objects:  20% (16/80)\u001b[K\rremote: Counting objects:  21% (17/80)\u001b[K\rremote: Counting objects:  22% (18/80)\u001b[K\rremote: Counting objects:  23% (19/80)\u001b[K\rremote: Counting objects:  25% (20/80)\u001b[K\rremote: Counting objects:  26% (21/80)\u001b[K\rremote: Counting objects:  27% (22/80)\u001b[K\rremote: Counting objects:  28% (23/80)\u001b[K\rremote: Counting objects:  30% (24/80)\u001b[K\rremote: Counting objects:  31% (25/80)\u001b[K\rremote: Counting objects:  32% (26/80)\u001b[K\rremote: Counting objects:  33% (27/80)\u001b[K\rremote: Counting objects:  35% (28/80)\u001b[K\rremote: Counting objects:  36% (29/80)\u001b[K\rremote: Counting objects:  37% (30/80)\u001b[K\rremote: Counting objects:  38% (31/80)\u001b[K\rremote: Counting objects:  40% (32/80)\u001b[K\rremote: Counting objects:  41% (33/80)\u001b[K\rremote: Counting objects:  42% (34/80)\u001b[K\rremote: Counting objects:  43% (35/80)\u001b[K\rremote: Counting objects:  45% (36/80)\u001b[K\rremote: Counting objects:  46% (37/80)\u001b[K\rremote: Counting objects:  47% (38/80)\u001b[K\rremote: Counting objects:  48% (39/80)\u001b[K\rremote: Counting objects:  50% (40/80)\u001b[K\rremote: Counting objects:  51% (41/80)\u001b[K\rremote: Counting objects:  52% (42/80)\u001b[K\rremote: Counting objects:  53% (43/80)\u001b[K\rremote: Counting objects:  55% (44/80)\u001b[K\rremote: Counting objects:  56% (45/80)\u001b[K\rremote: Counting objects:  57% (46/80)\u001b[K\rremote: Counting objects:  58% (47/80)\u001b[K\rremote: Counting objects:  60% (48/80)\u001b[K\rremote: Counting objects:  61% (49/80)\u001b[K\rremote: Counting objects:  62% (50/80)\u001b[K\rremote: Counting objects:  63% (51/80)\u001b[K\rremote: Counting objects:  65% (52/80)\u001b[K\rremote: Counting objects:  66% (53/80)\u001b[K\rremote: Counting objects:  67% (54/80)\u001b[K\rremote: Counting objects:  68% (55/80)\u001b[K\rremote: Counting objects:  70% (56/80)\u001b[K\rremote: Counting objects:  71% (57/80)\u001b[K\rremote: Counting objects:  72% (58/80)\u001b[K\rremote: Counting objects:  73% (59/80)\u001b[K\rremote: Counting objects:  75% (60/80)\u001b[K\rremote: Counting objects:  76% (61/80)\u001b[K\rremote: Counting objects:  77% (62/80)\u001b[K\rremote: Counting objects:  78% (63/80)\u001b[K\rremote: Counting objects:  80% (64/80)\u001b[K\rremote: Counting objects:  81% (65/80)\u001b[K\rremote: Counting objects:  82% (66/80)\u001b[K\rremote: Counting objects:  83% (67/80)\u001b[K\rremote: Counting objects:  85% (68/80)\u001b[K\rremote: Counting objects:  86% (69/80)\u001b[K\rremote: Counting objects:  87% (70/80)\u001b[K\rremote: Counting objects:  88% (71/80)\u001b[K\rremote: Counting objects:  90% (72/80)\u001b[K\rremote: Counting objects:  91% (73/80)\u001b[K\rremote: Counting objects:  92% (74/80)\u001b[K\rremote: Counting objects:  93% (75/80)\u001b[K\rremote: Counting objects:  95% (76/80)\u001b[K\rremote: Counting objects:  96% (77/80)\u001b[K\rremote: Counting objects:  97% (78/80)\u001b[K\rremote: Counting objects:  98% (79/80)\u001b[K\rremote: Counting objects: 100% (80/80)\u001b[K\rremote: Counting objects: 100% (80/80), done.\u001b[K\n","remote: Compressing objects: 100% (61/61), done.\u001b[K\n","remote: Total 7335 (delta 40), reused 39 (delta 19), pack-reused 7255\u001b[K\n","Receiving objects: 100% (7335/7335), 13.88 MiB | 25.76 MiB/s, done.\n","Resolving deltas: 100% (4939/4939), done.\n","/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-9vsezpbd\n","Created temporary directory: /tmp/pip-req-tracker-kki46dta\n","Created requirements tracker '/tmp/pip-req-tracker-kki46dta'\n","Created temporary directory: /tmp/pip-install-v7nmp3bc\n","Processing ./apex\n","  Created temporary directory: /tmp/pip-req-build-gj0l4clq\n","  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-kki46dta'\n","    Running setup.py (path:/tmp/pip-req-build-gj0l4clq/setup.py) egg_info for package from file:///content/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-gj0l4clq/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-gj0l4clq/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-gj0l4clq/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-gj0l4clq/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-gj0l4clq/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-gj0l4clq/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-gj0l4clq/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-gj0l4clq has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n","  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-kki46dta'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-90v52w_g\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-gj0l4clq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-gj0l4clq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-90v52w_g/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    /tmp/pip-req-build-gj0l4clq/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda-10.1/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    creating /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-90v52w_g/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-gj0l4clq\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-kki46dta'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"665tCOfRpIIx","colab_type":"code","colab":{}},"source":["import torch.random\n","import random\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqnYi7N2Kfjy","colab_type":"code","colab":{}},"source":["zip_name = 'gwd-split-0.7.zip'\n","zip_path = '/content/drive/My Drive/Colab Notebooks/gwdsplit/' + zip_name\n","!cp \"{zip_path}\" .\n","!unzip -q '{zip_name}'\n","!rm '{zip_name}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"978lsjD2vQAF","colab_type":"code","colab":{}},"source":["yolov5_name = 'yolov5.zip'\n","yolov5_path = '/content/drive/My Drive/Colab Notebooks/' + yolov5_name\n","!cp '{yolov5_path}' .\n","!unzip -q '{yolov5_name}'\n","!rm '{yolov5_name}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROkeoZzd_ljI","colab_type":"code","colab":{}},"source":["weight_name = 'last_x-b4-e25-coco.pt'\n","weight_path = '/content/drive/My Drive/Colab Notebooks/yolov5weights/x-b4-e25-coco/' + weight_name\n","!cp '{weight_path}' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1l1XG4zejqR","colab_type":"code","colab":{}},"source":["train_input = '/content/drive/My Drive/Colab Notebooks/yolov5/train.py'\n","data_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/wheat_colab.yaml'\n","cfg_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/yolov5x.yaml'\n","weights_input = '/content/' + weight_name\n","name_input = 'x-b4-e50-coco'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-Sb84dpxl6U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594276724291,"user_tz":-540,"elapsed":367101,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"e4baefbe-bb36-40a2-9ada-5a55051d991b"},"source":["%cd /content/yolov5/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/yolov5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lYOIA2YaeNgC","colab_type":"text"},"source":["# **train.py**"]},{"cell_type":"code","metadata":{"id":"9UCW99OCx4QA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1594276731142,"user_tz":-540,"elapsed":370871,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"3d16ad0a-0464-4e2a-e649-6a2cb370051e"},"source":["import argparse\n","\n","import torch.distributed as dist\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.utils.data\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import test  # import test.py to get mAP after each epoch\n","from models.yolo import Model\n","from utils import google_utils\n","from utils.datasets import *\n","from utils.utils import *\n","\n","mixed_precision = True\n","try:  # Mixed precision training https://github.com/NVIDIA/apex\n","    from apex import amp\n","except:\n","    print('Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex')\n","    mixed_precision = False  # not installed\n","\n","wdir = 'weights' + os.sep  # weights dir\n","os.makedirs(wdir, exist_ok=True)\n","last = wdir + 'last.pt'\n","best = wdir + 'best.pt'\n","results_file = 'results.txt'\n","\n","# Hyperparameters\n","hyp = {'lr0': 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","       'momentum': 0.937,  # SGD momentum\n","       'weight_decay': 5e-4,  # optimizer weight decay\n","       'giou': 0.05,  # giou loss gain\n","       'cls': 0.58,  # cls loss gain\n","       'cls_pw': 1.0,  # cls BCELoss positive_weight\n","       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)\n","       'obj_pw': 1.0,  # obj BCELoss positive_weight\n","       'iou_t': 0.20,  # iou training threshold\n","       'anchor_t': 4.0,  # anchor-multiple threshold\n","       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n","       'hsv_h': 0.014,  # image HSV-Hue augmentation (fraction)\n","       'hsv_s': 0.68,  # image HSV-Saturation augmentation (fraction)\n","       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n","       'degrees': 0.0,  # image rotation (+/- deg)\n","       'translate': 0.0,  # image translation (+/- fraction)\n","       'scale': 0.5,  # image scale (+/- gain)\n","       'shear': 0.0}  # image shear (+/- deg)\n","print(hyp)\n","\n","# Overwrite hyp with hyp*.txt (optional)\n","f = glob.glob('hyp*.txt')\n","if f:\n","    print('Using %s' % f[0])\n","    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n","        hyp[k] = v\n","\n","# Print focal loss if gamma > 0\n","if hyp['fl_gamma']:\n","    print('Using FocalLoss(gamma=%g)' % hyp['fl_gamma'])\n","\n","\n","def train(hyp):\n","    epochs = opt.epochs  # 300\n","    batch_size = opt.batch_size  # 64\n","    weights = opt.weights  # initial training weights\n","\n","    # Configure\n","    init_seeds(1)\n","    with open(opt.data) as f:\n","        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n","    train_path = data_dict['train']\n","    test_path = data_dict['val']\n","    nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes\n","\n","    # Remove previous results\n","    for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):\n","        os.remove(f)\n","\n","    # Create model\n","    model = Model(opt.cfg, nc=data_dict['nc']).to(device)\n","\n","    # Image sizes\n","    gs = int(max(model.stride))  # grid size (max stride)\n","    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n","\n","    # Optimizer\n","    nbs = 64  # nominal batch size\n","    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n","    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n","    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n","    for k, v in model.named_parameters():\n","        if v.requires_grad:\n","            if '.bias' in k:\n","                pg2.append(v)  # biases\n","            elif '.weight' in k and '.bn' not in k:\n","                pg1.append(v)  # apply weight decay\n","            else:\n","                pg0.append(v)  # all else\n","\n","    optimizer = optim.Adam(pg0, lr=hyp['lr0']) if opt.adam else \\\n","        optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n","    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n","    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n","    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n","    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n","    print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n","    del pg0, pg1, pg2\n","\n","    # Load Model\n","    google_utils.attempt_download(weights)\n","    start_epoch, best_fitness = 0, 0.0\n","    if weights.endswith('.pt'):  # pytorch format\n","        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","\n","        # load model\n","        try:\n","            ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()\n","                             if model.state_dict()[k].shape == v.shape}  # to FP32, filter\n","            model.load_state_dict(ckpt['model'], strict=False)\n","        except KeyError as e:\n","            s = \"%s is not compatible with %s. This may be due to model differences or %s may be out of date. \" \\\n","                \"Please delete or update %s and try again, or use --weights '' to train from scratch.\" \\\n","                % (opt.weights, opt.cfg, opt.weights, opt.weights)\n","            raise KeyError(s) from e\n","\n","        # load optimizer\n","        if ckpt['optimizer'] is not None:\n","            optimizer.load_state_dict(ckpt['optimizer'])\n","            best_fitness = ckpt['best_fitness']\n","\n","        # load results\n","        if ckpt.get('training_results') is not None:\n","            with open(results_file, 'w') as file:\n","                file.write(ckpt['training_results'])  # write results.txt\n","\n","        # epochs\n","        start_epoch = ckpt['epoch'] + 1\n","        if epochs < start_epoch:\n","            print('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\n","                  (opt.weights, ckpt['epoch'], epochs))\n","            epochs += ckpt['epoch']  # finetune additional epochs\n","\n","        del ckpt\n","\n","    # Mixed precision training https://github.com/NVIDIA/apex\n","    if mixed_precision:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n","\n","\n","    scheduler.last_epoch = start_epoch - 1  # do not move\n","    # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822\n","    # plot_lr_scheduler(optimizer, scheduler, epochs)\n","\n","    # Initialize distributed training\n","    if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n","        dist.init_process_group(backend='nccl',  # distributed backend\n","                                init_method='tcp://127.0.0.1:9999',  # init method\n","                                world_size=1,  # number of nodes\n","                                rank=0)  # node rank\n","        model = torch.nn.parallel.DistributedDataParallel(model)\n","        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","    # Trainloader\n","    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\n","                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect)\n","    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n","    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Correct your labels or your model.' % (mlc, nc, opt.cfg)\n","\n","    # Testloader\n","    testloader = create_dataloader(test_path, imgsz_test, batch_size, gs, opt,\n","                                   hyp=hyp, augment=False, cache=opt.cache_images, rect=True)[0]\n","\n","    # Model parameters\n","    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\n","    model.nc = nc  # attach number of classes to model\n","    model.hyp = hyp  # attach hyperparameters to model\n","    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n","    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n","    model.names = data_dict['names']\n","\n","    # Class frequency\n","    labels = np.concatenate(dataset.labels, 0)\n","    c = torch.tensor(labels[:, 0])  # classes\n","    # cf = torch.bincount(c.long(), minlength=nc) + 1.\n","    # model._initialize_biases(cf.to(device))\n","    if tb_writer:\n","        plot_labels(labels)\n","        tb_writer.add_histogram('classes', c, 0)\n","\n","    # Check anchors\n","    if not opt.noautoanchor:\n","        check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n","\n","    # Exponential moving average\n","    ema = torch_utils.ModelEMA(model)\n","\n","    # Start training\n","    t0 = time.time()\n","    nb = len(dataloader)  # number of batches\n","    n_burn = max(3 * nb, 1e3)  # burn-in iterations, max(3 epochs, 1k iterations)\n","    maps = np.zeros(nc)  # mAP per class\n","    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n","    print('Image sizes %g train, %g test' % (imgsz, imgsz_test))\n","    print('Using %g dataloader workers' % dataloader.num_workers)\n","    print('Starting training for %g epochs...' % epochs)\n","    # torch.autograd.set_detect_anomaly(True)\n","    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n","        model.train()\n","\n","        # Update image weights (optional)\n","        if dataset.image_weights:\n","            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n","            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n","            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n","\n","        # Update mosaic border\n","        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n","        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n","\n","        mloss = torch.zeros(4, device=device)  # mean losses\n","        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n","        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n","        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n","            ni = i + nb * epoch  # number integrated batches (since train start)\n","            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n","\n","            # Burn-in\n","            if ni <= n_burn:\n","                xi = [0, n_burn]  # x interp\n","                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n","                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n","                for j, x in enumerate(optimizer.param_groups):\n","                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n","                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n","                    if 'momentum' in x:\n","                        x['momentum'] = np.interp(ni, xi, [0.9, hyp['momentum']])\n","\n","            # Multi-scale\n","            if opt.multi_scale:\n","                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n","                sf = sz / max(imgs.shape[2:])  # scale factor\n","                if sf != 1:\n","                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n","                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n","\n","            # Forward\n","            pred = model(imgs)\n","\n","            # Loss\n","            loss, loss_items = compute_loss(pred, targets.to(device), model)\n","            if not torch.isfinite(loss):\n","                print('WARNING: non-finite loss, ending training ', loss_items)\n","                return results\n","\n","            # Backward\n","            if mixed_precision:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            # Optimize\n","            if ni % accumulate == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                ema.update(model)\n","\n","            # Print\n","            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n","            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n","            s = ('%10s' * 2 + '%10.4g' * 6) % (\n","                '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n","            pbar.set_description(s)\n","\n","            # Plot\n","            if ni < 3:\n","                f = 'train_batch%g.jpg' % ni  # filename\n","                result = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n","                if tb_writer and result is not None:\n","                    tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n","                    # tb_writer.add_graph(model, imgs)  # add model to tensorboard\n","\n","            # end batch ------------------------------------------------------------------------------------------------\n","\n","        # Scheduler\n","        scheduler.step()\n","\n","        # mAP\n","        ema.update_attr(model)\n","        final_epoch = epoch + 1 == epochs\n","        if not opt.notest or final_epoch:  # Calculate mAP\n","            results, maps, times = test.test(opt.data,\n","                                             batch_size=batch_size,\n","                                             imgsz=imgsz_test,\n","                                             save_json=final_epoch and opt.data.endswith(os.sep + 'coco.yaml'),\n","                                             model=ema.ema,\n","                                             single_cls=opt.single_cls,\n","                                             dataloader=testloader)\n","\n","        # Write\n","        with open(results_file, 'a') as f:\n","            f.write(s + '%10.4g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n","        if len(opt.name) and opt.bucket:\n","            os.system('gsutil cp results.txt gs://%s/results/results%s.txt' % (opt.bucket, opt.name))\n","\n","        # Tensorboard\n","        if tb_writer:\n","            tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n","                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/F1',\n","                    'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n","            for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n","                tb_writer.add_scalar(tag, x, epoch)\n","\n","        # Update best mAP\n","        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n","        if fi > best_fitness:\n","            best_fitness = fi\n","\n","        # Save model\n","        save = (not opt.nosave) or (final_epoch and not opt.evolve)\n","        if save:\n","            with open(results_file, 'r') as f:  # create checkpoint\n","                ckpt = {'epoch': epoch,\n","                        'best_fitness': best_fitness,\n","                        'training_results': f.read(),\n","                        'model': ema.ema,\n","                        'optimizer': None if final_epoch else optimizer.state_dict()}\n","\n","            # Save last, best and delete\n","            torch.save(ckpt, last)\n","            if (best_fitness == fi) and not final_epoch:\n","                torch.save(ckpt, best)\n","            del ckpt\n","\n","        # end epoch ----------------------------------------------------------------------------------------------------\n","    # end training\n","\n","    # Strip optimizers\n","    n = ('_' if len(opt.name) and not opt.name.isnumeric() else '') + opt.name\n","    fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n","    for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n","        if os.path.exists(f1):\n","            os.rename(f1, f2)  # rename\n","            ispt = f2.endswith('.pt')  # is *.pt\n","            strip_optimizer(f2) if ispt else None  # strip optimizer\n","            os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n","\n","    # Finish\n","    if not opt.evolve:\n","        plot_results()  # save as results.png\n","    print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n","    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\n","    torch.cuda.empty_cache()\n","    return results"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9csFml3nfP7H","colab_type":"text"},"source":["# **Train Option**"]},{"cell_type":"code","metadata":{"id":"h0uu9IpAyJC1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594294142086,"user_tz":-540,"elapsed":17410920,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"51c6fb0c-1955-4107-b71f-0fe5f788d81e"},"source":["check_git_status()\n","class opt:\n","    epochs=50                #parser.add_argument('--epochs', type=int, default=300)\n","    batch_size=4            #parser.add_argument('--batch-size', type=int, default=16)\n","    cfg=cfg_input           #parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='*.cfg path')\n","    data=data_input         #parser.add_argument('--data', type=str, default='data/coco128.yaml', help='*.data path')\n","    img_size=[1024, 1024]   #parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')\n","    rect=False              #parser.add_argument('--rect', action='store_true', help='rectangular training')\n","    resume=True            #parser.add_argument('--resume', action='store_true', help='resume training from last.pt')\n","    nosave=False            #parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n","    notest=False            #parser.add_argument('--notest', action='store_true', help='only test final epoch')\n","    noautoanchor=False      #parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n","    evolve=False            #parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\n","    bucket=''               #parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n","    cache_images=False      #parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n","    weights=weights_input   #parser.add_argument('--weights', type=str, default='', help='initial weights path')\n","    name=name_input         #parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')\n","    device=''               #parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    adam=False              #parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n","    multi_scale=False       #parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%')\n","    single_cls=False        #parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\n","\n","#parser = argparse.ArgumentParser()\n","#opt = parser.parse_args()\n","\n","opt.weights = last if opt.resume and not opt.weights else opt.weights\n","opt.cfg = check_file(opt.cfg)  # check file\n","opt.data = check_file(opt.data)  # check file\n","print(opt)\n","opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\n","device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n","if device.type == 'cpu':\n","    mixed_precision = False\n","\n","# Train\n","if not opt.evolve:\n","    tb_writer = SummaryWriter(comment=opt.name)\n","    print('Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/')\n","    train(hyp)\n","\n","# Evolve hyperparameters (optional)\n","else:\n","    tb_writer = None\n","    opt.notest, opt.nosave = True, True  # only test/save final epoch\n","    if opt.bucket:\n","        os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n","\n","    for _ in range(10):  # generations to evolve\n","        if os.path.exists('evolve.txt'):  # if evolve.txt exists: select best hyps and mutate\n","            # Select parent(s)\n","            parent = 'single'  # parent selection method: 'single' or 'weighted'\n","            x = np.loadtxt('evolve.txt', ndmin=2)\n","            n = min(5, len(x))  # number of previous results to consider\n","            x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n","            w = fitness(x) - fitness(x).min()  # weights\n","            if parent == 'single' or len(x) == 1:\n","                # x = x[random.randint(0, n - 1)]  # random selection\n","                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n","            elif parent == 'weighted':\n","                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n","\n","            # Mutate\n","            mp, s = 0.9, 0.2  # mutation probability, sigma\n","            npr = np.random\n","            npr.seed(int(time.time()))\n","            g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n","            ng = len(g)\n","            v = np.ones(ng)\n","            while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n","                v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n","            for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n","                hyp[k] = x[i + 7] * v[i]  # mutate\n","\n","        # Clip to limits\n","        keys = ['lr0', 'iou_t', 'momentum', 'weight_decay', 'hsv_s', 'hsv_v', 'translate', 'scale', 'fl_gamma']\n","        limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n","        for k, v in zip(keys, limits):\n","            hyp[k] = np.clip(hyp[k], v[0], v[1])\n","\n","        # Train mutation\n","        results = train(hyp.copy())\n","\n","        # Write mutation results\n","        print_mutation(hyp, results, opt.bucket)\n","\n","        # Plot results\n","        # plot_evolution_results(hyp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class '__main__.opt'>\n","Using CUDA Apex device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB)\n","\n","Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n","  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n","  2                -1  1    315680  models.common.BottleneckCSP             [160, 160, 4]                 \n","  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n","  4                -1  1   3311680  models.common.BottleneckCSP             [320, 320, 12]                \n","  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n","  6                -1  1  13228160  models.common.BottleneckCSP             [640, 640, 12]                \n","  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n","  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n","  9                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1   5435520  models.common.BottleneckCSP             [1280, 640, 4, False]         \n"," 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1   1360960  models.common.BottleneckCSP             [640, 320, 4, False]          \n"," 18                -1  1      5778  torch.nn.modules.conv.Conv2d            [320, 18, 1, 1]               \n"," 19                -2  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n"," 20          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 21                -1  1   5025920  models.common.BottleneckCSP             [640, 640, 4, False]          \n"," 22                -1  1     11538  torch.nn.modules.conv.Conv2d            [640, 18, 1, 1]               \n"," 23                -2  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n"," 24          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 25                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 26                -1  1     23058  torch.nn.modules.conv.Conv2d            [1280, 18, 1, 1]              \n"," 27      [-1, 22, 18]  1         0  models.yolo.Detect                      [1, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n","Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n","\n","Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n"],"name":"stdout"},{"output_type":"stream","text":["Reading image shapes: 100%|██████████| 2361/2361 [00:00<00:00, 12750.61it/s]\n","Caching labels /content/labels/train (2361 found, 0 missing, 0 empty, 0 duplicate, for 2361 images): 100%|██████████| 2361/2361 [00:00<00:00, 4657.42it/s]\n","Reading image shapes: 100%|██████████| 1012/1012 [00:00<00:00, 15524.18it/s]\n","Caching labels /content/labels/valid (463 found, 0 missing, 0 empty, 0 duplicate, for 1012 images):  46%|████▌     | 463/1012 [00:00<00:00, 4618.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saving labels to /content/labels/train.npy for faster future loading\n"],"name":"stdout"},{"output_type":"stream","text":["Caching labels /content/labels/valid (1012 found, 0 missing, 0 empty, 0 duplicate, for 1012 images): 100%|██████████| 1012/1012 [00:00<00:00, 4642.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Saving labels to /content/labels/valid.npy for faster future loading\n","\n","Analyzing anchors... Best Possible Recall (BPR) = 0.9991\n","Image sizes 1024 train, 1024 test\n","Using 2 dataloader workers\n","Starting training for 50 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     25/49     11.5G   0.04112    0.1452         0    0.1863        54      1024: 100%|██████████| 591/591 [09:04<00:00,  1.09it/s]\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:33<00:00,  1.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.734       0.941       0.937       0.483\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     26/49     11.5G   0.03499    0.1349         0    0.1699       148      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.752       0.944       0.941       0.519\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     27/49     11.5G   0.03426    0.1334         0    0.1677        42      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.749        0.95       0.946       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     28/49     11.5G   0.03384    0.1327         0    0.1665        58      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.757       0.947       0.947        0.53\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     29/49     11.5G   0.03356    0.1319         0    0.1654       128      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.763       0.947       0.946        0.53\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     30/49     11.5G   0.03337    0.1292         0    0.1625        42      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.773       0.945       0.945       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     31/49     11.5G   0.03314    0.1295         0    0.1626        51      1024: 100%|██████████| 591/591 [08:58<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.775       0.945       0.945       0.532\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     32/49     11.5G   0.03317    0.1289         0    0.1621        37      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.771       0.947       0.945       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     33/49     11.5G   0.03302    0.1301         0    0.1631        48      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:29<00:00,  1.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.764       0.947       0.946       0.536\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     34/49     11.5G   0.03277    0.1279         0    0.1607        74      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.776       0.944       0.945       0.533\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     35/49     11.5G   0.03255     0.127         0    0.1596        30      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.776       0.945       0.946       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     36/49     11.5G   0.03247    0.1284         0    0.1608       108      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.771       0.945       0.944       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     37/49     11.5G   0.03227     0.126         0    0.1583       106      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.779       0.944       0.945       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     38/49     11.5G   0.03233    0.1271         0    0.1594       156      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.782       0.945       0.945       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     39/49     11.5G   0.03228    0.1277         0      0.16       111      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.777       0.945       0.945       0.535\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     40/49     11.5G   0.03206    0.1259         0    0.1579        85      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.781       0.945       0.945       0.533\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     41/49     11.5G   0.03202    0.1245         0    0.1566        55      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.783       0.944       0.946       0.535\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     42/49     11.5G   0.03178    0.1243         0    0.1561        16      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.782       0.944       0.944       0.535\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     43/49     11.5G   0.03193    0.1237         0    0.1556        17      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.782       0.944       0.945       0.533\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     44/49     11.5G   0.03169    0.1225         0    0.1542        20      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:32<00:00,  1.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.791       0.942       0.943       0.533\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     45/49     11.5G   0.03172    0.1252         0    0.1569        78      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:32<00:00,  1.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.783       0.945       0.944       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     46/49     11.5G   0.03163    0.1243         0    0.1559        90      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.784       0.942       0.943       0.532\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     47/49     11.5G   0.03153    0.1239         0    0.1555       102      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:31<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.783       0.943       0.944       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     48/49     11.5G   0.03154    0.1241         0    0.1556        53      1024: 100%|██████████| 591/591 [08:59<00:00,  1.09it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.782       0.944       0.944       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     49/49     11.5G   0.03152    0.1234         0    0.1549        29      1024: 100%|██████████| 591/591 [08:59<00:00,  1.10it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 253/253 [02:30<00:00,  1.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    1.01e+03    4.38e+04       0.785       0.942       0.943       0.533\n","Optimizer stripped from weights/last_x-b4-e50-coco.pt\n","Optimizer stripped from weights/best_x-b4-e50-coco.pt\n","25 epochs completed in 4.827 hours.\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA1YAAAGmCAYAAAB/URVbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeXxcZfX/32cme9O96UablnRh7QJtQRCpBcoqoCIga4FCRVFUENn8CrII/sAiQqUsBSpgEQFBpYgVKYrSDSmlCKVt2qYLJWnapM2ezJzfH/feyc1kkk72yeS8X6+8cufe5977TObJnec855zPEVXFMAzDMAzDMAzDaD2Bru6AYRiGYRiGYRhGd8cMK8MwDMMwDMMwjDZihpVhGIZhGIZhGEYbMcPKMAzDMAzDMAyjjZhhZRiGYRiGYRiG0UbMsDIMwzAMwzAMw2gjZlgZRpIjIl8WkWbrKojI6yJyS2f1yeiZiMjtIrK0E+7zkYhc5Hs9RURWi8g+EXlaRC4SkY86sw+G0VJEJFdEykQkN462t4jI653RL8NoL6K/E0RkqYjc3nU9ajtmWHUyIjJJRH4vIjtFpFxECkRksYh8zT3eoolHU4PQnTw83W4dNxIWd9L4JxHZLSIVIvKx+yWbGu81VPU0Vf15O/XnMhHZ3B7XMrofIjJRRF5wn3FlIpIvIr8VkcM7qw+qepiqPufbdQ+wVFV7q+plqvqcqh7WHvcSkdEioiIyej99MJIA9zu3xh3be0VkrYjM7oh7qWqBqmarakEcbX+uqqd1RD+M5CbGmP5IRK7q6n51V8yw6kRE5ERgGbAd+ALQGzgIeAj4ehd2zeimiMgJwDvA/4BDgX7At4DLgFdExP7HjU5DRL4MLMd5xh2N84ybCvwbOLvrekYesLoL728kFz9X1WygP3Av8IQ79hvQksUtw+hivDHdD/gZ8KiIHN/FfeqW2KSrc5kPLFLV61R1s6qGVbVSVV9X1UtinSAiA0TkSRHZISKFIvKSiIzo5H4bicsjwEuqepOq7lTVGlX9J84k9mTgPK+hiFwoIptEpEREXhaRHN+xBp5PETlARH4nItvdcbcoqn2WiNwjIhvc8Kr1InKOiHwJZ5x7ISxlIvJV36r+xSKyxj3nPyJysO+aQRG53vW4lYrIe+5ihHd8koi87fZ/j3v8IPfYDBFZ5Z5XLCL/FpH+HfIXN5rjUeAFVf2hqm5Rh92q+qiq3h3dWESucVdH97ljbZ6IZPmOn+ce3ysiu0Tk775j3xWRje65n/s99CKy2fWcBkWkDMewmu+Ox3OivaoikiIiN7hjb5+IbBGRa9xjw0TkNff/YK+IrHQXNDy8kMKP3Ov/0t8H3z2Oc8d8ift/c5OIBH3HVUS+47Ypc/9Pjm3tB2F0PKoaUtVngWJgivsZfl9ElotIBXCKiGSIyM/dsbpHRP4pIkf4ryMil4vIB+7z6zMRucvd38Abup9nYHRIVbNzB3GiWp4TkYfdZ+ZO6eYhWEbbceelLwC7gaMARORoceYIxe6z8U4RSfHOESdkdZH7DC91v4u9cXmuiPzX3f+5O+YGdc276xzMsOokRGQ8MBb4XQtPfRY4AJgIjAEqgD/5v5CNnok7psYDT0cfU9WPgRXAV3y7zweOBA4EMoDfNnHddOBNYKt7/TygjoZjdwEwAzhdVXsDJwDrVfVfwNWAF8KSraqv+M67BJgJ5AA7gXm+Y/8HXIRjFPYH7gJeFZEx7vHfuP0a5J4/Gyhxjz3rXqsfMAz4EVAT6/0ZHYOIjMMZL8+04LTPcD7vPsCJOIsBt7rXy8L5XL+nqn2AEcDPfff6f8DZ7vgbAzwZfXF34psNFABXu+PxpRj9uBO4CrjY7ctUYKV7LAg8gfN/Mwh4Ffijb3LghRQe5l7/+uiLi8go4G84/3M5OBEK3wG+H9X0Spz/kX7A27Tsb2l0Mq5BfgkwgPrx8i1gFtAL53k1H5gCHI/z2f8eeENE+rnX+BaO1+uH7nUOBv7axC2bewZGE8/c4RyccTbY3b5VnMUxo4fijukLgYHAOtdA+jvO9+sQnHF8JnCj2z4L+AdQjjPW+uOMy33uJffh/D8MwPk/yAMe7Kz30xWYYdV5eKv9270d7gpmiWvJV7lfvviODwNOA36oqrtUdR/wXWASMK2zOm4kLI3GVBTbcL4wPW5S1T2quge4HjjVHWPRnAFkue3LVbUMx1A5SURGiOO5+ibORPVTAFXdqqpr4ujzz1T1c1WtwpkIH+U79kPgBlX91F01+yPwL+AC93gNkAuMUtU6VV2tqp/7jo0Bhrteu3dVtTyO/hjthzfWmhqPjVDVl1V1g+vZ+gRn4niSr0ktcIiIDFLVKlX9h7u/DhDgMBHpo6plrqe2xYiI4DxXf6yq77l9KVLVFW4ft6nqH93/hRpVvQtQWvYMvhBYq6rzVbXW/V/5f8CcqHb3q+pGVa3D8f7licjA1rwvo0O5SURKcBaHfgBc5ht/v1TVT1RVcZ6js4DvqOp297k1D8fD5S16XQvco6r/cBcCSlX1nSbu29wzMEIL5g7/VNU/uPf9N/ABDZ/JRs/BG9NVOAs6t6jqn4FrgFfccVKnqltwclYvd887A2cx6tuqWux+d3+gqjsAVPWvqvqhO8a24Tz3Toq+eTJhhlXnUeT+PsDboarvqGo/nIddOs5Ewc9I93e+75xS91qeSlAtECuOO9U9ZiQvjcZUFCOAQt/rTTG2R9KYccBwYI9r+JcA64BqnHE32m23rhV93uHbLgOyAURkCM7D+Y/ePd37Hk/9+7sMZ0L7DxHZKiIPiEgv99hZOCth74kTlnibeXU7HW+sNTUeGyEi3xCRZeKE+ZUCd+MaaKpaAZyK8yW8zg2N+657bBOOcX85UOCGXp0X+y77ZRDOOIw5nn0hVZvFCQUswRmrg2O1b4KR+J7jLhuof457RP9/gJOnZiQW96pqP1UdpKpTVNXv/fc/Z8e6v9+Leq6Nwnk+g/M8jfdZehlNPwP9xDN3gIbjDZwxZ+OtZ3KvOx/tDzyFs5CagjMfODdq/D4ODHXPGw1sUtWY801xwvSXumGAe3GMtpY8O7sdZlh1Eu7K/kaclct42er+PtDbISJ9cCYCnkrQJpyBH804935GkuKOqQ3ApdHHXPf9UcBrvt2jY2xvi3HpnUC+O3Hw/2So6n+AzW678U10LRz3m6jHWyk7NeqevVT12wDq5OxcpaqjcMIQTwZ+7B77UFUvVNWhwLk4YVaN/i5Gx6Gq64FPccI594ub7/F74H7gAFXtixMGGFlgUtV/qerXcJ551wL3i8gM99irqnqqe+yXwCJf2GhL2IUzoWxqPN+L8wz+ItAXZ+Kx19fPeMb7VnzPcZcx1D/HjeTBPx52ur8PjXquZanqve6xzTQ99hrQ3DMwinjmDobRCNe7eQ3O2LkGZwz/Nmr89lEnxBqc8XugxBBqEZE04M/AK0CeOiHdMfUEkgkzrDqX7wAXisgvRWSUiATcfJbjYjVW1c9wYq3nisggEcnGURD8iPp47oXA2eIkZKeJSKY4SdeH4UxajOTmO8B54iRHDxGRVBE5DicP5E3gBV/be0SkvziiDvcBf/Pc9VG8DGSIkwzdF0BEBovI+QCqWgQsAn7j5rrghghOdM/fCeRIC8QjVLUaJxfhPhE5RBwyReR4N5fMk3Ef4YZu7cUJBwu54/5yqRfXKAVC7o/RuXwLOF9E7hMnoVlEpJ+IzJbGddJ643wH7VLVanf8XOMdFJGh4iQ+93PDqkpwVutDInKQiJwuItlu2FwpjqHT4s/cvfZDwC9E5Ai3zzki4oVM9QUqgT04uYl34XpaXYpwJtMHNXObRcAEEZnj/o8ejjMhfqKl/TW6D27Y1Cs4z8pRACLSW0ROk/ow7AeBm0VkujhiK33dZ3gjmnoGxrhvPHMHw4iJ+318B/ATnBzu83xzzKCIjBWRU93mf8EZiw+73v2AOCU3hgNpOM/MElUtF5E84KbOf0edixlWnYiq/g04FscVvwInqW89zgrvV4EtMU67GPgc+BDHO9UbOFNVQ+41/w18A7gBZ0JbgJMYfZIbLmMkMaq6BPgSMAH4BOcBtwAncfksb5y4/AH4L84KUx1NrBy5K1bH4KxYfei67/+DE5bncRWOhPYb4qiuvUV92Ms/cDxlG9zQgbPifDs/wjEE/4Azid4M3Ex9qOsMnP+bMpxcgHdxDERw/gc+EpFynGTsp92/gdGJqOpSnLEzCliF84x7H2eMvhLV9mOcL+7fu2PsfhoKqgiOEEq+O8ZexIn7/yfOF/atwHb33F8Cl6jq5lZ2/ac4Y+Z5t8+rcAQswBFV6YtjQK3DeR5HPL2qWgncAix0x/v/i764269TcUIXd+EsfDwGPNDK/hrdhwtxpP6XiMg+nDF0Fa7HU1Ufw/k/eBjnufcJcEoT12ruGRhNs3MHw9gPz+AoA56EMx6/hZM/W4zzLB4FkeffiTie/P/hLEA9CWSrk5/9LeAO9xn+nPuT1IizWGcYRk9GRP4FvK7tVCTYMAzDMAyjp2EeK8Po4bix92NxvKeGYRiGYRhGKzDDyjB6MCJyDE6i8z+JCtUyDMMwDMMw4sdCAQ3DMAzDMAzDMNqIeawMwzAMwzAMwzDaiBlWhmEYhmEYhmEYbSSlqzvQGgYNGqSjR4/u6m4YHcB77723S1Vz9t+ye2BjNXmxsWp0F2ysGt0FG6tGd6C5cdotDavRo0ezatWqru6G0QGISKxaXt0WG6vJi41Vo7vQFWPVLey9EBiIU/vmUlVdH9VmMPAUMBKnXt1bwLVu0ecmsbGavNhz1egONDdOLRTQMAzDMIz2Zj4wT1XHA/OAR2O0uQX4WFUnAhOBKTgF7g3DMLolZlgZhmEYhtFuuJ6oI4FF7q5FwJEiEh06o0BvEQkA6UAasL3TOmoYhtHOmGFlGIZhGEZ7MhLYrqohAPf3Dne/nzuB8cBnwE7gDVX9d6wLisgcEVklIquKioo6rueGYRhtoFvmWEVTUFzB7IUryS8qJy+nFwtmTSN3YFZXd8swGuGN1V17K7jhSzlMGJpFSkC6ultGK1iyZMmEDz74YHNX96MVhIG1dXV1V06ZMqWwqztj9GjOBdYAJwK9gddF5Buq+mJ0Q1V9DHgMYOrUqVaAM4GwOVjrsb9d8pEUhtUlC5azZXcFABuLypi9cCVLrpvexb0yjMZ88/F32VFSxa3HD+SgEYNJ69OPg4b16epuGa0gFArVHX744bu6uh8tJRwOS1FR0aE7d+58Ajirq/tjJCVbgQNEJKiqIREJAsPd/X6+B1yhqmGgVEReBWYAjQwrI3GZvXAl6wvLANhgc7AWcdlTK8jfVQ607/x1S3E533xsGZ/vrWJMTrYZbJ1IUhhWW/dURLbDCvlF5V3YG8Nomp2lVQCM6pdKSlZvakK28Gp0LoFAQHNyckp37tx5eFf3xUhOVLVQRFYDFwDPur/fV9XoGL5NwKnAChFJA04CXu7Uzhpx05R3ZYNrVAGowvrCMr5831sAbN1d2aBtQXEFVyxcSX5RmU34gc3F9fPV9py/Xvj4cj5z5xvmcOhcksKwyh2QxeZix7gKCOTl9OriHhlGbIb1zWB7SRWCEBAhPcXSHI3OJxAIKJZja3QsVwMLReSnwB7gUgARWQz8VFVXAT8A5ovIh0AQR2798S7qrwGu4bOCTUUV5OX04s6zD+f/Xl1LflE5IlAXdhYD1xeWcfx9bxEUR4EkGm9O5m+bEpDI+WDeLYAhfTIiBlB7zl93lFZGts3h0LkkhWH10AVHcObDTr6rtwJiGInILWccyjXP/ReA9JQgowb13JU6wzCSF1X9BDg6xv7TfdsbgZmd2S8jtucJ4MInlrFtT/2EfH1hGd98fFn9iTEsqJYEXfiNKnC8Wz19wn/FF0dz9+JPgNbPX2N9nn0zUympqAVAzOHQqSSFYTV2cG8A0lMCPXrlw0h8Rg1wDKm0oDB+aO8u7o1hGIbRmTQnVtAWIQP/uSMHZKLAthhheJc/vYKNPmPGCxMrraylcF91m99fQBwPSbxte/qEv1d6KgDBgPDGD44n0Aoxq9kLV7KhsAyl/vM8YmQ/3lrnRN7mZKfHNNhMOKNjSIpQEC+cqrouTDje/2jD6AK8sdqTRunmzZsZNGhQ5PXtt99OTU1Nh9939OjRrF27ttk2CxcuRET4y1/+0qp7iMiU8ePHH3rwwQcfevDBBx+6YsWKTO/Y7373u74HHnjgYbm5uYefccYZefv27UuK561hGK3HE3oIqbKh0JkEe1z21Ir6Y0UNj8Vz3Q1FzrmbiyvYUlxBSDUShjdz7ttcsmB5A6MKHCNofWFZXEaVAKnB2BP/gMDogVmMyckmKMLogVmM3s8kvTMjjERkvIi8KyKfur/HxWgzVEReFZE1IvKxiFzsO3a7iBSKyGr3Z1579KuyNgRAKKzsq6pr0bkFxRXMnPs2612jCurD/naX13/HnnDw4AbG+4z7l3LgTa8x/b63IuNtYzPjzbvPmJsXM3Pu2xT4wjzj6V9Lz+vuJIXHKhBwclWq68JU14XJTAt2dZcMIyZpnmHVCsuqs1aX6urqSEnpuEfDz372M370ox+RlpbWYfeIh23btvHoo4/yhS98oU3XWbly5Sd9+/YN+/eVlpYGrr322tFvvfXWJxMmTKg+//zzR/3sZz8bcv/993/WppsZhtGt2VjkE3rAMWrybnqNYFCo9cXVeSIQM+e+HdezPr+ovNnvlY1FZXF7kppi7OB6Q2j2wpVsLCwjGBRCYW1SiML73oqnbQczH5inqs+6BtOjwAlRbeYCq1T1bLeY9Xsi8raqemqWv1XVH7Vnp6pcwwpgT0UNfbNS4z7X81T5ERwvoD+k8/2Cksj25U+vYNOuxuGXzeVh+T1i3mJAPNFhVyxcwcbC8gaetJ4QVZYUhhVAZlqQ6rowVbUhM6yMhCU9xRmb/u+30Te91uLreKuQ8bD53jP220ZEuO2223jttdc49dRTueGGG7juuutYs2YNVVVVzJgxg7lz5xIMBvnZz37GokWLyMjIQER46623KCkpYerUqeza5aiPb968ucFrj2uuuQaAY489lkAgwNKlS3nhhRd44IEHSE9PJxwO88ILL3DwwQfH7Oddd93Ff//7X15++WUqKio4+uij+cUvfsHpp58es/2zzz7LkiVLKC0t5Qc/+AHf/e53I8fmzJnDAw88wI033tjs32b58uXcdNNN7N27F4A77riDM85o/m/60ksv9Z0wYUL5hAkTqt33XXTFFVcc2JRh9atf/WrgI488MgQgNTVVX3/99fUjR46se/jhhwf++te/HgIwatSo6qeffnrLAQccUAdw8803D33ppZcGiAhZWVnhVatWfRIMBrn11luHvvDCCwMBJk2aVL5gwYKCaMPPMIzOwROD2FhYTiAgMY2bMBBuIlkp3glpn8wU9rg5NbFoiVE1bnB2A9GKWAt58U6QcwdmdflkWkQGA0dSn8+3CHhYRHKilConAQ8AqGqRq2x5HvDLjupbtGE1mvhDI/OLyhtFv2SlBZl34ZGc/Kt/RryLnxbuY19VLb0zUps0njyDbH/3UeLPi/Of15MENJLHsEoNUkItlbUh+nd1ZwyjCTyPVatcVh1MZmYmK1c6oQBXXnkl06dP54knniAcDnPRRRfx5JNPcs455/DAAw/w2WefkZmZyb59+8jMzKSkpGQ/V3eYN28ev/nNb/jPf/5DdnY2ADfccAOffPIJw4YNo7q6mlAo1OT5t9xyC6eeeioPPfQQ77//PqeddlqTRhVAYWEh7733Hp9//jlHHHEExx9/PBMnTuSRRx7hsMMO4+ijG+XWN6CkpISrr76axYsXM2zYMD777DOmTZvWIMTwi1/84kF1dXVy4oknlt5///07MjMzdcuWLWkjRoyIxGKMGTOmZufOnTFddH/5y196z507d9g777zzSW5ubl1paWkgNTVVV65cmXHHHXccsHLlyo9HjRpV+/3vf3/4nDlzcl977bX8hx56aODrr7/eb/ny5Z/0798/vHPnzmAwGOSFF17o88ILLwxcvnz5x/369Qufc845o2+66aZhjzzyyPZm36hhGO2K56lZ7/MohFrhMtrfhLSguIKLFiyLGFUBcZSSoaEyXzTjXO/T7IUrI96sgDjheZ4h1NUGUTsyEtiuqiEAt7baDne/37B6D/imiKwCRgPHApt9x78pIicDO4HbVPXdWDcTkTnAHIDc3NxmO+Y3rEqaMYxjMXpQViS0U8SZVgzpm0HAnWYM75dJv6w0Pthawt//9zn3/21dA0NMcD7zkDppCk/MmhrzPnk5vRqM45EDMmO2i2ZQdnokxLQn5dMljWGVkep4AvyD1DASjVg5VvF4lABmzn27yS/A9mDWrFmR7T/96U+sWLGCX/7SWairqKhgxIgR9O3bl7Fjx3LppZdy8skn85WvfIXevdsmwnHCCScwa9YszjzzTM444wzy8vKabBsIBHj22WeZPHkyubm5vPPOO81ee/bs2QAMGTKEM844g6VLl9K7d2+eeOKJ/Z4L8J///IdNmzZx2mmnRfaJCBs2bCAjI4P169evGTt2bO3u3bsD55577oE33njjsF//+tc74nzrAPz5z3/ue9555xXn5ubWAXjepb/97W99ZsyYUTpq1KhagGuvvbZoypQphwEsXry475VXXlnUv3//MMDQoUNDAEuWLOnzta99bfeAAQPCAFdfffWu6667biRghpVhtBMFxRVcsmA5BbsrSAkKdSElJSrMLVokIpqgCHk5vdhQVNZgnS0gjpCBFxbYnCcB4OIFy9m6uz7sy/+9EMu48+7ttfGMq2iFwB7K9Tgeq9VAAfAm4CU+zQfuVtVaEZkJvCoih6hqcfRFVPUx4DGAqVOnNmtNV/rmrP68qHi44+zDueiJ5QDkDerFxqJyCoor2LzLMahH9M9k3ODefLC1hJ+8spbymobz47GDs3n4wiM4d/677K2qY8b9S2OGad779YmcM/8/kdczDhocV//Onjycx/+1CYDRA3vO2EqaZGrPsKo0w8pIYNqSY7Vg1rRIYnBHJP16HiQAVeWVV15h9erVrF69mk8//ZT77ruPYDDIsmXL+O53v8u2bduYMmUKa9asISUlhXC4Ptqsqqoq7vu+/PLL3HXXXZSXlzNjxgxef/31Zttv2rSJQCBASUkJlZXOhOKNN95g8uTJTJ48mfvuu6/Z89999122b9/OIYccwujRo1m2bBmzZ8/mySef5Kmnnopc57nnnkNVmThxYuTvsHr1arZu3crUqc7K3tixY2sBBgwYEJ49e/au5cuXZwOMGjWqZtu2bREP1caNG9OGDh1aA3DJJZfkemIXH3zwQXrcfyjDMLqEguIKTvzl0kgS/sULlrFldwUK1IY08tsTgjjhl0ubNaq81fsFs6YxNiebAI4ohLdg9swVRzO0TwbgpDk0peh27L1vUrC7oVfK793ywvDGDc7GE5uL9hx4bTbeczpLrpuerKpwW4EDRCQI4P4e7u6PoKpFqnqxqk5S1TOB3sD/3GM7VbXW3V7intvmIutVtfXfm3sqWmZYhd2JxNEHDuDN67/MiP6Z1IWV/2x0bL0D+mVGvEvRRpVnXB80tA+pwYB7PWKKpuzc63yf93fzv/6xrhBt4STmZ2cflqxjqxFJ5LFyBoZ5rIxEJiXgfHkqjvEiEr+0amfGqp911lnce++9PPLIIwSDQXbt2sW+ffsYNGgQZWVlTJ8+nenTp/Puu++ydu1azjvvPGpra9mwYQNjx47ld7/7XZPX7t27N6WlpWRnZ1NXV8eWLVs46qijOOqoo9i4cWMkxC8We/bs4aKLLuL5559nyZIlXHXVVTz//POccsopnHLKKY3aP/3003zxi1+kqKiIxYsXc+211zJhwgQuvPDCSJsvf/nL/OhHP+IrX/kKAJdffnmD+61fv5633nqLGTNmALBy5UqmTp1KaWkpZWVlkp2drbW1tbz44ov9Dz/88EqAr33ta6U33HBD7ocffpg+YcKE6nnz5uWcffbZuwGeeeaZAn8fzzzzzNLvfOc7o77//e8XjRw5MhIKePLJJ+998MEHhxYUFKTk5ubWPfzwwznHHXfcXoDTTz+99Iknnsi56KKL9nihgEOHDg3NnDlz709+8pMRN9988+d9+/YNP/bYY4OmT5++N64P3TCMmFz0xDK2umIA8YhARNdr8kiNId7Q1DP9D1cfw5f+31ukpwQYOSCzkXhRSUUtRWUNlfyaCrfq6V4pVS1086UuAJ51f78flV+FiAwESlW1TkROACYA33CPHaCq293tyTihguva2re2hAJud8fkiP6OwTImJ5tteyr55/qiyP5n3t3S6LzoceK/ryeaMu7WxZGxOmlkXwAuPWY0zy7fwpbiCsbcsni/IiT+nL+PP9vLl8bltOj9dVeSxrDKjIQCWo62kbiISMRrFVZoQrm2y/nVr37Fj3/8YyZNmoSIkJ6ezq9+9StSU1M555xzqKysJBwOc+SRR/L1r3+dlJQUHnzwQWbOnElOTk6z4g7XX389J5xwApmZmbzxxhtcdtlllJSUEAgEGDlyJPfee2+T515xxRVcccUVHHfccRxzzDGceOKJzJ8/n6uvvjpm+0GDBjFlyhRKS0u5+eabmTBhQov+Dv379+dPf/oTN9xwAz/4wQ+oqakhLy+PP//5z+Tn5wcuv/zyQ0SEuro6mTp1atncuXO3u+eFH3zwwS1nnnnmuHA4zGGHHVZx2223fR7rHl/5ylf2rV+/fueJJ544XkRIS0vTxYsXr582bVrVT3/60+0nnXTSeIDc3Nzqp556agvANddcU7x9+/bUadOmHZKSkqK9evUKrVixYt15552394MPPth91FFHHQIwceLE8nvuuceUCA2jDfgV1lqjrOflM7VkxX5E/0wGZaexq6yGgt0VXLpgBVtc71R0aJ9HU5EMiSAgkQBcDSwUkZ8Ce4BLAURkMfBTVV0FHAX8WkRCwC7gTFX1XII/F5EpQAioAS5R1Z1t7VS0eEVL2BYxrByv1JicbN7+tCiiFDiif2aDMFGP6HESKyTVC0XdWFQW8YoedXemcJAAACAASURBVOAAfvvuZsD5P9ifsEqJ7/38b0fPWd9LGsMqEgpYYx4rIzYiMh5YCAwEioFLVXV9VJuTgZ/jrFQ95JdWdZWFnsJJeE0F3gKuVdUWFZ+IKAOq4kTQdz3Rbv3evXvzyCOPxGy7fPnymPs9o8fjtttuA5x6Un51wNtuuy1yDOBf//pX3P384x//GNkOBoMsXbq0ybabN28G4J577mn2ms1dA2DatGkx2xxxxBHhTz/99OOmzrv44otLLr744rhUPX74wx/u+uEPfxj5I61Zsyb9zDPPHFtSUpLSr1+/umeeeWaTpzAITq7ZXXfdtXP79u1pS5cu7VNRURF48MEHB1133XW77r777p133333ToAPPvggfejQoZMuueSSoscee2xbPH0xDKMh6amBmIu2Ao1yrIIBoS6saBtzYUWEySP78fePC1m9taRRyJ+fjsi5TTZU9ROgkVqRqp7u234daFTfyj02K9b+ttKWUMBte5wxcYBrWEV7Kw/ol0leTq/95mZ7Hs1YBntYnRqxwYAzHksraxsca05YxZ8z9vFn+1r03rozSZNjFfFY1ZlhZTSJV8diPDAPp45FNPnAlUCsRJ1bgI9VdSIwEZgCfL2lnUjrgUWCjZYxZ86cUXPmzCncvHnz2jlz5hReddVVo6LbzJ8/f+CmTZvSN2/evHbZsmWf/OIXvxi+bt26SF5XXV0dV1111eiTTjopPslGwzAaUV0XiqnmJzjJ/+vvPp1N957B+rtPJ/+eM3jzui8ztp1yYSeP7AfA2+uKmm3XmYV2jfbFrwuwp7yFoYAljT1WfkYMyIorNztWLl40hwzrTa/0FMbkZDdYDh4xIJPp971F3s2vNSoC7A8x3FBU1mNSdcxjZfQI4q1joaob3PZfjXEZBXqLSABIB9JohdpaWtATsDDTqimmTp1KXV1DR+AXvvAF5s+f30U96jy2b9+e8tFHH2XNmTNnN8CcOXN233jjjbk7duxIGT58eOSP8uKLL/afPXv2rmAwyPDhw+tOOeWUkmeffbb/nXfe+TnArbfeOvTUU08tKSsrC5aVlSXNIpphdCZrt5dSG1LGD8lmQ2F9flVT9XzaM+xukmtY/XH1dhTISg1S4Zucmqeq+9OSUMBGeXaVTvuRXo7V4HqPVTAgDOmdTkowEPf4iMjvu8Wc/QWrt+6upKC4opHi5RafIRUdGui9n/5ZqeypqGVDYRmHH9A3rr50Z5LIsHLFK+osx8qISbx1LJrjTuAl4DOgF/Cwqv67pR1JT63PsTJis2rVqq7uQpeRn5+fNmTIkNqUFOfxnJKSwuDBg2vz8/PT/IbVjh070vLy8iLhgbm5udVbt25NA3j33Xcz33zzzb7Lli1b9+Mf/3h4U/dqSb0Vw+iJrNq8B4ApowagChsKy1A6py7PxBGOYeWtwd1/3iQOH963RwtRJBstEa+YvXBlZPz5w/uG9nUUJHOy0+mdkcK+qjqG9c0gJdiy9bToRYETfeqWeytrI0bTm9d/mWPueZPPShuq//pDA0NhpcQNG/xC3kBeX7uT/+3Y2yMMq6RZxYyEAprHyug4zgXWAMOAA4DjReQb0Y1EZI6IrBKRVUVFjW22tGAARRvIkxtGe1FdXS3f+ta3Rj366KNbPOMsmnA4LEBYVR9T1amqOjUnp2coNhlGLAqKK5hx/9JGIU2rtjiG1dRR/R159MEdV/IimtKKWlJdhaOgCAcN7d1T5NF7DP4cq90VNc1GsmwsKoukEHgLs0P7ZETk0kUkEha4bU9lo9C8luLVw4LGHtrCvdWN2otvsWFvZS2q0DsjJdKnG19a0+Y+dQeSyGNldayMZonUsXC9VTHrWOyH7wFXqGoYKBWRV4EZwIv+RvsrDpieGmRLSS2H7dlN5pDBLZJcN5KfvLy8ms8//zy1rq6OlJQU6urqKCwsTM3Ly2sQJzJ8+PCa/Pz89OnTp1cAFBQUpI8aNaq6oKAgdevWrelnnXXWOIC9e/cGAfbt2xdctGjRlnA4LEVFRX2BtZ3+5gwjQbn86RVs2uVMHNcXlnH8fW8xbnA2hfucCeTU0f07XV1v9sKVkXCssCpXP/Oehf0lGX6PVU1dmMraEFlpKY3C/hbMmka/zFR2u14twTF2POEKD7+C5f5U+/ZHtPCF30PrP+YxrG9GZLHBCwMc0CuN1z50hGmVhv9bsZQyC4oruOypFWwproi87+62eJA0hlVmmie3boaV0Zh461jsh03AqcAKEUkDTgJebmlf0oMBHlq+hymj+rN3z+6Wnm4kCDt37kwJhUKDOuLaeXl5dXfeeeeor3/96+Uvv/xyrzFjxtQWFRX183tAjzvuuLrf/OY3wydPnhzcvXt34PXXXx+wYMGCnRUVFX2WLl0ayf178MEH+1VUVMhNN91U/sEHHwwCwsDaurq6Kzui74bRHcnf1ThfylNJC3bR4pffQ9BUTpfRvfGcAb3TU9hXXcfaHXu59eUPGyj0eQZS3uBsdruhqZlpQSpqQpEaVh5l1fW5yftT7dsfzdU/8x/LTAtQVh3i5tMOiRhBXg2rfllpfLitsX5SU0ZfdM24thiGXUWbDas4JayDwK9xJqUK3KuqT7jHfoujsOYxEfiqqv6pJf1Id5XWzGNlNMN+61iIyHHA80Af55B8E5itqm8APwDmi8iHQBBHbv3xlnYiPTXA3uow+1IHcOR4C7/qrhx66KEfqurUjrj2mjVrDl6zZs3CO+64oz/wOXDppEmT1vnH6uTJk4PAw0ceeeTJOLVVvnPWWWc9Fn2tp5566nYg+/nnn/9R9DHDMCAcVlICDZP1/YRUuXLhqk6f4DXnMTC6P6oacQYM65fBvs/LuP6F1WyLqj3lGUi90oORfd5cd0SUx2pMTi82FpVH5P7bMmaa89D6j9388hoWrdjK3qr6HLE95fXCFWNyshvVyYpl9BXuq4oYVU216Q60h8fKk7B+VkQuxpGwPiGqzUXAWJz6AAOB90Xk76q6WVUv9RqJyCTgH8AbLe1EvcfK8laM2MRZx+IdYEQT52+kXlWw1XiqgDUmtGI0QZxjNQR8O45r3d6unTOMJGPpp4XUhhzjqq4JVaGumOA15zEwuj81oTBhhdSgMLBXOlDG9j2VMUuxjBiQyZbiClJdtT7PSDmgX0PD6slZR3X6mOmTkQrQoMZVvSJgGnecdXijOllCQ6NvY1EZZ/y6YU1L6aaLCW0yrOKVsAbOBx53c1OKROQVHCGA6FpBs4HnVLVxVtx+yEixUECje+CpAlZbzTXDMIwuYUtxORc8vozPSuqVzfplpfLwBUfyf6+ubTAJ7CpvUWfndBmdi+cIyEgNMqCXU4Kwd0ZqAwPF47ypI7nvjXUcPy6HNdtLKXJz/6JDAbtizPTJdAyrvZX1YYiewmH/rLRInwqKK/jmY++yo7SK9NRAA6PvoseXNXKM+HO2uhNtVQVsJGENeBLWfnKBLb7XBdFt3JyVC4EnY91of0prnsfK6lgZiY55rAzDMLqW8x9dxo6SKpT6Yu27y2v4v1fXsuS66fzzhhmM60QFQKPnUe06AjJSg/TLcoyTtBQnn0+AcYOzOWhIbwBe/u82wCkaPWVU/8g1bn3lwy5X2YsYVlWxPFapkX25A7N4/QfHI+LIsQ/ukx459nkMlcGrvpTX7YQrILHk1r8KFKjq6lgH9ycLHJFbNy+AkeCku97VajOsDMMwuoSde6sa7fPndJisudHRVEYMqwD9sxyPVdE+xyD51Tcns+S66Zx82BCASD2pSSP7sbqgXgxi625HPbAr6ZPhBL/FCgXs53riPPpmpnLI0D7UhpT33fcRDit+fRhvc1MMQZnuQFsNq4iENUREKmJJWBcAo3yvc2O0uYImvFXx4IVXmcfKSHTSUsxjZRiG0VX4V9b9mECE0Zl4oW+ZPo+Vx1EHDgBgepTA1cQRfSNhgJAYAg99I6GAfvEKLxQwtVF7772t2OSoIn+4vZSwQkpACIowzC143CMNK1UtBDwJa2hawvoPwFUiEhCRHBzvVKT2j4iMAL4EPNfavtR7rGyyaiQ2noKl5VgZhpGsiMh4EXlXRD51f4+L0ea3IrLa9xMWkbM6um9//9/nAGSmBgjgiAcEBAv5MzqVSl8o4ACfZyd3QBbD+jqiFJNH9iM7rV4N8Nz57zJyQGbEw5MIiwF9YhlWXh2rrLRG7T3DauVmx7B6+1PHZDh/2kg23nM6z1zp6DZtLu6ehlV7qALuV8IaeAZH4cqTYb9DVTf5rjEL+LOq7mltJ7wCwVXmsTISHPNYGYbRA9ivYnB7qQK3lNfWOAVLbznjUC75wqj9tDaMjsETW8tICUZCAQGmjR4Q2U4JBvDHyW0sKiN3QBZjc7ITRi0y4rGqaixe0S+GYeV5pN7ZsIuT5i6NLDZ73rmR/bMICGzfU0l1XSiSPtFdaLNh1R6ywKp6d1v7YTlWRnfBcqwMw0hmWqAY7KfVqsDxUlBcwWVPr4iETh0+vE9H3cow9kvEsEoLNlC0/tf6IgqKKyJ5fRU1DYv+bt1dycZ7TidRiCW3vtsTr+jVOBTwxy+uiWxvKKz3So0c4LzftJQAIwdksaW4guX5xdz5l48bGJGJnu+YSOIVbcLzWFmOlZHomMfKMIwkJ17FYGD/qsBum2aVgeNh9sKVbPLlo/gneIbR2dR7rAL84q+fRPYXlVU3EKQYk5NNIIFC/6Lpk+n4aPZW1qKqqColvjpW0TSVE3btovcj26MHOu/xxpc+ZENhGSFVNhSVNSvUUVBcwQn3LyXv5teYOfftLlNLTBrDyvNYVVodKyPBqc+xMsPKMAyD/agCw/6VgeMhv6i8QfHVrk76N3o2EfGKtCBbd1dG9muUIMWCWdMYk5O40v/pKUEyUgPUhZWKmhDlNSFqQ0pmajDi9PCTl9MrYij68b/nAwc5htXO0qrI/6wqrC8s46S5SykorqCguIIv/eIfHHjTa4y5ZTHH3/cW+bvKCStsKGzeCOtI2iPHKiHISHMnq7U2WTUSmzQzrAzDSG4iisGqGmpGMdijTarA8ZKX0ytS+FcScOXf6FlU+nKs8nJ6saGoDNXGXqnuUCi6b2YqVbXV7K2qpS7kmEKxFAHBMRRnL1zZbBFuz7AScQwqPxsKyzlx7lJqQ/UHQuGGjRTHCPvyfW8BTvikP5SwoLiCWU+tYEtxecRYba8Qw6TxWKUFA4hATSjc6A9sGImEqQIahpHMtEAxuF1UgePlgfMnRbbHJEDSv9GzqfLVsVowaxpjE9grtT+8PKu9lXX1NaxihAFCvaHYXBHu0a5h1dR03m9UNcfm4go2F1cQUmWjL5Rw9sKVbPK8W/sJMWwpSeOxEhEyU4NU1ISoqg3RKz1p3pqRZFiOlWEYPYB4FIOhHVSB42VXmTPhmzKqPy99+9iOvp1hNIsXCpiRFuwWXqnm8CTXSytrI564Ab1iG1Yezb3njJR6v09aUBjWL5OtuyuaNLTiwV/zyx92GB162VaSyvrIcA2rSjOsjATGVAENw0h24lEMdl+3WRU4XlZvLQGc2kCG0dX4QwG7O/4iweWuimF00eOWcOsrayPbtSFFcEQ88ovKCQSgLqQN8iVTg0IorIzJyaY2FKYghhEm1IcbjhqYRb6vAHF7hgUnlfURkVw3AQsjgUk3j5VhGEan84EZVkYCUe3OVTPTur9h1SfDMSfyi8p4/F/5APxr/a4GsvEtwa/eqTSUmC8ormD2wpVNSrD7j48ckMm+qjqKy2sIBoRHLp4CwPWnjOea5xwVwoyUQLuGXiaVYZWe6kxYzbAyEhnLsTIMw+hcVNU8VkZCUemTW+/ueB6rx/+VHwm53VtZy+yFK1sV4piX04uNRWWEWyHmEX28LhTmhF8upWB3JTMfeJuxOdl87YgDIseDAWHkgMzI6y3F5Vzw+DJ2lla1Stii+3+aPiKS6zXmCTASF8uxMgzD6FwKdlewp6KWgb3SGNE/c/8nGEYHUy9ekQQeK9ew2lVWUy+PTutzl9pTYj4lGIiIXajCxqIyHnO9agDlNSG2l1RSUFzBSXOXMv2+pewoqSLstm2psEVSeawioYDmCTASGC/HqiZkhpVhGEZH4YUEbSwsQ9zCOVW1Ibburmw3aWXDaC2VvjpW3R3PY9U3M5WSylqgbSUN2lvMo3BvdWQ7rFBS4fQxJSDUhZVPP9/H3a99zMYoQzDcCmGLpPJYZUQ8VmZYGYlLpI6V1VwzDMPoMGYvXMmGwjLC1Ne5qagJdVnhUMPw43ms0pNAvMKTW586un9kX96gxClp4DfwBMegAjh27CAA1u0sayBm0dS58ZCUhpXlWBmJTES8wjxWhmEYHUZ+UTnR6sxtCU8yjPakKpnEK1yPlefxGTkgkzev/3LCeIYXzJpGTrYj/56VHqQurKQEhJmHDgFg3c69pAYam0RBER67dGqL7pVkhpXzdirNsDISGPNYGYZhdDyxVpqjE+ENo6uoSiLxij6ZTmbRJtfrM35w767sTiNyB2bxzJVO9Qdv7jV6UC8OG94HgH98UkhNKExAHGNq3OBshvfLIKTKZyWVLbpX9/80fZjcutEdSDOPlWEYRofjX2lODQoBoc2J8IbRXkQKBCeDeEVGw5pVY4dkd1FPmmb84N4M6JVGnRsWPDYnm3GDnX7urXJqb13yhVFsvOd0llw3nZMOcbxZFz2xnJlz36aguCKu+ySVYVUfCmgTViNxicit2wKA0QQiMl5E3hWRT93f42K0CYrIPBHZKCIbRORK37H/E5GPRGSNiLwnIqd07jswjK5nWN8MwHnmrr/7dPLvOYMl101PmPAko2dTmUShgJ54hUeieawAAgHh6AMHRF6PHZzNnvLaSL4VwLFjBkW2315XBDjhwy1RB0wqw8obnBYKaCQy5rEy4mA+ME9VxwPzgEdjtLkIGAuMA44BbheR0e6xFcA0VZ0IXAH8XkRMY9roUVS7JS3Sgkk11TGShPpQwO5vWPWJMqzGJaDHCuCgofUG3x/e28qlTy6PeLAA7v/busj2tj31IYAtUQdMqqeNF6dqoYBGIuN9ydeGlHA4OrXa6OmIyGDgSGCRu2sRcKSI5EQ1PR94XFXDqloEvAKcC6Cqb6iqF7ewBkcIaWCHd94wEgivVmBaEuSwGMlHJBQwrfuPz97pKUi944exgxPTsPrjf7dHtgv3VbM5KrzPbzzl5fSKvKeW5GZ2/0/TR4Z5rIxugIiY18pojpHAdlUNAbi/d7j7/eQCW3yvC2K0AbgU2Kiq26IPiMgcEVklIquKiorapfOGkSjUhsywMhKXZCoQHAgIvdMdAYsR/TPJSkvMMrl+L5S669qBJoynBbOmMbYVRYoT8523Ek+8wtTWjEQnPSVATV2Y6tpwUjxUjcRERKYDdwIzYx1X1ceAxwCmTp1q7lMjqfA8VqkWCmgkIMkUCgjQNyuVvVV1jB+SePlVHnk5vdhYVEZYHUMqd0AWqcEA+UXl5OU0rLvV2iLFSWVYWYFgo7uQnhJgH1AdCgGp+2tu9Cy2AgeISFBVQyISBIa7+/0UAKMAL6O2gQdLRI4BngXOVtV1GEYPo8Y8VkaCUhsKUxdWggEhNSj7PyHBKSiuoHBvNQDvF+yhoLgiIUViFsyaxuyFKxsYUu3dz6QyrDyPlYUCGomOV2ndvKtGNKpaKCKrgQtwDKMLgPfdPCo/fwCuEpGXcfKnvgp8CUBEpgG/B76hqv/ttM4bRgJRY+IVRoLir2El0v0Nq9kLV0bEYkoqapm9cGWrvD0dTWu9UC0hqZ42XoFgE68wEh3LsTL2w9XA90TkU+B77mtEZLGIeMV5ngHygfXAMuAOVd3kHvsNkAk8KiKr3Z8JnfoODKOL8Z6vqeax6vHEWcJiqIi86pap+FhELo7R5iARqRCR+9vSn2SSWoeGog9K/Ap6yUhSeawyzGNldBPqa1mZYWU0RlU/AY6Osf9033YI+HYT51sFVKPHU+uuoKebx8qoL2HxrGswPQqcENVmLrBKVc92VVjfE5G3VXUrOLUD3fNeaWtnvO/+9CTJr4rOXYpXQS8ZSaqnTYaJVxjdBPNYGYZhdCz1HqvuH2pltJ4WlLCYBPwVwA29Xg2c5zt+E/AX4NO29qleETA5puELZk1jTCsU9JKRNnusRGQ8sBAnxr8YuFRV10e1CQK/Bk7F8RLeq6pP+I6fB/wfTq0VBU5S1c9b2hfLsTK6C/UeKxurhmEYHYHlWBkujUpYiIhXwsKfu/oe8E0RWQWMBo4FNgOIyCTgFGAGzny1TSRbKGBn5C51F9rjaeO5V8cD83DcpNFcBIwFxgHHALeLyGgAN1/gdmCmqh4OHAeUtqYj3gC1HCsj0TGPlWEYRsdidayMFnI9MATHU/Vr4E2gTkRSccpSXO0ZZ80RT33ASHHgJAkFNOppk8fK5171aqQsAh4WkZwoBavzgcdVNQwUicgrwLnAfcAPgftVdSeAqrbKqIL6AWoeKyPRMVVAwzCMjqXa6lgZDnGVsHDnrRHBChFZDPwPGAaMARa7Cn79nMPSR1XnRN8snvqAyeaxMupp69OmkXsV8NyrfhrUV8Gpv+K1ORTIE5F/ish/ReQnEkN7Mp4VgIw0UwU0ugdeaIp5rAzDSEbiUWFz250nIh+KyFr395D26kMkFNA8Vj0aVS3E8UJd4O6KWcJCRAaKSIq7fQIwAfidqhao6iBVHa2qo4Ff4TgLGhlV8eLNU5NFvMKoJxGeNkFgIo7XazpwGnBJdCNVfUxVp6rq1Jyc6HxDB0+8osq8AEaCk+4mrFbX2SKAYRhJyX7TBNozFSAWtSHHWZBuhpURXwmLo4CPReQT4A7gTFWt6IjOJJt4hVFPW8Ur4nKv4nioRgEr3dd+D1YB8KKqVgPVIvIqzuD+bUs7U7TPqfpcVl3HzLlvd0hFZcNoDyIeqzpbBDAMI7loQZpAu6UCxKLGXbiyUEAjzhIWr+NoAezvWre3tT+eYeWJrhnJQ5ueNvG6V4E/AFeJSMCVt/wq8KJ77HfAyeKQCpwIfNCa/lz9zHuR7Y1FZcxeuLKZ1obRddR7rMywMgwj6Yg3TSCuVIDW4oVamyqgkWjsKKkC4MX3tjFz7tsUFHeIY8zoAtrjaROPe/UZIB9YDywD7lDVTe6x54FCnATB1cBHwILWdMRf6TmsPbvys9GYOCuvn+zm8lXHqqzeXvkAaUFnlco8VoZh9GDiSgWA+PKso/FCAS3Hykg0nl3mBG0p5ghINtpcxypO92oI+HYT54eB69yfNpGX04v1hWWAUxCrJ1d+NmIST+X1fOBK4BtAhv+ALx/gBFXdKSJ9gerWdMQ8VoZhJDEtSROIKxUgHqW1aEwV0EhUdpfXRLbNEZBcJNXTZsGsaeT0TgcgOyOlR1d+NhoSb+V1Vd2gqquBuhiXaZQPoKpVremPF5pihpVhGMlGC9IE2i0VIBZWx8pIVPpnpUa2A2KOgGQiqZ42uQOzeOoyx5ga2CvNhCsMP/HG/DdHu+UDeB4rCwU0DCNJiSdNoN1SAWIRkVs3j5WRYJxy+FAARGBMTrY5ApKINocCJhoHD+1NRmqAzcUV7C6vYUCvtK7ukpE8+PMB0oC/4oSyNAhbEZE5wByA3NzcmBeq91iZ3LphGMlHnGkC7ZYKEAurY2UkKpmpzvT71tMP4cov5XVxb4z2JOmeNinBABNH9ANg9dY9XdwbI4GIxPwDNBPz3xyRfABV3Qd4+QANiKfmWnqqiVcYhmF0JBYKaCQqNSFnUdXGZvKRlJ/oEbmOYfXfLSVd3BMjUWhBzH9ztFs+QLrlWBmGYXQoNSZeYSQotXWO/oqNzeQjKT/RI0b2B+B981gZDdlvzL+IHCci23BCU74lIttE5BT3/HbLB7AcK8MwjI6l2jxWRoJSazXWkpaky7ECGOIqA/57QzEnzX2bJ2dNMyELI96Y/3eAEU2c3275AJZjZRiG0bHUmniFkaB4xatTzehPOpLyE/3xS2si21Z4zUhEzGNlGIbRsdREPFatEm81jA6jXrHSxmaykZSGlb/QmlrhNSMBSQs64hWWY2UYhtEx1E9eg13cE8NoiBcKaDlWyUdSfqL+QmuCFV4zEg8v5t88VoZhGB1D/eTVvAJGYlEbMvGKZCUpc6wWzJrGmQ+/Q2llLYOy06zwmpFwpKeYKqBhGEZ7UlBcweyFK8kvKicvpxfBgGNQmXiFkWjUmLBK0pKUn2juwCxmHTsagAuOHmXCFUbCYR4rozlEZLyIvCsin7q/x8VoExSReSKyUUQ2iMiV8RwzjGRl1lPLWV9YRkiVjUVl5O9y0gBs8mokGlYKIHlJ2k+0f1YqACUVNV3cE8NoTHFZNQDrPt/HzLlvU1Bc0cU9MhKM+cA8VR0PzAMejdHmImAsMA44BrhdREbHccwwkpItvudoWP05Vkk71TG6KSa3nrwk7SfaPysNgD0VtV3cE8NozE9eWRvZXl9YxvH3vWUGlgGAiAwGjgQWubsWAUeKSE5U0/OBx1U17Ba6fgU4N45jhpGUDMpOj2wHBFIsFNBIUCL5f6ZYmXQk7dOmn3msjARm6+7KRvusNIDhMhLYrqohAPf3Dne/n1xgi+91ga9Nc8ciiMgcEVklIquKioraqfuG0TVceHRuZHtMTnZkHmCGlZFoeOIV5rFKPpL2E633WJlhZSQeeTm9kKiFqrCVBjA6GVV9TFWnqurUnJxoh5hhdC+y0x09rsOG92HJddMJO3NXy2MxEg7LsUpekvYTjRhW5RYKaCQeC2ZNY2xOdoN9VhrAcNkKHCAiQXCEKIDh7n4/BcAo3+tcX5vmjhlGUuKprFbWhABfjpV5rIwEw1QBk5ek/UT79bJQQCNxyR2YxZLrpvPPG2YwwA1X6ZeVaqUBDFS1EFgNXODuugB4382V8vMH4CoRCbj5V18FXozjmGEkJZ4hVeEZViYQYCQoViA4eUnaT7R3egop6ax97wAAIABJREFUAaG8JmSS1kbCkjswi5+eeRgAX8gbaKUBDI+rge+JyKfA99zXiMhiEZnqtnkGyAfWA8uAO1R1UxzHDCMpqY4YVnWoqqkCGglLrXlTk5akLBAMICL0y0plV1kNJRU1DO6T0dVdMoyYeOF/m3ZZfpXhoKqfAEfH2H+6bzsEfLuJ85s8ZhjJimdIVdaGIuIAKQEhEDDlNSOx8MZnatDGZrKR1KZy30wnxMok141E5sBB9YZV2Mu2NgzDMFpETcgJAawNaSTPykKtjERDVSNhqqkBG5/JRlJ/oqYMaHQHemekMrh3OtV1YbaXNJZhNwzDMPZPdW192H9ppbOgaqFWRqJh3tTkJqmfOP1cw8oELIxEx/Na5Vs4oGEYRqvwvAAAJZXO974ZVkaiYcIVyU1Sf6r9I0WCLRTQSGzyXOn1/KKyLu6JYRhG98QvVOV975twhZFo1JrUelLT5k9VRMaLyLsi8qn7e1yMNkERmSciG0Vkg4hc6Tt2u4gUishq92deW/vk0b+XFwpohpWR2IxxBSysQLBhGEbraGBYWSigkaDUmMcqqWmPT3U+ME9VxwPzgEdjtLkIGAuMA44BbheR0b7jv1XVye7PNe3QJ8CpCwQWCmgkPp4yYP4u81gZhpEcxLnw2m6Lq9U+w6rU/d43j5WRaNSXAbD8qmSkTU8cERkMHAkscnctAo50C1L6OR94XFXDbpHLV4Bz23LveDDxCqO7kDfICQXcZB4rwzCSh3gWXqGdFlf9HitPvCI1xSavRmIRkVo3b2pS0tZPdSSw3a2Z4tVO2eHu95MLbPG9Lohq800RWSMifxORY9rYpwhejpWFAhqJTlidB+2O0ipO/OVSCoorurhHhmEYracFC6/tRnXIcqyMxCeSY2VjMylJhE91PnCgqk4E7gNeFZGB0Y1EZI6IrBKRVUVFRXFd2FQBje7Ct555L7Kdv6uc2QtXdmFvDMMw2ky8C68Qx+JqPHOA6tpQZNvLsbI8FiPR8DyrNjaTk7Z+qluBA0QkCI5IBTDc3e+nABjle53rtVHVnapa624vcfcfHn0jVX1MVaeq6tScnPgWvOpDAc1jZSQ2ftEKVROxMAyjxxDX4mo8cwC/3LrVsTISlYh4hY3NpKRNn6qqFgKrgQvcXRcA77t5VH7+AFwlIgE3DOCrwIsAInKA10hEJgOjgXVt6ZdHfxOvMLoJeTm98GcCjB6U1WV9MQzDaAfiWniNd3E1HhrkWLkLquk2eTUSjFoTr0hq2uOJczXwPRH5FPie+xoRWSwiU902zwD5wHpgGXCHqm5yj/1cRNaKyAfA48AlqrqzHfrlCwWsRd0cFsNIRBbMmsbYwdmR12dOGt6FvTEMw2gb8S68tufianUs8QoLtzISjIh4hY3NpCSlrRdQ1U+Ao2PsP923HQK+3cT5s9rah6ZISwnQKy1IeU2IfdV19MlI7ahbGUabyB2YxZLrpvP6h5/x7ef+y8P/2MBDb24gL6cXC2ZNI3egebAMw+h2XA0sFJGfAnuAS8FZeAV+qqqrcBZXpwAhoIY2LK42rGPlyq2bx8pIMKxAcHLTZsMq0emXlUZ5TSUl5bVmWBkJzymHDSU1KJEVrY1FZcxeuJIl103v4p4ZhmG0jDgXXtttcbWBYWWqgEaCUm3iFUlN0n+q/Xt5kuuWZ2UkPoGAUBeuD1sNdyMhi4LiCmbOfZsxNy9m5ty3TTLeMIxOxS9eEZm8mlfAIO5i1UNF5FVXofJjEbnYd+xyd/9qEflQRK5tbV9Mbj25SfpP1YoEG92NA/plRrYD4ghbdAdmPbWC9YVlhFQjnjbDMIzOoC4UJhRunEttk1fDJZ5i1XOBVa5C5fE4YapeeYCXgEmqOhk4FrheRCa2piOeYZVq4hVJSVI/cQqKK3i/oASAG19aYyvoRrfg8UunRra9HKvuwJbies9ad/K0GYbR/fF7q/yYKqDRgmLVk4C/ArgiK6uB89zXe7VeBS0LSAVapYpmOVbJTVJ/qrMXrqSsug6Awr3VtoJudAsOGdaHAwc5XqoHv3lEtxGuGJidHtnuTp42wzC6P/78Kj+Wx2IQf7Hq93CKVYuIHIjjmYrUYBWRs0TkI2ALcJ+qfhjrZvsrZl1jqoBJTVJ/qg2KrmIr6Eb34dDhfQD4aMfeLu5J/Hx1cr1E/Jic7G7jaTMMo/tT3YRhZV4BowVcDwzB8VT9GngTqPMOquqfVPUwYDxwiYgcFOsi+ytmXWPiFUlNUn+q0UVXbQXd6C4c5hpW/+tGhlUw4DxOUoPCGz84vtt42gzD6P6Yx8pohniLVRep6sWqOklVzwR6A/+LvpiqFgArgK+0pjMWCpjcJPWn6q//kxIQW0E3ug2HDe8LwEc7Sru4J/Gzr8qRN64NaaQ4p2EYRmfgeayiJ6s2eTVaUKx6oIikuNsnABOA37mvD/G1GwTMAGKGAu6P2jpTBUxmkvpTzR2YxZvXTSc16EhYD+qd1tVdMoy48HuswjGUrhKRfVWRiAl2lVV3YU+6LyKSJSK/F5ENIvKJiDS5IioiV7ntNorIwyIScPefLSLvichaEflIRK7vvHdgGF1DdV0IgP/P3p3H11nW+f9/vZOmS5qWpU2RrZRu6Iig2KI4yCag4jojiChQENl08KFV5wvOyDA6riA6M6LsUmVExAXmpxVEBdxYWqSyjNCdFBCSpoU2CW2zfH5/XNd9cufknOQkOclZ8nk+HueRnPu+c+7r5Fy5c1/X9bk+1x71fder9IaViy4ALpK0GrgoPkfScklJxqjDgb9KehL4PPAuM0uynp0Xr6erCCGC3zKzXw2nIL1ZAb1uVqOqXyB4Qm0Ns/esZ11LOxs2t2dGApwrZzMbJrHX9Em8sG0nG1vbmdvYUOoiDSoZsQJo2b6TBXtNK2FpKtangW1mNj+us/J7SfPNrC19UJxY/W/A64BW4JfA6cD3gOcJNwTPSdoNeFjSQ2b2+zF9J86NoSQUcI/6ibywrbdjZ6KntHYUvFj1L4F+61vFfZ8sVll2Jg2rCV43q9G4aC4nN6UbNnvyClc55s4M9fb4K++riAV30yNWLT5iNVynEtdXMbM1wErg7TmOOxm4Pc4J6AGuiz+LmT1oZs/F718C/koqs5Vz1ShpWDVMmkBN6n7VR6xcuensClEoHgpYncbFp5okrfCsgK6SPPV8SFzRY7CmuY2jLr+nrBtYfUMBfUHuYZpNSOWbaKJ/SuCCj5P0SuCNwG9znWywtMDOVYpkHatJdTXUT+wNxplYW1uqIjmXk4cCVrdx8anOnZk0rNoGOdJVM0kLJd0vaXX82m/IX9KJ8UZzp6Qr8rzOQZI68u0vlhdzJIBY19JWtuuxZYcCuv4k/VnS5jyPot4BStobuAP4aDKClW2wtMDOVYqdnb0JAaZM7P1TqvNQQFdmPCtgdRsXn6qHArroauAqM1sIXEUMucqyHvgIcHmuF4g3v9cAt49WIRPZywVAGL0q15FXT14xODM7zMxm5nl0E0ae0mF7s8lKCRwNeJykWcCvga+Z2W3FfyfOlZfMiNWEWupTDSu/eXXlZpePWFW1cfGpHjizNxTQrDIyrLniijeahwG3xE23AIdJ6tNNb2ZrzWwVqUUBs1wM/BxYPVplTdy45HDmz+qbtEKU53psPT1G2y5vWBXBbcD5AHFEdTFwZ47jfgK8V1JjzAZ4LvCj+HMzgLsJWatuGJNSO1diu1Lp1qfUpRpWfvPqykzvAsE+mlqNxsUVZ8bUiUyfPIHtO7t8Uv34tT/wbBwVIH59jtzzV3KSdCjwVuAbgxxXlHkrs2fUc/fSo/ndZ45lz6khhfD0KXVluR5b+64u0n0WHgo4bJcDu0taS2jAn2dm2wEkfV7SBQBmth74AvAAsIYw0npzfI2LgYXA+ZJWxcfZY/w+nBtTSbr1iRNqfMTKlbVMKKA3+qtS1adbB5DE3MYGVm16kQ0t7cyaNrnURXIVRlIdcC1wtpl1S/l7mszs2ngsixYtGvEQ6ewZ9Vx35iLe95372XPqxMyi1+UkCQOsUQhX9BGr4TGzduCUPPsuzXp+DTnCWc3sM8BnRqWAzpWp9IhVn+QV3rByZaazO2YF9LpZlcZFwwpgr2mTAPjAdQ8wv7GBG5YsLssbVDdqNgH7SqqNDaNaYB9yz1/JZW9gHrA8Nqp2ByRpupmdNyolTjl0v92ZNnkCGza38+D6Vv719sdZ39LO3MapZVGXk4bVfnvU07Slg9a2XfT0GDU1HurgnBt9O7uSOVbZySv85tWVF88KWN3Gzae68umtAJiVd2Y1NzrMrBlYBZwWN50GPGJmBcXqmVlTTDAwx8zmAN8ErhuLRhWEha6PnD8TgH+65RHWNrfRbcba5vKoy0lGwBkNIey2q8dyZjV0zrnRsKs7PWLloYCufO3s8oZVNRs3n+rWjt51dco5s5obVRcAF0laDVwUnyNpuaRF8fsjJT0DLCXMUXlG0ltLVuKUg/fdDQjzl5L4QiOscVXq9a2SEatpk+uYGUeHPRzQOTdWknTrk2qzGlZ+8+rKTG+6dY/oqEbjJhRwbuNU1jaHxpRUnpnV3OgysyeBN+TYflLq+z8A+xXwWpcVtXAFuG1l/qjFZOTq7qVHj2GJem2LI1bTJk9gZsMk1re0s3n7ThbuNa0k5XHOjS+9CwTXMqXO51i58uWhgNVt3HyqNy45nN2mhIvtbmWaWc25gWza8nLefUZpR2GTEavpkyfQGEesPAOnc26sZJJX+IiVK3OdXZ68opqNm0919ox6bjwrNKYmTahh3z2mlLhEhWlq7eCEK+9j3iXLSx7u5UprbuNUklwQNQprYKRzQ5QygUU6FLCxITasPOW6c26MpNOtT/E5Vq6M+YhVdRtXn+phs/dg790m8cK2nSz4l8poqHx42UOsiYkKPOnG+HbDksXMa2ygVmJeYwPf//AbmNfYu4Dwe167T8nKliSvmDZpAjMbJgKwuW3XQD/inKtykhZKul/S6vh1wQDHHiSpQ9IVwznXrlRWwHrPCujK2M4uX8eqmo2bOVYQ1rNKJrj2pLIDlmpeSiHS4V2edGN8SxYMTrt76dHc/sizfOLWVfxpXSufOD73zza1dnDOshWjlqK9d8RqApPrwk2Nj1g5N+5dDVxlZjdLOp2w7tpx2QfF5S+uAW4f7onS61illxmsq/UEAa68+IhVdRvxp1pIj5SkWklXSVonaa2kj+Q4ZkS9VYVKp4CuhIbKXtN7FzOu8aQbLoe3vGoWdbXioQ1bmHvJL/qMxCahpEddfk9m5HPtKIx8ZkasJtdltv3kz89UxKiwc674JM0CDgNuiZtuAQ6T1Jjj8IuBnwOrh3u+namG1ZS4QPDE2hoGWszduVLoTC0N4KpPMT7VpEdqIXAVodcp24eA+cAC4AjgMklzkp3F6K0qVDp0SpR/Q+X0Nx6Q+f7AmVM96YbrZ9rkukxIQY+F9OtHXX4PJ1x5H2fe+CBrm9v6HG+j0KGQHrG66p61me0evurcuLU/8KyZdQPEr8/F7RmSDgXeCnxjoBeTdJ6klZJWtrT0X36wNxSwlvo4au43rq4cdXaH5BU+mlqdRnTVGUKP1KmExVR74oKstwOnpPaPuLeqUDcsWcy+u4dRoBrBtz902GifsmiuPXNRSRMUuPL1cmd3v23rWtrY2NqRWfMqbf89i5u8JZ284tkXe7MXVsKosHOuNCTVAdcCFyQNsHzM7FozW2Rmixob+w965Vog2BtWrhzt8gWCq9pIP9WCeqSA2cDTqedNyTHF6q0q1OwZ9fzh/x3HvMYGug1O/ObvyjpcaUt7bwKA51/aUcKSuHI2t7GB7L6vnlwtquj1B+xR1POn17Gal1WWAxu9M8C5cWgTsG+MSEkiU/aJ2xN7A/OA5ZI2Ap8AzpV07VBPlk4I8FIM+d/Svqus/7+78cfMejsBvGFVlUr6qRazt2qI56VtZ2d83fIOV/KGlSvEjUsWM39WQ979NYIFsxq48axFAPz0z88WNYV/7zpWYY24dIjtuuZ2v7lxbpwxs2ZgFXBa3HQa8EiMWkmOaTKzmWY2x8zmAN8kRLecN9TzpedYffmXT2a2l/P/dzf+dMUezwk1oqbGQwGr0UgbVoX0SEEYoTog9Xx2PKZovVVDtXl7b4OlnMOV+jSstnnDyuWWZAz83WeOZUFWA0uEuYV3Lz2a4165F/V1tRjQbdZnPtZIGj5tO3vnWM2eUc9vPnVMJuTWoGjncc5VlAuAiyStBi6Kz5G0XNKiYp4onW792a0ejuzKk2cErH4j+mQL6ZGKbiM0mGri/Kv3Aj8uZm/VUM1tnNonXKlck1j4iJUbiqSBVZvKhGX0vbHY0ZV7PtZwe3XNLNOwapjcu4LD8y/1T7fuvcfOjR9m9qSZvcHMFsavT8XtJ5nZyhzHX2Zmnx7OuXbF69qkCTX9FlMv1//vbvzpnV/lo1XVqhhN5kJ6pL4PrAfWAA8AnzezDUU497DdsGQxc2b2Xmy/+r5DSlia/NINq795w8oVaKAbi3mNDWRnIB5Jr27Hrm66e4wpdbV9euHSZUifZ01zm49cOeeKKh0KmL2YumfTdeVil6dar3oj/mQL6ZEys24zu9DM5sVHzlC/kfRWDdXsGfXc8+ljOOagmQC87zt/KsubvXTD6gUPBXQFGujG4oYli5nf2D9ccLi9uulU67nKkMtorKXlnBu/0unWk5H7dV8+ibuXHu3ZdF3Z6E217g2rajVh8EOq25oXwho/Rm+Y0t1Ljy5toaKXd3X3SaPtI1auUMmNxUD7mlo7eM9Vf2BrRye7TanL2avb1NrB2Tc9xMbNHcxtDOuoZd+kbE9lBMx3nnOWrWBNaj2t0VhLyzk3fvlIgKsEnV1eT6vduP9k0/NAym2Sa2t7KFvjtEnUKDxPeuWcG6nZM+q5+vTXA7Bnw8ScvbofvP4B1rW0022Wd37UttQaVvnOc/fSo1kwq28Ioqdhd84Vy85Ov2F15c+TV1S/cf/Jzm2c2udmr9usbEICt7aHkYDGhknMmjYZM2je7qNWrnhef8AeTJ88gfUt7Tzd2rdToafHeKaA7Fr5RqyyZYcgnv2mA/sd09TawQlX3lfUVPDOueqXjFhN8oaVK2M7fXHgqjfuP9lc803WNLdx7NfvLfnNXTJiNaNhInvtFlJXe2ZAV0wTams4amFYF+4nDz/DcVfcy9xLfsEJV97HDX9c3+/4XB0P6TWsBpKMXP2/t70SgPvXt/Y75pxlK1jb0jbgCJlzzqV19xjdPYYU1gdyrlx1ZhYH9nparcZ9wypXemqIF+oS39wliSv2nDqRvafHhpUnsHBFduj+uwHwX79dy/rN7ZnMfV/8RVhkc8bUiX2Oz16TKl/yinzedejeAPz6ry/w5N+28ZZUJ8a6ljYszO0tu9DcsSCpXtKtktZKelLSOwc49tx43DpJ35JUk7V/sqQnJPVLa+1cNUlC5CfW1qDslKfOlZEkeYWHrFYv/2SjXKmhobTpoZOG1R71E3mFj1i5UXLLg9nrefe1x9SJOf82kgbW525/DIA7n3i+oL+R/fao5+B9prOjs4e3/efvM3O41jS3ERelzxiH6898GthmZvOBdwHXS+qXWlHSgcC/AUcAC+Lj9KzDvkhY3sK5qrYzrmHlN6uu3Pkcq+rnn2yUTk9dVyuy7yNLMXKVNKxmTPWGlRs9Tw/SGNrQ0h7WvsqzP3bA8VJHZ8F/Iy9s7794cC6fPGFh5vtxMv/qVOAaADNbA6wE3p7juJOB282sxcx6gOvizwIg6c2Extb3R73EzpVYOtW6c+Vsl8+xqnr+yUbpdS9+s/QY5s/q20lcipGrTChgw0ReEUMB/+ahgK7I8o3WQu/iwjcsWdzvbyKbUXjo3pa2XQPuT4pz1xPPZ7ads2wFa5urfv7VbODp1PMmYP+hHCdpKvBN4MLBTibpPEkrJa1saWkZdqGdK6WdXZ64wlWGXT5iVfX8k82hT3rorH1jeUOXaVilQgFf8BErV2Tp0doFsxr44blvZMGsvosLJ38Tv/vMsSzI08BKGmGFGKwxt8/uob7fseo5jr3iXppaO1jb0kYSKZief1VJI1mS/ixpc55HsbrbLweuMrNnBzvQzK41s0VmtqixsbFIp3dubO30tYFchej07JVVzz/ZAeTqpR/Lkat08oqeOPlk5dNby/7m0VWW9Gjt3UuP5o3zZvR5nl7fKruBVQPU1YoakWmEFWKwxly6N2/D5nZOv+FBSM2/ErD/nlM44cr7OOrye1gTR7LWlvlIlpkdZmYz8zy6CSNPB6R+ZDaQaxLcQMcdCVwqaSPwQ+A1kh4t+ptxrkzs8hErVyF651h5kpVqVVgar3EquYk84cr7WNvclr6vy0zcn9c4le+edXjOxVVHKjPHqmEi53//4cz25Obx7qVHF/2czhUi+dso5s+nn8+7ZHmffU1b+nYkTK4LN1BrW9r6bLfKzyR4G3A+sFLSAmAxcFqO434C/E7SvwOtwLnADwDM7JDkIEnHAFeY2aJRLrdzJZOEV/mIlSt3nV3hTtJDAauXf7IFGGh+ybqWdo4bpTWvtnT0ZgXcuLn3da2EmQqdGwv5QgWlMFr1cmcPTVs6MqnZ02ZNnzTq5RtFlwO7S1oL/Bw4z8y2A0j6vKQLAMxsPfAFQta/NcB64ObSFNm50kqnW3eunO1MRqy8E6Bq+SdbgD5zrnLc7HXFNa+y1/cZia7uHl7s6ESC3esn5rzRLOb5nCsn6VDBNDP6zLPKZfP2nRUx3yoXM2s3s1PMbL6ZHWRmd6T2XWpmV6eeX2Nm8+LjwhhKmP169/polat2Sbr1SXV+S+Nyk7RQ0v2SVsevC3Ic8wpJd0h6VNJfJZ2e2ve5uC7go5IelvTW4ZSj0zsBqp5/skNww5LFzG8cODMa9DZ4FvzLcuZe8oth3eBt7egEwmhVbY0yN5r5zveWK0dn1My5UkjP+1owqyHTqVAjmJBjKGvBrAZ+vfRoJtSIzjJY3Ns5N3Z8xMoV4GpCUp+FwFXEZS2yXAmsjOHURwFfkpRkZX0IWBz3fRi4VdKUoRai08NWq55/skOQPXE/s+ZVnjmInd2WSXYx1JGlrZkwwLo+507fZGafq9ijZs6Vg/To1bzGBnqy4v9qJe5eejTzZ/Xd1zPIfKum1g6O9w4J5yreLs8K6AYgaRZwGHBL3HQLcJik7FSohwJ3AphZC7AKeH98fpeZJf8kHiVEpc8Yalk8eUX18+QVw5CeeN/U2sE5y1awprltkJ8KDazjvn4v3T3GhBrRbdYnnXXa/z23DQhzuE648r7MMTcsWVzQ+ZIG1oQahfPVhq/5zudcucpOdHHClfexrqWNHuuf4n1eY0Ofv42B0r9/8PoHeGbry0DvMgqeEMa5ytObbt0XCHY57Q88m4RLm1m3pOfi9vQCfg8DH5C0EpgDvAnYmOP1zgTWmdkzQy2ILxBc/bxhNULJTV+hDayuODGkM35NGkB1NaKrx6iNDaH0KFj6pi/7fOtb2qmpga5uI9eUk8z5unvP95Yr76Wnp3fhV29kuUqSdC6sb2nP1OH0vjNvfJCNcfRpTXMbCz67PGcnxrOxUQV9l1H4wnsO5nN3PN7n9f1vxLny5enWXZF8CvgGYaSqCfgN0JU+QNLRhMRBJ+R7EUnnAecBzJ49u8++Xd2eFbDaecOqSLIbPOua26itVaZBM5ikoZU0hNLRTrlCmoY7agZ9G1lHXX4PdT6a5SrIQKneZ8+o597PHMvhX/w1zdt3Av07MRbMauDbHzosBHJk/XmuaW7jA9c9kHnuI1nOlb+dPm/FDWwTsK+k2jhaVQvsQ9YagTH8L52wYjnwf6nnRxCyr77HzJ7KdzIzuxa4FmDRokV9/sv4AsHVzxtWRZZ909dvZKnHcqaIHkh2uFO+cw61gZXIbmjV1oieHmPPqSFxxubtO6n1xperIK1tu/LuW9fSxpk3PoRZ+OeWhBHlkh7J8nrvXHna2RmyAnryCpeLmTVLWkVYE/Dm+PWR2JDKkDQDeMnMuiQdB7wGODnuWwzcCpxsZn8ebll651h5Xa1W3rAaZblGlgoZzRIwoVZ9QvYKPVf2ebq6e+dY1caQw4Ead92xh7+1vffmtCfHKFf6db3B5crJ3MapmXlY2XoMnn9pBwCfPvEgfrRyU95jE2ubfeTKuXKVLBDs6dbdAC4Alkm6FNhKmCeVjEpdamYrgcOB/5LUDWwG3pVKWPFtYApwjXrnapxhZo8NpRA+x6r6ecNqDOUbzcrVABpJQ2WgUKnhjmplSxqF6dGuY6+4h24LWdq6zfokzki/t9l7hve0acvLPofFjYpkHla+Tozk2etm785bX/2KQf8mjIEzDDrn+pK0EFhGyJzWCpxpZmuyjjkb+CTQA9QC15nZfw3lPE2tHVz/+/UA3LZiEx86/AD/f+L6MbMngTfk2H5S6vtfAv3Wt4r7Bu/dHkRTawe/eOxvAPznr1dz5PyZXlerkDesSmigBtBon3O4c8EGkrxEt/WdL5bdCNuYSmvtc1jcaMjXiZHdeLrkp49x99Kj+yWEmds4lc7uHpq2dOTMPuicG1SybtDNcaHVa4Djso75CXCTmZmkacDjku41s0cLPck5y1awtT2s+7i1o9P/n7iydc6yFWzfEXJhNG/f6XW1SnnDapwabPQsPbLU1NpRtMZXtsHWGnKuGJL6Pu+S5ZmGP/StewPNjyw0HNc512fdoCRz2i3AtyQ1pue1mNm21I/VA3X0SykzsPUt7Zkf8JFlV87SddPravXyhpUDChs9G+ncrVx8JMCNpfTcq0KTwjjnhqzQdYOQ9G7gy8A84JJ8c1bypbCe2ziVtS1tmIH8/4krY+m66vc+1WvEs+ckLZR0v6TV8Wu/+FRJtZKukrRO0lpJH0ntO1vSo5JWSXpM0sdHWiY3OpIbzfVfeQdrvngSG+LX9V+ytSapAAAgAElEQVR+B79ZegzzGxuoIaworjxfawRzZtQzZ0Y9tVJmLplzY+GGJYuZ19jgdc+5MmFm/2tmrwYWAmdIOijPcdea2SIzW9TY2JjZfsOSxcyPf9Pz/W/albF0XfX/P9WrGCNWhcRRfwiYT5gUOAN4RNKvzWwjRYixdqVXCb37BU6mPhH4EiHN6n+b2adT+z4HfADoBjqBz5rZXWNUfFcElVBPnasCBa0blGZmTZIeAt4J5F0jKJv/TbtK4XV1fBjRiFUqjvqWuOkW4DBJjVmHnkrI9tMT46tvB06BEGNtlgkgG1aMtXMFSjoBFgJXEToBsq0HPgJcnmPfQ8BiMzsE+DBwq6Qpo1VY55yrRGbWDCTrBkH+dYNelfp+JnAsMKT01c45V05GGgrYL44aSOKo02YDT6eeN6WPkfRuSU/EYy7PFWMt6TxJKyWtbGlpyd7t3IAK7QQws7Vmtgroyn4NM7srtabFo4TlxmaMXqmdc65iXQBcJGk1cFF8jqTlkhbFY86T9ERcvPU3wLfM7FelKa5zzo1cWSSvMLP/Bf5X0mzgdknLzeyprGOuBa4FWLRokY9ouaEqeDJ1gc4E1pnZM9k78k2yds658aLAdYM+OaaFcs65UTbSEatMHDWEJBXkjqNuAg5IPZ+d4xjMrIkQbvXOEZbLuVEj6WjgC/SGufSRb5K1c84555yrXrKh5sfOfgHpXuD6VPKKc8zs2KxjziLchL6dmLwCeLOZbZD0KjP7azxuJvBH4KKBwgEktdA3tDAxE9g8ojc0try8/R1gZkVvjcRQwNXAjNRk6lZgQXbcfzz+MqAhnbwibj8C+BHwHjP7cwHn9bpaGhVbV0uliupqIarxPUH+9zUe6mqlfaZe3ty8rpYfL29/eetpMUIBLwCWSboU2EoIkULScuBSM1sJfJ8QEpBkYPu8mW2I358XM7F1EuasDBpjne/NSFppZoty7StHXt6xY2bNMY7/NOBm8kymHoikxcCtwMmFNKrieb2ulkCllbccVEtdLUQ1vieo3veVLVddrbT37uUdH7yujr1Sl3fEDasC46i7gQvz/LzHWLuxMmgngKQjgR8C08MufYAwCnsX8G1gCnCNpOQ1z8i3oKVzzjnnnBs/yiJ5hXNjocBOgD8A++X5eV/NzznnnHPO5TTS5BXl5tpSF2CIvLzjV6X9Lr2841c1/i6r8T1B9b6vQlTae/fyjl+V9rv08g7BiJNXOOecc84559x4V20jVs4555xzzjk35qqiYSVpoaT7Ja2OXxeUukxpkmbE1eafkvSYpJ9Kaoz73ijpL7Hsv4ppwcuCpH+TZJIOjs/LtqyVwuvq6PC6OnyS6iXdKmmtpCcl5V1HUNK58bh1kr4lqSZuP0ZSh6RV8fHg2L2DPuUb9O9LUq2kq+J7WCvpI4XsK6UivK/LJDWnPp+rxvYdjK5yvq5W6jUV/Lo6Gryujo6yqqtmVvEP4LfA6fH704HflrpMWeXbEzgm9fxy4AZCw3YtcGTc/q/AjaUubyzLYcAvgY3AweVc1kp6eF0dlTJ7XR3Z7+9S4Lr4/QLgecIabtnHHQg8AzTG3/FdwJlx3zHAyjJ4L4P+fRGygd4V30NjfE9zBttX4e/rMuCKUr+PUv5+Sli2irumxvL4dXV0fq9eV4tf7rKqqyX/hRThFzoLeBGojc9r4/PGUpdtgDK/D/g1sBh4PLV9JtBWBuWbBNwPzElV1LIsayU9vK6OSvm8ro78d/gEsCj1/OfAKTmO+wxhncHk+cnAL+L3x1DihlWhf1/ALwhr0SXPvwV8ZrB9Ff6+LqNKG1aVdl0t92tqLItfV0fn9+p1tfhlLLu6Wg2hgPsDz1pYK4v49bm4vezE0JkLgf8FZpNakdvMNgM1kvYsUfESnwduNrONqW3lWtZK4nW1+Lyujlyf3xfQRO46OdhxCyX9WdKDkpYUv5iDKvTva6D3UejvYiwV430BfEDSozEs5ojRLPAYq5jraoVcU8Gvq6PF62rxlV1drYaGVaX5b6CN0JtYduI/3EWExXDd+OZ1tQrExs7mPI/aIp3mz8D+ZnYY8AHgUknHF+m13chdDRxoZocQwnvukDSjxGUaj8r6mgp+XXUZXleHqRoaVpuAfZMbhPh1n7i9rEi6gjCH4VQz6yH0KB6Q2j8T6DGzLSUqIsDRwKuADZI2EhbLvQuYT/mVtdJ4XS0ur6sFMLPDzGxmnkc3WZ8tobcvV53Me5yZbTOzl+L3G4Dbgb8fjfczgEL/vgZ6v4X+LsbSiN+XmT1vZp3x+7vj9oNHudxjpSKuqxVyTQW/ro4mr6vFVZZ1teIbVmbWDKwCToubTgMeMbOW0pWqP0lfAl4PvNfMdsbNDwNTJB0Zn18A3FaK8iXM7Ctmto+ZzTGzOYQJ0G8l9HKWVVkrjdfV4vK6WjS3AecDxAxVi4E7cxz3E+C9khpjmMi5wI/iz+0tSfH7PYETCXV9zAzh7+s24FxJNTHj1XuBHxewrySK8b4k7ZscJOm1hPkIT41y0cdEJVxXK+WaCn5dHU1eV4urbOvqWE3mGs0H8ErgQWB1/HpQqcuUVb5XA0b4R7YqPn4W970JeAxYA9wN7FXq8maVfSNwcCWUtRIeXldHtexeV4f3e5tK+KezNn7u70nt+zxwQer5+cC6+PgOvZOw/4mQBGMV8DglSviQ7+8LWE5M0EGYMP6d1Ps4L/XzefeV+DMa6ftaFj+XvwArgJNK/Z7G4vdTDo9KvqbGMvp1tbi/T6+ro1f+sqirigVwzjnnnHPOOTdMFR8K6JxzzjnnnHOl5g0r55xzzjnnnBshb1g555xzzjnn3Ah5w8o555xzzjnnRsgbVs4555xzzjk3Qt6wcs4555xzzrkR8oaVc84555xzzo2QN6ycc84555xzboS8YeWcc84555xzI+QNK+ecc84555wbIW9YOeecc84559wIecPKOeecc84550bIG1bOOeecc845N0LesCoRSfdKuqzU5XDVpVj1StIxkmyQY34p6bMjPZerTsWoi5Iuk3RvcUo04HmekPSh1PPXS1olabukmyR9SNITY1kG55xzlccbVmVM0qGSbpX0vKR2SU2Slkv6h9QxQ7rxyHezE28ebipKwV1ZizeN/ytpi6QOSX+V9FlJdUN5HTN7u5l9qUhlOkvSxmK8lqsckg6R9KN4jWuTtF7S9yQdPJblMLNXm9n/pDZ9GbjXzKaZ2Vlm9j9m9upinEvSHEkmac4gZXDOOVdhvGFVpiS9BXgAeBZ4IzANOAj4b+AfS1g0V8EkHQf8Afg/4O+A3YHzgbOA2yX5NcGNCUnHAA8SrnFvIFzjFgF/BN5TupIBMBdYVeIyuCog6SuxIX16jn33StoVOxW2SXpc0jk5jtsnvs6jsUPsWUk/l/S+POf8ROyI7ZD0R0mHDlLGjZJ2xHIkj3cO/127sVat9UzS6yT9KZ6jSdLHBzl+T0k3SHouRhzcIWm/1P6kY6s9qxy7DfS6Q+E3UcMg6aOSnszaNi1+OMfF51+QtDZuezo+H8rv+2rgFjNbamYbzazHzF42s1+a2RkDlG1PSTfGStUs6SfpSuXK1xjVq+8APzGzi83seTPbZWa/I9zIngi8P+v8H5S0QdKLkn4qqTG1r8/op6R9Jf0gXoybJd2SdXy9pC/H8m+XtEbS+yS9mVDfZ6cucu9NXQBPjxf67fEC+8rUa9ZK+pTCqNtLkh5W6JRI9h8q6b5Y/q1x/0Fx37GSVsafa43/GPYYwu+yao1RXbwG+JGZfdLMnrZgi5ldY2ZfzFOujymEzG2P9ewqSfWp/e+P+7dJ2izp16l9/yRpXfzZF5QaoVf4h39WrE9thIbV1fG9vU9ZI6qSJkj6TKx32+P7/1jct7ekX8S/gW2SViS/sygJKXwivv7X02VInePIWN9fjL/niyXVpvZb/Jz+FF/nUUlvGsLv340ySROBDwOtwAV5DvuSmTUAewBfAa5X6HRIXuOdwJ+ADuAUYC9gHvA14AyF6IMpqeM/AFxKuJbvCfwKuFPStEGKe4GZNaQePx/yG3YlUa31TNJ04E7grniO9wOXSTp5gNdfBswidBzvHd/P/5fjf9Ors8rx0iDlLpyZ+WOID0Iv/8vA36e2fQRYByg+Px3YDxCwGNgMnJs6/l7gsjyvvxAw4PgCynIZIWQleb6cUAlnEnqAvw/8Gagd6LzATcBNpf7djudHqesVYaTg5vj9MfHYOwgX4j1i3fplrnMBk4Anga8CU4GGWPfuTh1/C2EUdmF8vj9wSPz+LGBjVnnmxDLcRbjITwZ+CvwmdcxlsX4vJHQU/QPQBsxLvadLgQnx8Vpgr7jvWeDs+LucCBwBTC11PSiHxxjUxQUD1cWsz/fe1PN/BObHc74SWAN8Me6rB3YBx8Xnk1PfLyD8gz04Pm8Ajkq97kbgrAGe96mfhFDB1cDrY1kagcPjvv1iPZwa69W/Ai8BM7Pq9Zys95o5J3BALO8FQB1wCNAELE0db7Huz4t1+7+BdaWuO9X6iPX5P4GfANuB9cAJwLHAY8A2wvVyeupnPhj/jt4RP6+Dc7zmZVnbNgOfit8fGuv4gQOU60vAN7Ne86up5zXA34AzB3iNPvXdH17PyqGeEa67zwE1qW1fBX6b5/ipQA+wKLVtfvydvDk+n0OO628xHz5iNQxm9iKh0qeHUs8BbrT4yZnZzWb2jAUrgP8Bji/wFEkv/7PJhth7+WLsXd8h6YDsH5K0N/B24JNmttnMtgP/RPijWTzEt+nGWCnqVZZnCD09aReb2VYz2wp8CnhbrGfZ3kG4sb3YzNrNrA34NHC8pP3iyNUHCL1Vq+N72WRmjxZQ7n83sxfMbAdwI3B4at8ngc+Y2WoLo7o/A34PnBb37wJmAweYWZeZrTKzF1L75gH7WBi5u9/M2gsoT9Ubg7qY1LN8dTFfuX5qZmvjOZ8Evp11zk7gVZJmmtkOM/tt3N5FaAC9WtJ0M2uzMFI7ZJJEuK7+s5k9HMvSYmYPxTI+Y2Y/i38Hu8zsPwj/yIdyDf4g8LiZXW1mnfHv5GvAeVnHXWFm68ysizACOFfSjOG8L1eQ04ErCB0PPyR0Hn0UOBo4kBCu/8nU8RcSIgR+QQi/vjDfC8dR0DMIPfMr4uYvAheZ2QaFubH3x9H1OxXmRZ8BfI5wXU4+90OBlcnrmlkPoQH+2kHe29cUwr8el/TPGuKcW1dU47KeKUTIvJg69lDgkfjaiZUDnENZX9Pfvy7r2D/GqIY/KZW3oBi8YTV81wPvl9Qg6e8I/zS/m+yUdKFCVqmtsaKcT/+b1nxa4td9kw1m9gcz251Q0SbRt+Ik9o9f16d+7qX4erPjpk5CD2i2urjPldaY1qss+wHNWds25Ph+f/pbAOwDbI0dAC8CTwE7CXVvTjzuqQLLmvZc6vs2wmgDkvYCpgM/S84Zz3sUve/xLMJN7W8lbZL0DUlT4753E0K+HlYIS/y3dKiVG9W6mNSzfHUxJ0knS3og/kN8iXBDMAvAzDqAtxEaWk/F0Lh/ivs2EBr2ZwNNkh6U9P7cZxnUTEIdzFmX1RuOvTGGAr5IqKeF/m4g/I2tz9q2lt7reCL7bwNCpIIbHT+OHTDdwM2EkfQrLYSwthJG9RcBKCRgOZLwd0T8enrq+pO4ONaR54FPEHr0fydpEiFc6c7YmP8p8E1CPfos8E5CJEo3YSRjYXy96cCLWedI6mA+SwidTLMIjfcLgP8o9Jfiim5c1jMz+0G8z00M6RyxQ/e3wL9LmqEwb+qLhHuA5Lq4GXgToYG6P/At4BZJJw1Q7iHxhtXw3UcY9jyVENt6p5k9BxDj3L8JfBxojBXlGnI3hvqJPfrrCL2WQ7Epfj0w2RBjVGcSwkgg3BwvyPGzC+I5XWmNdr1aC5yZvU9h3tHhwC+yds3J8f0zOV7+eWC9me2e9ZhsZn8ihABA70U5W0+e7QN5EdgBvC3rnFPN7EIAC3N3zjWzAwihFCcC/xz3PWZmHzSzVxBiyj9Kjt/NODaadXENIZSu4PTiCnNFbyX05O5rZrsB/5I+p5n93sz+gXDN+zhwhaRj4747zOxtcd/XCf9M5xV6/pTNhEZMvrr8FcI1+O+B3QhhtNtS5Sykrm8idR2P5tF7HXel8bfU9x15tiU3cBcCa8zs3vj8e4RO0ew6/5V43ZppZq83s+/F7TMI11UIdbbezG41s24z+zMhFCtxQOrYbYR6l7Z73J6Tmd1nZtvjqP6fCOHTeedyu1Hn9WyY5yCM9m0BHiXMZ/0j4Xq9OZahLTZad1nIW/ADQuO1X9KP4fKG1TDFcJgbCb20Z9DbWwChInQTRgi6FSbnD3V9ko8CH5T0dUkHSKqJPQtHDlCmvxEm+l0paaakBkLc/RP0DvkuA96jMCF7oqQpCpOuX024aXElNEb16v2SviRpL0l1ko4kxGz/BvhR1vFflrSHQlKHy4FfJTfXWX4KTFZI/78bgKRZkk6N76uFMMfq25IWxP37STok/vzzQKOGkDzCzHYSkl5cLulVCqZIOkrSwniOs+J5RLgYdxF+dxMlna3e5BovEX633YWev9qNQV08HzhV0uWSZsfPb3dJ5yj3+mjTCP+zNpvZzlh3PpbslPQKSadI2j2W/UVCT2W3pIMknSSpIYbNvURo6Az5846v/d/AVxUyVklSo6Qk1G83wnyHrYR5Xv9BHGWNWgiNq4MGOM0twGsknRf/Rg8mdAhcP8DPuDIR//eeAeyvsJTA84T/w7XkTy6QrRV4Rfx+M9Au6VSFBCuHEEbm6yVdDDwfR2UB/kIczYhlqSGEQQ0ly2UPBXaSuNIZB/XsL8Dr1DfxxOsHOoeFpFynm9m+ZrYfYQRrGn0biEMtx5B4w2pklgGHEf55pzOb3AXcQGgpbyH0nA5pfRIz+xVhuHI28BBhEuMaws3Le4Gn8/zo6cALhCHbDYQK9a44jIuZ/RE4GfgM4Wa2iTAh/PjUH4wrrdGsV3cDbwZeQ0g2sS2+5s3Au5N6knIbIW56I6FRkrN3ycJ8viMIveyPSdpGyDB0VOqwc2PZ71LIvHYPYWIphIvfL4C1CiF97y7wLX2a0Bi8jXAjvRG4hN5w12MJfz9thIv0/YQGIoS/gycktRNGZ26KvwfXazTr4r2EOnMAIW5+O/AIoX7enuP4vxISQdwa69cVhN7ZhAg3E+tj/fox8FkLc6kmEka3no0/+3XgDDPbOJQyp1xKqC8/jOVeSe9NxucIjasWQrjgC6RGec3sZUKIzbJY17+W471uJIQ1nk242bkDuBb4xjDL68bWh+hNlpN+vINwo/iGwV4gdhytk/SW2Jh/H2FezXOEUdE76E1ucmrqR68GzpV0eOyMTUZ1f5brPJIWxM6oybED93Dg84TGvStv1V7Pfhrf379ImhR/5lxCduOcYifazNjh9WpC+PoNZvZU3P9mSX+nMNdsYuz8PWOQcgyNlUEWFH/4wx+V9yAkifhsqcvhD3/4wx+j/SArsxo5sosRRifvJXQQfDvP6/we+G6u18xx7OGEkNn98+yfkGf7JwnhpC8TOrgOTe2bTehoenPqHH8hdBBsA/5KaPjXlfp3Ph4f47meERqKbVnneB2hQ/TleK6PZ+3/JXB16vmHCUmROggdrZcSs2LH/Ul223ZCp+ADwCnF/AyTtLnOOVcwhbl7TxEucreVujzOOVeNFBZn/TphtPJ2ws3lHsBJhBHc91oYzXVu2LyeFY83rJxzQyLpCMJcvjuB083Ms0k659woUVheZSkh+c4+hB7/3wP/aWYPlrJsrnp4PSsOb1g555xzzjnn3Ah58grnnHPOOeecGyFvWDnnnHPOOefcCE0odQGGY+bMmTZnzpxSF8ONgocffnizmTUOfmRl8LpavbyuukrhddVVCq+rrhIMVE8rsmE1Z84cVq5cWepiuFEgKd/6XBXJ62r18rrqKoXXVVcpvK66SjBQPfVQQOecc84555wbIW9YOeecc84559wIecPKOeecc84550aoIudYZWtq7eCcZStY39LO3Map3LBkMbNn1Je6WM7143XVOecqU77rd3r7/ntOyRxbWyu6e4x5jQ184T0H87k7Hmddc1tm++w96/sc29VtTEh9TX7W/0+4Uhqofntd7a8qGlZn3PAgT2/pAGBdSxvnLFvB3UuPLnGpnOvvtOvu59kXdwBeV51zrtwkN5HpBtA+u0+htkY83dqROW5NcxtHXX4PtTXhmMTG1DE93ZY59gPXPdBve65jO7O+Zv+faGrtYMl3H2Lj5vbMubNvbIE+7yG7wZbrZjj7Z7IbhOmb6k1bXmZu49Q++7yjsPJkdxRkN/67uo0JNaJzkPqdr64Wo0yVWKeqomG1aWvqgzZY39JewtI4l9/fXtqR+d7rqnPOlVZTawdnffchnm7tYG7jVDq7e3i6tQOj98bxma0v5/35dKNqNGT/nzj56j/RvH0nAF09fW9sk8aeBBaLla/Blv6ZY79+b5/3ka9BuDGrYZnel5x7Qmzs7b3bZCZOqKGptYOauC1fQ7DSbpwrSXZHQbpRDaF+Qf7Gf+cQ6nd2XS1khDfZDnDWTQ/1+fmkTtVllXv2nvWYwaYtHX0agF09/TsOZjVMottgc9vOYXcuDLWOVkXDavae9Zk/+BrB3MapJS6Rc7nts9sUnnkx/JP2uuqcc2MnO6TJoN8oVDmaM7OejZvb+Ydv/5GtHZ2DHm9DbOsVs3GYNPaeS3cixm3ZDcG1HrUxappaO/jg9Q/06RTIbmQXm7LuadINpfRo1tk3PcS6uD3TGQDkK1V2Z0DOUbOe3B0Hz8dOiFz7sjskatTb0OzJ2jd/1lRuXHJ4QQ2sqkhe8fVTXpv5Pt3ydK7c/Ms7XpX53uuqc86NnXOWrWBtSxvdZmxs7ejTqCqWOTPqmTOjnhqgrlY5j6mrFTXqf6yyvibWt7Rz7BX3FtSoqiTmURuj5h+/88cBR1qHK1edTWrqK6ZP7nNPsyH12fZYaKTM/+zyTKMqbXTHfQszUP/CupZ2zlm2oqDXqYoRqzkzQwtyj/o67/lwZW3OzNCbc9Be07jrk0eVuDTOOVe9ssOghtpTX5eVZCJ7blEhIUMjmTNy5Fd/yzNbX85705kOkUq/N0EIc+q2QedY1cYQqvQoV12e39WcWO6NRWqQetRG8TW1dnDqtfezuW3XoMcm9STXZ50dfjdQSNzFP3mUH67YxMeOnZ/Z35OqW2nZzyvFUDoBqqJhNXFCGHjb1dVT4pI4N7DJdbUA7OjqLnFJnHOuup2zbEUmvK9nCI2qGoWIgoE6agvtxJ09o37YHb5/e3FHzu25yjfcBtxQ5sEkrzdY0oOhZjx0xfPB6x/oM5c7LV9jCRhRwohZ0ycD0Lyt97wPbtgS5jzlaFzlkjTyenrIn0ijwLo1lGMH6pBI7xtKJ0B1Nay6vWHlytukWFd3dHrDyjnnRtNAPczJ6MtAGe5KbW7jVNa1tNFj/W88s8s33AZcvp8b6PVy7fNoodLJjMzGupJtwazBEzCM5PObNW0SQCapCsBtKzcBcOEx87jz8ef7la1GIT9CXW1N3gbdWNSpoXQgFHpNqIqGVV1NuFnt7DZ6eoyamtxxzc6VWjJitdNHV51zblTNmVnfbz7HQKNR5dY4uGHJ4opPPe1GXzJ3MDtpSSEjr8WwVxyxemHbDppaO/okp3jTvBmc8vr9i5JtbzQMtQOhEFXRsKqpUSYmuLOnh0k1taUuknM5Ta7zESvnnBsLV5xyKP/w7T8BvfOlKin8bCRhhK76JSMqubJZ1kpjNvKaHrFKOgISl97xBHcvPXpc1eOqaFgBTKytobO7m11dPUya4A0rV56SurmjswczQ/LRVeecGw27108E4IAZ9dz3mWNLXBrnRiY7NG1XXHMtbaxGqdJ6R6x2srV9V59kK+Mx62NVpFsHT2DhKkNtHF0FDwd0zrnR1LajC4CGSVXTh+zGsTNvfJA1zWG5gHUtbTmXCyjFiOzMholI0Nq+kwMbe0P7xPjM+lg1Dau62t55Vs6Vs8kTfJ6Vc86Ntu07w7pP3rBy1SDdkMpOUlGjkKTi7qVHj/m8pQm1NcyYOgkz+Oo/HkptzHNwwIz6igm7LaaCGlaSFkq6X9Lq+HVBjmNqJV0laZ2ktZI+ktp3maRmSavi46rUvnpJt8afeVLSO4fzRnzEylWKSUkCC59n5ZxzoyYZsZo22RtWrrJtbR94XapSzx1M5lnt6Oqmu8eYXFfDbz91TMmTU5RCoSNWVwNXmdlC4CrgmhzHfAiYDywAjgAukzQntf97Zvba+PhYavungW1mNh94F3C9pIahvY10ynW/WXXlrTeBhXcCuNxG2pmVOuYgSR2SrhibkjtXPtp2eiigG1yB19u8AwRj4QcPNWFA/cT+OQRqpZKMVKXtNT00rB5Y3wrAnBlTx22G7kEbVpJmAYcBt8RNtwCHSWrMOvRU4Doz6zGzFuB24JQCynAqsaFmZmuAlcDbCyt+r4kxFNDDq1y5y6xl5YsEu/xG3JklqTb+3O2jXVjnykFTawcnXHkf8y5ZzglX3kfTlhA61eAjVm5ghVxvIf8Awaha29zGlXevBmD3+jrmzKgnabMMZeHa0TRrWkhgkTSsyqFMpVLIiNX+wLNm1g0Qvz4Xt6fNBp5OPW/KOuYDkh6V9CtJRwzh5wCQdJ6klZJWtrS09CtkMmLlc6xcucusZeUjVi6HInZmXQz8HFg9ykV2rixkT+7//v3h1qJhUl2JS+bK1RCutyXzweseoDtOqnr+pR1ACP2rlUoeAphIRqxWbXoRgLkzhxx4VjXGqhvnauCLZtYp6QTgDkmvMrPWQl/AzK4FrgVYtGhRv9ZTMmLlc6xcuUsaVj5i5fLo15klKenMSvcq5e2UknQo8FbgWOBz+U4k6TzgPIDZs2cX8S248U7SQmAZMANoBc6MUSnpY4xsXMIAACAASURBVF5BGB04EKgj3CfcPNxzJiNUECb3b4nzUnyOlRtAoddbCAMEJwLPA/9mZvePduE2bemgefvOzPMeg01bXmbdl08a7VMPSWNMuZ4MbviI1cA2AfvGsJIkvGSfuD2tCTgg9Xx2coyZPW9mnfH7u+P2gwf7uaHw5BWuUmRCAT15hRsFkuoInVAXJDcL+ZjZtWa2yMwWNTaWTQetqw6FhFddCaw0s0OAo4AvSeoXsVKoxjiBHkKI1PQpoUHlDStXBFcDB8a6ejlhgGBGrgMHi7AqRFNrB8dfeS9v/to9fbaXS+hftr1Sf3sAcxvH74jVoA0rM2sGVgGnxU2nAY/E0JO024BzJdXEIdT3Aj8GkLRvcpCk1wJzgKdSP3d+3LcAWAzcOdQ30ptu3RtWrrx5KKAbxEg7s/YG5gHLJW0EPkG4Nl87yuV2DhhSeNWhxP/38Z5iFfD+4Z53yRFzMt/Pa2zg8Dl7Ap68wg2ooOvtIAMEZB074g6rc5atYF1z7+K6dbUqq9C/bLPiiFWiHBt/Y6XQrIAXABdJWg1cFJ8jabmkRfGY7wPrgTXAA8DnzWxD3PclSY9L+gtwHXCGmT0f910O7C5pLWE+wHlmtn2obyQZsfLkFa7cZbICeiigy2GknVlm1mRmM81sjpnNAb5JmIt13hi9BecKnZv9MCG8SpIOBN5E386CjEJGAXarD3OpZjZM5O6lR5PMGfCGlcun0OvtIAMERbe+pZ30nJeeHlj35ZNKnv0vn2SOFcDMhklMnzx+5zUWdLUxsyeBN+TYflLq+27gwjw/v2SA126nsOyBA+pNt+4NK1fekgWCPd26G8AFwDJJlwJbgTMhdGYBl5rZSkJn1hsInVnQtzPLuUrwKeAbhBvbJuA3QFeuAwebZw1kJvi/2NGJmbE9rmPlWQHdIAq53n5J0uuBbmAXfQcIiu6AGfWs3xxGrMo1/C9tZsMkJDAr/7KOtqq52kzy5BWuQkyq8zlWbmAj7czK+pnLilo45waXCa+KyQDyhVe1AKcnz+ON7P8N96RdceJ8V4+xfWdXZh2raZ4V0A2gwOtt3gGC0XD238/hc3c8AZR+8d9C/O3FHdQQWp1P/m0bTa0dZTmyNhYKDQUsez7HylWKSXHEysNWnXPVaAjhVTMkTYjfHwe8BvjBcM/b1dN7TX2xvbN3gWAfsXIVZm1zGwBLT1hYtuF/aecsW0Gy2tH2HV2cs2xFaQtUQlXTsPKsgA4KXkH9xBirv1PSFXle5yBJHen9kuol3SppraQnJb1zOGXMpFv3ESvnXPUqZG724cBfJT0JfB54l5l15Hy1AnT19EYIbu3YRVsSCuhzrFyFeWD9FgDeODdn4sGys76lN9GGZT0fb6rmauMNKxclKX5vlnQ6IcXvcVnHrAc+ApwMTM7al2QFuoaw4Grap4FtZjY/Nth+L2m+mbUNpYBJ8oqd3rByzlWpAsOrfgn06/waru7u3obViy93sj0JBfQRK1dBWtt28tQL25lcV8Oh++9W6uIUZG7jVNa2tGFWGXPCRlPVjFgloYCevGL8KjTFr5mtNbNV5JkkDVxMyFC5Omv7qcS1WOJClyuBtw+1nEko4A7vBHDOuaJJj1g1b9vBrq4eJtQos3agc5UgGa1adMCemfuFcnfDksXMb2wo65TwY6VqunF8xMoxtBXUc5J0KPBW4Fjgc1m7ZwNPp5430T998KB8xMo554qvO9Ww2rT1ZSDMr5JUqiI5NyRNrR189mePAfDXCkoCMXtGPXcvPbrUxSgLVdONM8nTrbsRklRHSOd7QdI4G+brDLjeSu8cK6+rzjlXLOkRq2e2hKlaPr/KVZJzlq3gpZc7AdjSsWtcJ4GoVFVzxZno6dZdgSl+B7A3MA9YHns4dwckaXpcXLWJsHhl0lqaDdyT/SKDrbfiCwQ751zxdaU6Vjdt9YaVqzzrWnqnbJuN7yQQlapqRqzqasNQv6dbH78KTfE7wM83mdlMM5tjZnOAbwLXxUYVwG3A+QAxecVi4M6hljOTbt1HrJxzrmj6jFjFUEBPXOEqhZllBgnAk0BUqqppWE2MN6s+YjXuDZriV9KRkp4BlgLnS3pG0lsLeO3Lgd0lrSUktzjPzLYPtYA+YuWcc8WXnmP1/LYdAEyb7IsDu8rw0IYt7OjqoUZ4EogKVjVdOZ68wkHBKX7/AOxXwGtdlvW8HThlpGWcPMHXsXLOuWJLj1hZ/NZDAV0laGrt4Oybwnyq3evruP2jR1ZE0grXX9WMWCWhgDs9FNCVuUnJiJWHAjrnXNF09/S/pjZ4KKCrAGd99yE6doXO1hc7Oj1pRQWrmoZVkhWw00esXJnLzLHyuuqcc0WTHrFKTPMRK1cBNrb2Jqno8aQVFa1qGlYTPd26qxBJunVfx8o554qnO0fDykMBXSV4xW6TM9970orKVj0Nq1pPXuEqQyZ5hTesnHOuaLq6czSsPBTQVYAz33gAAAJPWlHhquaK4+nWXaVIQgF3eCeAc84VTVeuOVY+YuUqQNIncN5Rc7nkpFeVtjBuRKpnxMqzAroKkYxYeSigc84VT65QQF/HylWClu07AWicNqnEJXEjVXUNK08I4MpdMsfKR6ycc654kuQVtTXKbGuY5OtYufLXvD2suzZr+uRBjnTlrmoaVpM8eYWrEHW1NdTWiO4e89BV55wrkmTEas+pEzPbfI6VqwTN28KI1Swfsap4VdOwqquN6db9RtVVgEk+wuqcc0WVJK+Y2dB7c+pzrFwlaN7uDatqUTUNK59j5SpJJhzQ51k551xRJMkr0vNUfI6VK3dm5qGAVaR6Gla13rBylWPyBE+57pxzxZTMsZrZkAoF9BErV+a27ehiR2cP9RNrvb5WgappWNVNSEIB+2cFcq7c9I5YeUeA60/SQkn3S1odvy7IcUytpKskrZO0VtJHUvs+J+kJSY9KeljSW8f2HTg39pI5Vo0xFLBGUD+xtpRFcm5QLXG0ai8fraoKVdOw8hErV0l6s1j6iJXL6WrgKjNbCFwFXJPjmA8B84EFwBHAZZLmxH0PAYvN7BDgw8CtkqaMdqGdK6VkjlVNvLPpMTjxG7+jqbWjhKVybmBJ4gpPtV4dqq9h1d2DmY9aufLmI1YuH0mzgMOAW+KmW4DDJDVmHXoqcJ2Z9ZhZC3A7cAqAmd1lZsnd5KOAgBmjXnjnSigZsfrpn5/NbFvX0sY5y1aUqkjODcoTV1SXqmlY1dSIutqwdoWnXHflzhcJdgPYH3jWzLoB4tfn4va02cDTqedNOY4BOBNYZ2bPZO+QdJ6klZJWtrS0FKXwzpVKkrwiWWwVwqjV+pb2UhXJuUFlEldM81DAalA1DStIp1z3EStX3iZNCCNWnm7djSZJRwNfAE7Ltd/MrjWzRWa2qLExe0DMucqSJK/Yf896kjWCawRzG6eWsFTODSyzhtV0H7GqBlXVsPKU665SJCNWnhXQ5bAJ2FdSLYQkFcA+cXtaE3BA6vns9DGSjgBuBt5rZk+NaomdKwPJHKuvve8Q5jU2UCsxr7GBG5YsLnHJnOurqbWDE668j3mXLOeHK8Jl20MBq0NV5XX0BBauUmTmWHnyCpfFzJolrSKMMt0cvz4S51Gl3QacK+mnhPlT7wXeDCBpMXArcLKZ/XnMCu9cCSVzrGbPqOfupUeXuDTO5bfkuw+xYXMIUW3b2QV4KGC1qKoRq95QQG9YufI2eYInr3ADugC4SNJq4KL4HEnLJS2Kx3wfWA+sAR4APm9mG+K+bwNTgGskrYqP14zpO3BujCWhgLVJHKBzZWpja/95f3t5KGBVKGjEStJCYBmhV7QVONPM1mQdUwv8F/A2wICvmNn1WcccBDwCfNvMPh233QQcD2yOh91mZl8czpuZFEMBN2xu58M3rWB9SztzG6dyw5LFzJ5RP5yXdG5UTPLkFW4AZvYk8IYc209Kfd8NXJjn5z32yY073TF5xYSaquozdlVoj/o6trR39tnmI1bVodCrz0jXVEkaXtcQUgJn+4qZvTY+htWogt45Vv96++OsbW6j28xTrbqy1BsK6CNWzjlXDD5i5SrFSa/Zu8/zuloxfUpVzc4ZtwZtWBVjTZXoYuDnwOoRlzqPpGH1zNYOkryAnmrVlaPJEzx5hXPOFVOSvGKCN6zcEEhaKOl+Savj1wUDHHuQpA5JV4zknFMn9m1EdXabL2ZdJQoZsRrxmiqSDgXeCnwjzzmWSnpM0u2SXjWE8veRzLHaZ/cpmW3CU6268jPJFwh2zlWxQm5WJc2S9AtJj0r6q6RvSxp2t323j1i54SkkKmuwyKshSdZbTddUj7CqDqMeiCypDrgWuCBpnGX5F2C+mb0G+ClwZ5JmOOt1Bl3IMskK+KkTFmbWsNhr+mRPterKTjIfcKdnBXTOVadCblY/C/zVzA4BDgFeD/zjcE+YLBCcdLI6N5ghRGVBESOvciVZ8wir6lDI1Weka6rsDcwDlkvaCHyCkCL4WgAze9bMeuL33wMagP2yC1HIQpZJKOD0KXXEjis+deJCT1zhys5kH7FyzlWpIdysGjBNUg0wCZgIPDucc/b0WOb/vg9YuSEoKCqrgMir9LGDDgQkYauN0yb5YtZVZtCGlZk1A8maKjD4mio18eL5XuDHZtZkZjPNbI6ZzQG+SZiLdR6ApH2TF5D0VqCbYV5Yk16q1rZdmW0du3xEwJWfpGHlWQGdc1Wo0CkEXwAWAn8DngfuMrM/5nrBwW5Wu613fpXkLStXPAVEXvVRyEBAEgp4zpEH+mLWVabQWOYLgGWSLgW2AmdCWFMFuNTMVhLWVHkDYU0V6LumykCWSdoL6AG2Ae82s64hvIeMJLyqefuOzDZvWLly1BsK6CNWzrlx6xTgUeAtwDTgl5JONrMfZx9oZtcSbm5ZtGiRZe/3+VVumDJRWWbWnScqKx15BbA7IEnTk0GCoUpGrF6x22RfzLrKFNSwGumaKlk/c1nW8+MLKUMhJmYaVjsz217eNaw2mnOjqjcU0Bv+zrmqU8jNKoTFrz8cpwO8JOkO4FigX8NqMMmcFc8I6IbCzJolJVFZN5MjKsvMmoCZyXNJlwENyXqsw5HUV58PWH2q6hNNkle0pBpWPmLlytHkuEDwDk9e4ZyrMkOYQrABeBuApInA8cDjwzmnj1i5EbgAuEjSakJj/wIIUVmSFo3GCTt9aYCqVVWrkdVNCBW0T8PKRwRcGfLkFc65KlfIFIJPAFdLegyoBe4BrhvOyZLFgX0EwA1VIVFZWdsvG+k5MyNWE7y+VpuqalhNrA03q31DAb1h5cqPp1t3zlWzAqcQrANOKMb5fMTKVZKkYTXROwKqTlV9oskcq76hgD7HypUfH7FyzrniSUasPLTKVYIuDwWsWlXZsHo5Ff7nc6xcOdoSlwRY29zGCVfeR1NrR4lL5Jxzlas73qjW1vqNqit/uzwUsGpV1Sc6MccF1UMBXTm65GePZb5f19LGOctWlLA0zjlX2bp6kqyAVXVb46pUUl/rvL5Wnar6RCfmaPm3e8NqXJG0UNL9klbHrwtyHHNiXGhyp6QrsvadLelRSaskPSbp46l9l0lqjvtWSbpquOVMj1D1GKxvaR/uSznn3LjX5XOsXAXp7IrJViZ4fa021dWwyjEJ0NexGneuBq4ys4XAVcA1OY5ZD3wEuDzHvp8Ah5rZa4E3AZ+SdEhq//fM7LXx8bHhFnJu49T/v717j46rPO89/n10syUbcJBlgg2K8Y3SkEt97KRQEnAaJytpLpzVpIETwCWAgbSkDQnrhOSUULLSG5R2nUDKpU5xQkIpIc2luEmABHKScrELDpdA8BVhgyNZ+CYJ29LMc/7Ye4+2xiNpJI00e+/5fdaaNTN775l59+xXo/3u93mft/DYbOhzEREZG41ZkTTpz2seq6zK1BEtFauqMVa1w8zmAEuBu8JFdwFLzawtvp27b3b3jcARrW533+/uHj5tARoBL95uotasWl7IDHjCrGbWrFpe6Y8QEakZUVbABo2xkhQopFtXKGDmZOqIlu6xUsOqhpwI7HT3HEB4/3K4vGxm9iEzexZ4Ebje3Z+OrT4nDBX8sZmdNszrV4ehhhu6uornwwy0t7bwjsXBRO5f+IPfpr21ZSxFFBGRmGjMSr1OVCUFFAqYXZn6BSo1xqqvP8dgB4TI6Nz9++7+RmAJcL6ZnRyuugU4yd3fTBBG+D0zay3x+tvcfZm7L2trayteXTDn6OkA/Gb/wUrvgohITckp3bqkiJKtZFemjui0WMNqWkMdjfVGLu+FtJaSeS8B88ysHiC8nxsuHzN37wAeBz4QPt/l7v3h4/vD9z11vIV9vRpWIiIVoeQVkiaHBzRBcFZl6ojGBwEe09xIczgJq8IBa4O7dwIbgXPDRecCT7p76Xi8EszslNjj2cAK4Onw+bzYurcC84Ffj7e8xx09DYBdaliJiEyIkldImgxoTGBmZaphFQ8FnNXSSEtTA6AEFjXmMuAKM3sBuCJ8jpmtM7Nl4eMzzGwHcCVwqZntMLP3hq9fbWbPmtlG4EHgJnf/cbjur8zsGTP7JXA7cL677xpvQY8Le6w69x8a71tIRpU5bUC9md1sZlvMbLOZXVzOOpEsKoRWqQdAUqCQvEL1NXMaql2ASop3qc5qbipcwepTyvWa4e7PA28vsfz9scc/B04Y5vWfHuG9V1WijJGoYaUeKykhmjbgTjM7j2DagHcVbfNxYBGwGGgFnjSzB9x9+yjrRDJHY6wkLdyd/vD8tFE9VpmTqaZyPN36MS2NNDcFoYDqsZIk0hgrKaXcaQOAjwG3u3s+DHf9LvDRMtaJZI7GWElaDMQuApipvmZNphpWTUVjrFrUsJIEm9XSSFNDHQcODqhXVeLKnTagnWBKgEhHbJuR1hWUMzWASBqox0rSQmGA2ZapoxrPCjireXCMlZJXSBKZWSGBxW80zkqqoNypAUSSTj1WkhZRGKASV2RTphpWRyavUI+VJFsUDrhrn8IBpaDcaQM6gDfEnrfHthlpnUjmDOSieYF0sirJFvVYKdV6NmXqqA5Jt97SFBtjpTArSSZNEizFxjBtwD3AJWZWF46/Ohv4dhnrRDJnsMcqU6c1kkED6rHKtEz9AsV7rOJjrF7rV4+VJJMSWMgwRp02APgGsBXYBDwKXOfu28pYJ5I50RgrZVmTpNMYq2zLVrr1YcZYKRRQkkqTBEspZU4bkAMuH+b1w64TySKNsZK0UChgtmXqqA6Zx6qlkeZGjbGSZNMkwSIiE5fTGCtJCSWvyLZMNaziCQA+ddeTHBoIGlR9hzTGSpJJkwSLiEycxlhJWigUMNsydVQv+fqGwuOOV/u494mdAPRpjJUklMZYiYhMXGHSVfUCSMJFDasGNawyKVNHdWtXb+Fx3qG7Jwiv0jxWklSHB4If2B17XmPljQ/T0d1X5RKJiKRPTmOsJCWiUMAmXQTIpEw1rBa0zSD6Ta2zwTArpVuXpPqTbz1ReLy5q4eL1q6vYmlERNIpSmHdqIaVJNzgnGuZOgWXUKaO6ppVy1nYNpN6Mxa2zeTTKxcDSl4hyRXvZXUf+lxERMqTywcnqxpjJUl3OBpj1aC6mkWZSrfe3trC/VeeWXj++LZXAYUCSnItaJvBps4eACx8LiIiY6MxVpIWAwoFzLRMN5ejCYLVYyVJtWbVco5pDq5vzJ7ZxJpVy6tcIhGR9NEYK0mLfoUCZlqmj2pz2LB6TVkBJaHaW1u49MyFAJz9O/Nob22pcolERNKnMDeQGlaScAoFzLayjqqZLTGzR8zshfB+cYlt6s3sZjPbYmabzeziEtucbGZ9ZnZDbFmLmd0dvuZ5M/vAxHZp0GCPlZJXSHLNm9UMwMt7lXJdRLKhzPOGr5vZxtgtb2YfGs/nDY6xUsNKkk2JVrKt3ObyLcDN7r4EuBm4tcQ2HwcWAYuB04BrzWx+tNLM6sPXfbfodZ8F9rv7IuCDwD+b2cwx7MOwWhqDEKu+Q+qxkuSaGzasdu59rcolERGpmFHPG9z9And/q7u/FVgF7AF+NJ4PGxxjpV4ASTZNEJxtox5VM5sDLAXuChfdBSw1s7aiTT8G3O7ueXfvImhAfTS2/nPAfwAvlHjdrQDuvgnYALxvjPtRUhQK2Nefw90r8ZYiFTfYY6WGlYik3xjOG+IuAr7p7ofG85nRGCuFAspYldm7eqGZPRX2rD5tZp8a7+f1K9FKppXTXD4R2OnuOYDw/uVweVw78GLseUe0jZm9BXgv8A8l3n/Y18WZ2Woz22BmG7q6usooNjQ11NFQZ+TyXohpFUmaOUdNo77O6DxwiEMD6l0VkdQr97wBADNrAv4X8LXxfuCAklfI+JUTlXUv8Jawd/V04DNm9ubxfFj/gHqssmzSj6qZNQK3AZdFP7Lj4e63ufsyd1/W1jbSRa+hCgkslBlQEqqhvo7Xh5NZ79qncVYiUnPOBjrcfeNwG4x2cVU9VjIe5fauuvt+Hwx9agEagXGFQg2E4wGblLwik8o5qi8B88IxUtFYqbnh8rgO4A2x5+3hNscDC4F1ZrYd+HPgEjO7bZTXVYRSrksazJ0VNKw0zkpEMqDc84bIJxilt2q0i6vqsZJxKrt31cw+ZGbPEkRZXe/uT5d6w9EuAiiDZbaN2rBy905gI3BuuOhc4MlwHFXcPQQNprqwpX828G1373D32e4+393nA/9IMBZrdex1lwKEca3LgR9OcL8KZjSFCSzUsJIEm6vMgCKSEWM4b8DMTgDeAXxzIp85oLmBZJK5+/fd/Y3AEuB8Mzt5mO1GvAhwWKGAmVbuUb0MuMLMXgCuCJ9jZuvMbFm4zTeArcAm4FHgOnffVsZ7Xw/MMrPNBMktVrv7gTHsw4gUCihpMFcJLEQkW8o5b4AgG+AP3H3PRD5sQAkBZHzG2ruKu3cAjwPjmh4oCgVsVF3NpIZyNnL354G3l1j+/tjjHHB5Ge91bdHzXoZmD6wozWUlaaCGlYhkSTnnDeHzL1fi8zTGSsbD3TvNLOpdvZNhelfN7BR3fy58PBtYAXxnPJ8ZhQKqxyqbMn9UmxUKKClwguayEhEZN42xkgkop3d1tZk9GzbCHgRucvcfj+fDonmsNOdaNpXVY5VmLY1KXiHJpx4rEZHxy+U1xkrGp8yorE9X6vOihlWTQgEzKdO/QB3dffxiy24ArvvBs3R091W5RCKlRVkBX957UJNZi4iM0UBOPVaSDgMKBcy0TB/Vi9au58DBYGxV54FDXLR2fZVLJFLant5+6gxe68/x+3//sC4C1CgzazGzu81ss5k9b2bDDo42s0vC7baY2U1mVhcu/7CZ/beZPROGrnxm6vZApDqUvELS4rBCATMt00d1a1dv4bEXPRdJkovWric8L2Bbd68uAtSuzwL73X0R8EHgn81sZvFGZnYS8EXgNGBxeDsvXL0L+KC7nwqcDlxuZu+YisKLVMuAkldISgwmr1BdzaJMN6wWtM3Aip6LJNGQiwCuiwA17GPArQDuvgnYALyvxHYfAb7r7l3ungduD1+Luz/m7i+Hj/cBzzF0EnaRzNEYK0mLaM41hQJmU6aP6ppVywtjV6Y31rFm1fIql2jsOrr7WHnjwyy8eh0rb1SIWFbFLwIYughQw9qBF2PPO4ATx7udmf0W8LvAT0p9mJmtNrMNZrahq+uIuVtFUkNjrCQt+tWwyrRMH9X21ha+efHvAtA6YxrtrS1VLtHY/fEdj7Ops4ecO1u6ehQillFrVi3n9ccEFwGaG+tTeRFARmdmT5jZ7mFu9RX+rOOB7wGfjHqwirn7be6+zN2XtbW1VfLjRaZUTmOsJCWiUEDV1WzKdMMK4Piwx2rX/oOF7tc02b57MCQsrxCxUZnZEjN7xMxeCO8Xl9jmPeFV+kNmdkPRugvN7Ckz22hmT5vZp2Lr6s3s5jBZwGYzu7hS5W5vbeHbl58OQFNjHSce21ypt5YEcfel7j57mFuOoOcpHrbXDrxU4q1G3M7M5gAPAH/n7vdUfk9EkiWneawkJQbTrWf+FLwmZf6oTmuoZ85R08jlnV37D1a7OGMW9WIA1JlCxMpwC3Czuy8BbiYcr1JkK3AxcH2JdfcCb3H3txIM/P+Mmb05XPdxYBFBooDTgGvNbH6lCj73mOkcO6OJvX397Nij+axq1D3ApQDhRYHlwA9LbHcvcLaZtYXZAC8B/i18XStwP8EElmumpNQiVdZfGGOlhpUkm9KtZ1tNHNV5rwuu/u9M4cnq6ncuKDxe2DZTIWIjCK/SLwXuChfdBSw1syExTu6+2d03AgPF7+Hu+31wIqkWoJEgqSQEyQFud/e8u3cB3wU+WsHyc+q8YwB4Zue+Sr2tpMv1wCwz2wz8B7Da3Q8AmNl1ZnYZgLtvBb4EPApsIrhYcGf4Hp8DlgCXhj2vG83swineD5EpldMYK0mJwXTrqqtZ1FDtAkyFebOaebJjLzv3pq9h1dw4OOziB1ecwfTGig7DyJoTgZ1hSBXunjOzl8PlZY/MN7MPAX8NLASudvenw1XlJgxYDawGaG9vH9MOvGne0fzshS6e3rmP973p+DG9VtLP3XsZprHu7tcUPb+VEj2y7n4VcNWkFFAkoaJ06+oFkKRTKGC21cRRTXOPVc+hXOHx3r7+Kpakdrj79939jQRX/c83s5PH+PpxJwR4U9hj9bR6rEREyqYxVpIWA0pekWk10bA6YVbYsEphj1XvocFotX2vqWE1ipeAeVF2tfB+LqUH/4/K3TuAx4EPhIvKTSwwbvFQwBe7e1lxw0NKtS8iMgpNECxpoXTr2VYTR7XQY5XyhtXevsNVLEnyuXsnsBE4N1x0LvBkOB6qLGZ2SuzxbGAFEIUC3gNcYmZ14bits4FvV6LskXmzmjl2RhN7+vr54Fd+zrbdvUq1LyIyCvVYSVpEiVYUCphNNXFU580K5q9KZyigeqzG6DLgT0m6FAAAGzVJREFUCjN7AbgifI6ZrTOzZeHjM8xsB3AlwQD/HWb23vD1q83sWTPbCDxIkFntx+G6bxAkCdhEkDTgOnffVsnCmxkL22YCsP/g4LFXqn0RkeFFvQANdTVxWiMp1j+gUMAsq4nkFXPDuax27n0Nd8csPZV5SI+VGlajcvfngbeXWP7+2OOfAycM8/pPj/DeOeDyChRzRFu6DhyxTKn2RUSGpx4rSYuBvEIBs6wmjupR0xs5enoDhwbydPemK5wunrxivxpWNWFf3xFZ4Jk3q1mp9kVESnB3jbGS1Dg8EDas1LuaSTVzVOe9Lp3hgEpeUXsWtM2g+NzgsrMW0t7aUp0CiYgkWNimos6gTg0rSbj+aILgBtXVLKqdhlVKMwP2Ho4nr1DDqhasWbWchW0zqTdj9swmAJ57ZX+VSyUikkxRaJXGV0kaqL5mW02MsQI4pjnY1T/51hMsapvJmlXLU9EDoOQVtae9tYX7rzwTgJ9v2s15ax7juVeOHHeVNB3dfVy0dj1bu3pZ0DYjNX9jIpJuGl8laeHugz1WSl6RSTXTXH74hSDjtjupSl2t5BW17ZTjjwKCHqt8FO+SUJ9Y+zibO3uUHl5EplR0oqrxVZJ00VjAxnpLVSI1KV/NNKxejSWtSFPq6t5Y8gr1WNWe1pnTOO7oafQdztHxarInCN7a1UvU9EvT35iIpFuhx0o9AJJwmhYg+2rmyEZzA0Vy7qy88WE6upN7suruQ8ZY7dMEwTXpt48/GoBfJXyc1bEzmgqPlR5eRKaKxqxIWigMMPtq5lcoSAgw9EQv6eFKfYdzeCz6Sz1WtemUsGGV9AQW733j6wuPF4bjGEVEJltOqdYlJaIeK81hlV01c2TbW1t48DNnDUljnfRwpWh81ayWRiBoWCV9nI1U3pyjpwHwlZ9sTnQvazQ3B8B3Pnm6EleIyJQYyCl5haSDGlbZV3NHNh4SaCQ7XCnKCDiruZEZTfXkHXoOHzl5rGTbv/xie+FxkntZu3oOFR6nbVqDUjq6+zjr+p9y0ufu4103PJTYBq1IrSv0WCm8ShIuugiguppdNdewWrNqOceFPQBNDXWJDleKElfMmNbArJZg/Mo+zWVVc3a8OthIyTts6exh5Y0Ps/DqdYnqweo6MNiwipc5rS5au57t3X04sHV3b2IbtCK1bkDp1iUlDoc9Vk3qscqsmjuy7a0tPHzVCo5pbuTQQJ79B5PbUIl6rGZMa+Do5sFwQKkt8V5VsyDzVRLTmu+O9Vjt2JOMxt5EbO7qGfI8yWHDIrVsMHmFGlaSbAOF5BU1d/pdM2ryyE5vrOfdp8wB4INf+XmirvrHRWOsZjTVM0sNq5q1ZtVyjpoeTHDdNnMaubwnLq15Pu/s7hnMWpnGUMCO7j5W3vgwCz53H4s/v25I4hhlORQZGzNbYmaPmNkL4f3iYbb7IzN72syeCe+PG+tnFcKrlBVQEq6Qbl2hgJlVs79C67fvAcBJ7riVKNX6jGkNHBM2rPYqFLDmtLe28KcrFgHwvlNfz0mzh57gJ+GEf0/f4cI4B4Ade9LXsDpvzaNs6uwhD/QXJYlRlkORMbsFuNndlwA3A7cWb2Bmy4BrgZXufipwBrBvrB+kMVaSFkpekX1lHdlyrjyZWb2Z3WxmW8xss5ldHFt3oZk9ZWYbwytSn4qtu9bMOsN1G83s5srs2sh27hk6biUJV/2LRaGAM2MNK/VY1aYlrz8KgF//5gCfec/JheVJGScYT1wByeux6uju4103PMSCq+87ooe6o7uPt335ATqGGRdWb8b9V56pLIciZTKzOcBS4K5w0V3AUjNrK9r008AN7r4LwN33ufvBsX6exlhJWmgeq+xrKHO76MrTnWZ2HsGVp3cVbfNxYBGwGGgFnjSzB9x9O3AvcIe7u5kdBTxjZg+5+1Pha7/u7p+d6M6MxYK2GWzu7CmEVCXhqn+x3tgYqyh2fO9rmiS4Fp18XNiw2nWAV/YNnnfk8k7bUdOqVayC3QeCermwbQZbunoT12N14R2Ps3V3cPFkU2cP77z+p8xvbcEdXnx1+DBghQCKjMuJwE53zwG4e87MXg6Xd8W2+21gm5n9DJgJfAf4sruPaV4RzWMlE2FmS4C1BOeu3cAF7r6paJu/AM4BckA/8Hl3/9FYP0s9Vtk36pEdw5WnjwG3u3ve3buA7wIfBXD3/bEfyhagEajqhExrVi1n4ZzBE6Y/X1ky/LuqemJZAZW8orYdf8x0jprewJ6+fh741W8Ky3N55+mdY46cqbiunqCxd8rxR9NUX8ervYfpS9DUAKV6pLd39w3bqGqsN+ps6kMAzazFzO4Oe/2fN7MPjLDtJeF2W8zsJjOrK1o/3cyeNbMNk19ykXGpB94MrATOBN4HnF9qQzNbbWYbzGxDV1fXkHVR8gr1WMk4jRq2CjwOLHf3NwOfAO42s+axftDgGCs1rLKqnB6rcq88tQMvxp53hNsAYGYfAv4aWAhc7e5Px7Y9x8zeA+wCvujujxQXwsxWA6sB2tvbyyj2yNpbW3jgyrP4wr8/zTcf6+BPvvkkf1a3kZw7C9tm8LVVb6t66E9vIRSwnhnTgkOldOu1ycw4+bij2PDiHh7Z2g3A0vZZPNGxlyc79vC2k46tavmiVOtzjprOvNc1s213Lzv3vMbisKdtMnR093HR2vVs6eyhvt7I5b3QEIr+dju6+zhvzaNlX8WJGlP3X3nmpJV7FJ8F9rv7ojDk+v+Z2SJ3H5Ki0MxOAr4I/A7BFdb/BM4Dvh7b7MvAo8BbpqTkIoNeAuaZWX14zlAPzA2Xx3UA33b3Q8AhM/se8DaG1mMA3P024DaAZcuWDfmTVvIKGa9Y58HKcNFdwE1m1hZ2EgBQ1Dv1FMFUqK3AjrF8XhQK2KRQwMyasl8hd/++u78RWAKcb2bRQJFbgJPCqwDXA98zs9YSr7/N3Ze5+7K2tuLOsvH7xebdhccDeccdtnQmY86aeCigxlhJNM4KoLmxnj9aFly32PjS3moVqSBqWLUdNY15s4KLeDsmeZzVRWvXszlKNpFz8g6bO4cmornwjseHHTtVSgKSVHyM8GppGIqygeAqfrGPAN919y53zwO3h68FwMzeQRCW/Y1JL7FIEXfvBDYC54aLzgWejJ+ohr4FvMcCjcDvA78c6+flNMZKxu+IzgMg6jwYzgXAFncfU6MKYEChgJlXzpEtXHmCIEkFw195ekPseXuJbXD3DoIu1Q+Ez3e5e3/4+P7wNaeObTfG76USJ11RpsBqiyevmNUcTBCsrIC16+RY78+bTziGZfNfBySjYRWlWm87ahonvC5sWE3yOKstXT1H9ETF/3ZzeWdLUQhgHbB4zkzqzZjf2sL81hbqzVg8ZyY/u2pFEpJUjNjzX852ZjYD+Efg8tE+bKTwKpEJugy4wsxeAK4In2Nm68JsgAD/CnQCvyJoiD0LrBnrB0XJK5QQQCabmZ0JfInBiwalthn2d/WwQgEzb9RQQHfvNLPoytOdDH/l6R7gEjP7DkH36NnAOwDM7BR3fy58PBtYQTBIFTOb5+47w8dvBeYDv574rpVnQdsMtnT1UJRdmaOnN05VEYY1OI+VeqyEQh0AeOE3B+jcf4g6g1f2HWTFDQ+x9sLqha9GPVazZzYVeqx2TnLDqqWpoXDxIS7vMP9z91F88ToBYX6Y2RMEjaJSxjx/zzCuJxgvsHO4uYMiI4VXiUyEuz8PvL3E8vfHHueBK8PbuOU0xkrGr9ywVczsNILz4A+7+7DnqeWEreoiQHaV22Qu58rTN4CtwCaCuP7r3H1buG51OIh6I/AgcJO7/zhc91fhxIC/JAhnOT9KvToV1qxazsK2mdQxOGAdYO9r/SVTM0+l3ljyiigRwK9e2Z/YCY1lcv3fBweTFO3t62fVvzxeuCCwfXd1w1fjoYDTm4KflVsf3jIpdbWju493/t1PC40qY/BvN/6vqvhiSQLC/HD3pe4+e5hbjjJ7/kfZ7gzgGjPbTtAj8CYzewqRjBrIa4yVjE+5Yatmthy4G/iIuz8x3s8rZAVUXc2sstKtl3nlKccwoSfu/ukR3ntVOWWYLO2tLUdcwT71iz+i59AAeR+cPLgaV7njoYB/dveTheXVLJNUz4uxBoozOAg2el7Nudh29ww2rNb+14uFMlWirhYnqYjvN8CiOYO9UAuvXkeuRKbmaC6qFLgHuBTYEPY2Lad0yMm9wM/M7C8JkldcQjBehXC8KgBmdhbBPEHLSryHSCZojJVM0GXAWjO7BthDMIYKM1sHXOPuG4CvAs3ArWaFenZ+USK2URUaVg2qq1lV7jxWNSWeJjrvwbw3K298eEi2sanQezhKXlHPi7sHT6qTOqGxTK4FbTPY3NWDexDWVl9nhYQrAHNfN70q5erP5Xm17zBmcGxLEy/vrezk2+fc/ggv7w3SuedzRzaa4u9fKrQ3ZXNRXQ/cYWabCeZLWe3uBwDM7DrgZXe/xd23mtmXCKIDAH5MEKIiUnP6C1kBdbIqY1dm50FFwh36lcEy83RkS1jYNhMr+n2OrrxPpd5Yj1X8xNBI1YmiVMiaVctZ1BYkXljYNpNvfOLtLGqbWQh/O3FWdcZXvdp7GHdondFEQ30dC9tmFtaNt652dPex8saHOelz9xUaVaUUN5pKhfYmIQSwXO7e6+4fdfdF7n6yu38vtu4ad78l9vxWd18Y3i6PsloVvd9D6q2SrNMYK0mLqMeqqUGn31mlHqsS1qxazkVr17OpczAzYDV6rnpi6dbXrFrOx257hFf2HWRaQ11qThSlckqFrd5/5Zns7jnEGX/7E/5razfzP3cf9XWG+5FzOk2WwcQV04Dg7+d/fvUXdPceZmZYd8cqSqU+WjaF4kZTqe9IRLKtMMZKmdYk4ZTBMvvUsCohOjlbeePDR5zcTdX4poFcnoP9ecygpameGa0tPPiZM1n6pfs52J9XfK4UzJ45jebGeg72B1fCovEGU1VXu2LjqyD4+/nOJ0/nzOsfAuC4Y4LlHd19/PG/PM627l6Ont6AYew/2M/Ctpl86cOn8hffe4atXb2FcL7iRlUdDDsRsIjUrlxeoYCSDocHwnTrCgXMLDWsRjBcz9VUjG/qPRxmBGxqIBoo2dLUwFlL5vDDZ3fxw2d2ceHvnTTp5ZB02P9a6bTjY6mrUZKIqHFTTsOlo7uP//3tIOHcxpf20tHdR3trC29oncEpxx/Nc6/s5782d7Pit+YE7707KM++WHk3dfZwzu2PDnkel4Q06SKSXFEKa4UCStIN5BUKmHU6siOIeq4Wz5k5JI3zVFwlL8xhNa1+yPLlJwWTwv7lD37F4s+vY8HV9/G2L9/P2778AAuvXqdU7DVqQduMI+ZtAsi5l10novC7nDubyxxTeNHa9YVQwJ6DA0Nec9qCYwH4xB3rWXnjw+OedDtNY6REZOqpx0rSQolWsk89VmUo7rnatruXRZ9fR34Sx7H0xsZXxX3rsY7C4/7wn0nngcOFZZs6e3jn9T+lscohUyP1fnR093HhHY+zfXdf2T0jMrKojpZKSb6lq4cLvvYYjfV1Q44HMOQ4bI6F33mZvV1bu3oHX8PQ1zz4XGdh+ZauHurMyJdIhT6SFKVJF5Eq6Y+SV2jciiRcFArYqPGAmaWGVRminqsVN/yUbWHa82gA4qbOHn7/xofI5byi4z/ic1jFbd9dXm9UdGJd3NCaN6uZQwN5ug4coqHeGMh54b6uLtimoS44AW4/drAhVF+0bfw+l3def/R0BvJeeN/4iX1xGWBw8lbNyVUZxUkb4vM55R22d/dhBI2cTZ09fGLt4/QeyvHKviDjXnH4HQS9Xe++8SG+tuptw9bltqOmsWt/8B7FGfp27Bmadj1qVBnwhvD9to/Qk5ayNOkiUiU59QJISkShgEpekV1qWI1BR/drJZdHjYh8icZMvPExlgZX76HBMVZxpebpKUdUxpdiJ7vRsug+avREjcb4SW++aNvi+5f3DabELp7AdaTlmpNrcixom3FE4pX4482d5X3nmzt7OeuGn5L3YPxCLu+F+8ZYA9o4MmSvuAwN4bxb937ydJa2ByGtxT2bxUksFAIoIqMpZAVUQgBJuP6BKCug6mpWqWE1BmNt1BQ3PjZ19rDi7x8iHzs5Ha4HqHVmEwCPbO0ekuK9OOQrlx/sWRrp6n9SqVdicpRKvDJeUX2PGt7RfbyhvHDOjCN6HYvLMBDW+2NbmgrbDJdCXkSkHB3dfXz9ke0A3Pnoi/zh0hMUWi6J1NHdx7pnXgHgHx54gdMXzlZdzSA1mceg1OSjjfXGWDp0c3nHCU4yneDktPg+79AVGzcVn5w4OhHd+jd/wKYvv5+tf/0HPHTVCh66agU/u2oFi+cMli+pjKB80US3leyVMLMlZvaImb0Q3i8usc17zGyDmR0ysxuK1v2FmT1rZk+Z2X+b2Xtj6+4wsx1mtjG8faFiBa+wqJ7Mm9U8ptdFdXqsETXbuo5s1EdlmDtremFZLu9c8vUNY3tzEZFhXLR2PXv7+oFgsvJyku6IVMNFa9dz4GAwzKNz/yHV1YxSj9UYlLq6HoUylUoaUCnlhssVl6+4bPHerXLGTU1k2yj0MQrtipdhkhNq3ALc7O53mtl5wK3Au4q22QpcDHwEmF607nHg7929z8zeAjxsZse7exRD+TfuftNkFHwy7IqFaEKQDCLe81pn0H5sy5DEFvFwvLq68CLACNV6tF7H3+w7NOS5Qj9FpFJGSqAjkiTxuqm6ml1qWE3QaI2ZeOOjPhxjMsbEaOMOlyvVEKyGqSqDmc0BlgIrw0V3ATeZWZu7d0XbufvmcPuzi9/D3X8Ue/oUQQdbK7Bjsso9mYobUdG4pdHmq4qO2XD1udTYwbGUQUSkEgoZTR1Mvy+SYKqrtUENqwobqTEz2klqqR6gl159TYP4y3cisNPdcwDunjOzl8PlXSO+srQLgC3uHm9UXWlmlwJbgKvd/bmJFnoyDdeIKrexW4nGeakyiIhUgn5fJC1UV2uDGlZTKCk9SDI6MzsT+BKDvV8AXwBecfe8mV0A/NDMFkQNudhrVwOrAdrb26eqyCUloc4loQwikk36fZG0UF2tDUpeIVnyEjDPzOoBwvu54fKymdlpwJ3A2e7+62i5u+9093z4+OvATOCE4te7+23uvszdl7W1tY17Z0REREQkPdSwksxw905gI3BuuOhc4Mn4+KrRmNly4G7gI+7+RNG6ebHH7wVywM6JlltERERE0k+hgJI1lwFrzewaYA/BOCnMbB1wjbtvMLMzgH8Fjg5W2TnARWHiiq8CzcCtZoWc4+e7+9Ph+x4H5IH9wIfcfWAK901EREREEkoNK8kUd38eeHuJ5e+PPf45JUL4wnXDjiZ193dXoowiIiIikj3mY839nQBm1gW8WGLVbGD3FBdnIlTeI73B3TMzMEl1tWpUV8coQ3W1HFncJxh+v2qhrqbtmKq8pamuJo/Ke6Rh62kqG1bDMbMN7r6s2uUol8pbu9L2Xaq8tSuL32UW9wmyu1/lSNu+q7y1K23fpco7NkpeISIiIiIiMkFqWImIiIiIiExQ1hpWt1W7AGOk8tautH2XKm/tyuJ3mcV9guzuVznStu8qb+1K23ep8o5BpsZYiYiIiIiIVEPWeqxERERERESmXCYaVma2xMweMbMXwvvF1S5TnJm1mtk6M/u1mT1tZt8xs7Zw3e+a2S/Dsv/YzOZUu7wRM/uimbmZnRo+T2xZ00J1dXKoro6fmbWY2d1mttnMnjezD4yw7SXhdlvM7CYzqwuXn2VmfWa2Mbw9NnV7MKR8o/59mVm9md0c7sNmM7u4nHXVVIH9utbMOmPH5+ap3YPJleTf1bT+poJ+VyeD6urkSFRddffU34CfAOeFj88DflLtMhWV71jgrNjz64E1BA3bzcAZ4fL/A3yt2uUNy7IU+E9gO3Bqksuappvq6qSUWXV1Yt/fNcDt4ePFwC5gZontTgJ2AG3hd/wj4IJw3VnAhgTsy6h/X8AFYdnrwn3ZAcwfbV3K9+ta4IZq70c1v58qli11v6lhefS7Ojnfq+pq5cudqLpa9S+kAl/oHGAvUB8+rw+ft1W7bCOU+Q+BB4DlwDOx5bOBngSUbxrwCDA/VlETWdY03VRXJ6V8qqsT/w6fBZbFnv8H8NES210F3BR7/hHgvvDxWVS5YVXu3xdwH/CR2PObgKtGW5fy/bqWjDas0va7mvTf1LAs+l2dnO9VdbXyZUxcXc1CKOCJwE53zwGE9y+HyxMnDJ25HPg+0E5sRm533w3UmdmxVSpe5DrgTnffHluW1LKmiepq5amuTtyQ7wvooHSdHG27JWb2hJk9ZmarKl/MUZX79zXSfpT7XUylSuwXwDlm9lQYFnPaZBZ4iqXmdzUlv6mg39XJorpaeYmrq1loWKXNV4AegquJiRP+w10GfLXaZZGqU13NgLCxs3uYW32FPuYJ4ER3XwqcA1xjZu+u0HvLxN0CnOTubyYI7/membVWuUy1KNG/qaDfVSlQXR2nLDSsXgLmRScI4f3ccHmimNkNBGMYPubueYIrim+IrZ8N5N391SoVEeBM4BRgm5ltB04giNtfRPLKmjaqq5WluloGd1/q7rOHueUoOrYEV/tK1clht3P3/e6+L3y8Dfgu8HuTsT8jKPfva6T9Lfe7mEoT3i933+Xu/eHj+8Plp05yuadKKn5XU/KbCvpdnUyqq5WVyLqa+oaVu3cCG4Fzw0XnAk+6e1f1SnUkM/sr4H8AZ7v7oXDxfwPNZnZG+Pwy4J5qlC/i7n/j7nPdfb67zycYAP1egquciSpr2qiuVpbqasXcA1wKEGaoWg78sMR29wJnm1lbGCZyCfBv4euONzMLHx8LvIegrk+ZMfx93QNcYmZ1Ycars4Fvl7GuKiqxX2Y2L9rIzN5KMB7h15Nc9CmRht/VtPymgn5XJ5PqamUltq5O1WCuybwBvwU8BrwQ3p9c7TIVle+NgBP8I9sY3v49XHc68DSwCbgfOK7a5S0q+3bg1DSUNQ031dVJLbvq6vi+txkE/3Q2h8f9w7F11wGXxZ5fCmwJb//E4CDsPyVIgrEReIYqJXwY7u8LWEeYoINgwPg/xfZjdez1w66r8jGa6H6tDY/LL4H1wPurvU9T8f0k4Zbm39SwjPpdrez3qbo6eeVPRF21sAAiIiIiIiIyTqkPBRQREREREak2NaxEREREREQmSA0rERERERGRCVLDSkREREREZILUsBIREREREZkgNaxEREREREQmSA0rERERERGRCVLDSkREREREZIL+P8jqqNTSHEiLAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 864x432 with 10 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"HXFQcoG0kWE7","colab_type":"code","colab":{}},"source":["import shutil\n","new_weight = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + name_input\n","if os.path.exists(new_weight):\n","  shutil.rmtree(new_weight)\n","os.mkdir(new_weight)\n","\n","weight_last = '/content/yolov5/weights/last_' + name_input + '.pt'\n","weight_best = '/content/yolov5/weights/best_' + name_input + '.pt'\n","\n","!cp '{weight_last}' '{new_weight}'\n","!cp '{weight_best}' '{new_weight}'\n","!cp 'results.png' '{new_weight}'\n","!cp 'labels.png' '{new_weight}'\n","!cp 'test_batch0_gt.jpg' '{new_weight}'\n","!cp 'test_batch0_pred.jpg' '{new_weight}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtigXSkIl15i","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}