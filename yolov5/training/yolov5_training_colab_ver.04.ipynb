{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolov5_training_colab_ver.04.ipynb의 사본","provenance":[{"file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","timestamp":1594362418856}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","authorship_tag":"ABX9TyNrdodx9R9srVEpp9HbhUcx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AiQPodddRTaD","colab_type":"code","colab":{}},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yf22UTuLusak","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1594336759778,"user_tz":-540,"elapsed":2274,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"d32fdf90-5869-47fd-ece8-68dc91adcec1"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul  9 23:19:17 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OoBHduIadAI7","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlvKV6g3Kcdn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"executionInfo":{"status":"ok","timestamp":1594336771839,"user_tz":-540,"elapsed":7223,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"c1b5eaff-6ef3-4fbf-d230-0fe942ab212e"},"source":["!pip install -U PyYAML"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting PyYAML\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\r\u001b[K     |█▏                              | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 2.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=5658928fbe4ec5754c68fe6d4ce68cb2125e7b676c1ac779b0230810df66b6cb\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built PyYAML\n","Installing collected packages: PyYAML\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SDekTiLTg1WA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594336771840,"user_tz":-540,"elapsed":6427,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"720ba89b-0db1-484b-ad66-5b1a7854770f"},"source":["%%writefile setup.sh\n","\n","export CUDA_HOME=/usr/local/cuda-10.1\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wPuaO9L9g2GF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594337203788,"user_tz":-540,"elapsed":437671,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"f6847b2a-60b0-4497-f3ba-ce990abe1968"},"source":["!sh setup.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'apex'...\n","remote: Enumerating objects: 80, done.\u001b[K\n","remote: Counting objects: 100% (80/80), done.\u001b[K\n","remote: Compressing objects: 100% (61/61), done.\u001b[K\n","remote: Total 7335 (delta 40), reused 41 (delta 19), pack-reused 7255\u001b[K\n","Receiving objects: 100% (7335/7335), 13.88 MiB | 7.14 MiB/s, done.\n","Resolving deltas: 100% (4939/4939), done.\n","/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-tztm6fo5\n","Created temporary directory: /tmp/pip-req-tracker-_c1xbv1x\n","Created requirements tracker '/tmp/pip-req-tracker-_c1xbv1x'\n","Created temporary directory: /tmp/pip-install-1j34k3fi\n","Processing ./apex\n","  Created temporary directory: /tmp/pip-req-build-0gdmlw2w\n","  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-_c1xbv1x'\n","    Running setup.py (path:/tmp/pip-req-build-0gdmlw2w/setup.py) egg_info for package from file:///content/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-0gdmlw2w/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-0gdmlw2w/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-0gdmlw2w/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-0gdmlw2w/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-0gdmlw2w/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-0gdmlw2w/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-0gdmlw2w/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-0gdmlw2w has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n","  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-_c1xbv1x'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-_gl3xob0\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-0gdmlw2w/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-0gdmlw2w/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-_gl3xob0/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    /tmp/pip-req-build-0gdmlw2w/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda-10.1/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    creating /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-_gl3xob0/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-0gdmlw2w\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-_c1xbv1x'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"665tCOfRpIIx","colab_type":"code","colab":{}},"source":["import torch.random\n","import random\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqnYi7N2Kfjy","colab_type":"code","colab":{}},"source":["zip_name = 'gwd-split-fold0.zip'\n","zip_path = '/content/drive/My Drive/Colab Notebooks/gwdsplit/' + zip_name\n","!cp \"{zip_path}\" .\n","!unzip -q '{zip_name}'\n","!rm '{zip_name}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"978lsjD2vQAF","colab_type":"code","colab":{}},"source":["yolov5_name = 'yolov5.zip'\n","yolov5_path = '/content/drive/My Drive/Colab Notebooks/' + yolov5_name\n","!cp '{yolov5_path}' .\n","!unzip -q '{yolov5_name}'\n","!rm '{yolov5_name}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROkeoZzd_ljI","colab_type":"code","colab":{}},"source":["weight_name = 'yolov5x_coco.pt'\n","weight_path = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + weight_name\n","!cp '{weight_path}' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1l1XG4zejqR","colab_type":"code","colab":{}},"source":["train_input = '/content/drive/My Drive/Colab Notebooks/yolov5/train.py'\n","data_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/wheat_colab.yaml'\n","cfg_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/yolov5x.yaml'\n","weights_input = '/content/' + weight_name\n","name_input = 'x-b2-e25-coco'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-Sb84dpxl6U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594337237585,"user_tz":-540,"elapsed":468320,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"38dd7804-2332-4d42-e981-2eeb50f36f4d"},"source":["%cd /content/yolov5/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/yolov5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lYOIA2YaeNgC","colab_type":"text"},"source":["# **train.py**"]},{"cell_type":"code","metadata":{"id":"9UCW99OCx4QA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1594337240136,"user_tz":-540,"elapsed":468224,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"55670a21-8765-47c9-e7bc-e6e517438864"},"source":["import argparse\n","\n","import torch.distributed as dist\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.utils.data\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import test  # import test.py to get mAP after each epoch\n","from models.yolo import Model\n","from utils import google_utils\n","from utils.datasets import *\n","from utils.utils import *\n","\n","mixed_precision = True\n","try:  # Mixed precision training https://github.com/NVIDIA/apex\n","    from apex import amp\n","except:\n","    print('Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex')\n","    mixed_precision = False  # not installed\n","\n","wdir = 'weights' + os.sep  # weights dir\n","os.makedirs(wdir, exist_ok=True)\n","last = wdir + 'last.pt'\n","best = wdir + 'best.pt'\n","results_file = 'results.txt'\n","\n","# Hyperparameters\n","hyp = {'lr0': 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","       'momentum': 0.937,  # SGD momentum\n","       'weight_decay': 5e-4,  # optimizer weight decay\n","       'giou': 0.05,  # giou loss gain\n","       'cls': 0.58,  # cls loss gain\n","       'cls_pw': 1.0,  # cls BCELoss positive_weight\n","       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)\n","       'obj_pw': 1.0,  # obj BCELoss positive_weight\n","       'iou_t': 0.20,  # iou training threshold\n","       'anchor_t': 4.0,  # anchor-multiple threshold\n","       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n","       'hsv_h': 0.014,  # image HSV-Hue augmentation (fraction)\n","       'hsv_s': 0.68,  # image HSV-Saturation augmentation (fraction)\n","       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n","       'degrees': 0.0,  # image rotation (+/- deg)\n","       'translate': 0.0,  # image translation (+/- fraction)\n","       'scale': 0.5,  # image scale (+/- gain)\n","       'shear': 0.0}  # image shear (+/- deg)\n","print(hyp)\n","\n","# Overwrite hyp with hyp*.txt (optional)\n","f = glob.glob('hyp*.txt')\n","if f:\n","    print('Using %s' % f[0])\n","    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n","        hyp[k] = v\n","\n","# Print focal loss if gamma > 0\n","if hyp['fl_gamma']:\n","    print('Using FocalLoss(gamma=%g)' % hyp['fl_gamma'])\n","\n","\n","def train(hyp):\n","    epochs = opt.epochs  # 300\n","    batch_size = opt.batch_size  # 64\n","    weights = opt.weights  # initial training weights\n","\n","    # Configure\n","    init_seeds(1)\n","    with open(opt.data) as f:\n","        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n","    train_path = data_dict['train']\n","    test_path = data_dict['val']\n","    nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes\n","\n","    # Remove previous results\n","    for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):\n","        os.remove(f)\n","\n","    # Create model\n","    model = Model(opt.cfg, nc=data_dict['nc']).to(device)\n","\n","    # Image sizes\n","    gs = int(max(model.stride))  # grid size (max stride)\n","    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n","\n","    # Optimizer\n","    nbs = 64  # nominal batch size\n","    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n","    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n","    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n","    for k, v in model.named_parameters():\n","        if v.requires_grad:\n","            if '.bias' in k:\n","                pg2.append(v)  # biases\n","            elif '.weight' in k and '.bn' not in k:\n","                pg1.append(v)  # apply weight decay\n","            else:\n","                pg0.append(v)  # all else\n","\n","    optimizer = optim.Adam(pg0, lr=hyp['lr0']) if opt.adam else \\\n","        optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n","    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n","    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n","    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n","    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n","    print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n","    del pg0, pg1, pg2\n","\n","    # Load Model\n","    google_utils.attempt_download(weights)\n","    start_epoch, best_fitness = 0, 0.0\n","    if weights.endswith('.pt'):  # pytorch format\n","        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","\n","        # load model\n","        try:\n","            ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()\n","                             if model.state_dict()[k].shape == v.shape}  # to FP32, filter\n","            model.load_state_dict(ckpt['model'], strict=False)\n","        except KeyError as e:\n","            s = \"%s is not compatible with %s. This may be due to model differences or %s may be out of date. \" \\\n","                \"Please delete or update %s and try again, or use --weights '' to train from scratch.\" \\\n","                % (opt.weights, opt.cfg, opt.weights, opt.weights)\n","            raise KeyError(s) from e\n","\n","        # load optimizer\n","        if ckpt['optimizer'] is not None:\n","            optimizer.load_state_dict(ckpt['optimizer'])\n","            best_fitness = ckpt['best_fitness']\n","\n","        # load results\n","        if ckpt.get('training_results') is not None:\n","            with open(results_file, 'w') as file:\n","                file.write(ckpt['training_results'])  # write results.txt\n","\n","        # epochs\n","        start_epoch = ckpt['epoch'] + 1\n","        if epochs < start_epoch:\n","            print('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\n","                  (opt.weights, ckpt['epoch'], epochs))\n","            epochs += ckpt['epoch']  # finetune additional epochs\n","\n","        del ckpt\n","\n","    # Mixed precision training https://github.com/NVIDIA/apex\n","    if mixed_precision:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n","\n","\n","    scheduler.last_epoch = start_epoch - 1  # do not move\n","    # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822\n","    # plot_lr_scheduler(optimizer, scheduler, epochs)\n","\n","    # Initialize distributed training\n","    if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n","        dist.init_process_group(backend='nccl',  # distributed backend\n","                                init_method='tcp://127.0.0.1:9999',  # init method\n","                                world_size=1,  # number of nodes\n","                                rank=0)  # node rank\n","        model = torch.nn.parallel.DistributedDataParallel(model)\n","        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","    # Trainloader\n","    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\n","                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect)\n","    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n","    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Correct your labels or your model.' % (mlc, nc, opt.cfg)\n","\n","    # Testloader\n","    testloader = create_dataloader(test_path, imgsz_test, batch_size, gs, opt,\n","                                   hyp=hyp, augment=False, cache=opt.cache_images, rect=True)[0]\n","\n","    # Model parameters\n","    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\n","    model.nc = nc  # attach number of classes to model\n","    model.hyp = hyp  # attach hyperparameters to model\n","    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n","    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n","    model.names = data_dict['names']\n","\n","    # Class frequency\n","    labels = np.concatenate(dataset.labels, 0)\n","    c = torch.tensor(labels[:, 0])  # classes\n","    # cf = torch.bincount(c.long(), minlength=nc) + 1.\n","    # model._initialize_biases(cf.to(device))\n","    if tb_writer:\n","        plot_labels(labels)\n","        tb_writer.add_histogram('classes', c, 0)\n","\n","    # Check anchors\n","    if not opt.noautoanchor:\n","        check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n","\n","    # Exponential moving average\n","    ema = torch_utils.ModelEMA(model)\n","\n","    # Start training\n","    t0 = time.time()\n","    nb = len(dataloader)  # number of batches\n","    n_burn = max(3 * nb, 1e3)  # burn-in iterations, max(3 epochs, 1k iterations)\n","    maps = np.zeros(nc)  # mAP per class\n","    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n","    print('Image sizes %g train, %g test' % (imgsz, imgsz_test))\n","    print('Using %g dataloader workers' % dataloader.num_workers)\n","    print('Starting training for %g epochs...' % epochs)\n","    # torch.autograd.set_detect_anomaly(True)\n","    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n","        model.train()\n","\n","        # Update image weights (optional)\n","        if dataset.image_weights:\n","            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n","            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n","            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n","\n","        # Update mosaic border\n","        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n","        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n","\n","        mloss = torch.zeros(4, device=device)  # mean losses\n","        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n","        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n","        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n","            ni = i + nb * epoch  # number integrated batches (since train start)\n","            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n","\n","            # Burn-in\n","            if ni <= n_burn:\n","                xi = [0, n_burn]  # x interp\n","                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n","                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n","                for j, x in enumerate(optimizer.param_groups):\n","                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n","                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n","                    if 'momentum' in x:\n","                        x['momentum'] = np.interp(ni, xi, [0.9, hyp['momentum']])\n","\n","            # Multi-scale\n","            if opt.multi_scale:\n","                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n","                sf = sz / max(imgs.shape[2:])  # scale factor\n","                if sf != 1:\n","                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n","                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n","\n","            # Forward\n","            pred = model(imgs)\n","\n","            # Loss\n","            loss, loss_items = compute_loss(pred, targets.to(device), model)\n","            if not torch.isfinite(loss):\n","                print('WARNING: non-finite loss, ending training ', loss_items)\n","                return results\n","\n","            # Backward\n","            if mixed_precision:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            # Optimize\n","            if ni % accumulate == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                ema.update(model)\n","\n","            # Print\n","            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n","            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n","            s = ('%10s' * 2 + '%10.4g' * 6) % (\n","                '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n","            pbar.set_description(s)\n","\n","            # Plot\n","            if ni < 3:\n","                f = 'train_batch%g.jpg' % ni  # filename\n","                result = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n","                if tb_writer and result is not None:\n","                    tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n","                    # tb_writer.add_graph(model, imgs)  # add model to tensorboard\n","\n","            # end batch ------------------------------------------------------------------------------------------------\n","\n","        # Scheduler\n","        scheduler.step()\n","\n","        # mAP\n","        ema.update_attr(model)\n","        final_epoch = epoch + 1 == epochs\n","        if not opt.notest or final_epoch:  # Calculate mAP\n","            results, maps, times = test.test(opt.data,\n","                                             batch_size=batch_size,\n","                                             imgsz=imgsz_test,\n","                                             save_json=final_epoch and opt.data.endswith(os.sep + 'coco.yaml'),\n","                                             model=ema.ema,\n","                                             single_cls=opt.single_cls,\n","                                             dataloader=testloader)\n","\n","        # Write\n","        with open(results_file, 'a') as f:\n","            f.write(s + '%10.4g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n","        if len(opt.name) and opt.bucket:\n","            os.system('gsutil cp results.txt gs://%s/results/results%s.txt' % (opt.bucket, opt.name))\n","\n","        # Tensorboard\n","        if tb_writer:\n","            tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n","                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/F1',\n","                    'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n","            for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n","                tb_writer.add_scalar(tag, x, epoch)\n","\n","        # Update best mAP\n","        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n","        if fi > best_fitness:\n","            best_fitness = fi\n","\n","        # Save model\n","        save = (not opt.nosave) or (final_epoch and not opt.evolve)\n","        if save:\n","            with open(results_file, 'r') as f:  # create checkpoint\n","                ckpt = {'epoch': epoch,\n","                        'best_fitness': best_fitness,\n","                        'training_results': f.read(),\n","                        'model': ema.ema,\n","                        'optimizer': None if final_epoch else optimizer.state_dict()}\n","\n","            # Save last, best and delete\n","            torch.save(ckpt, last)\n","            if (best_fitness == fi) and not final_epoch:\n","                torch.save(ckpt, best)\n","            del ckpt\n","\n","        # end epoch ----------------------------------------------------------------------------------------------------\n","    # end training\n","\n","    # Strip optimizers\n","    n = ('_' if len(opt.name) and not opt.name.isnumeric() else '') + opt.name\n","    fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n","    for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n","        if os.path.exists(f1):\n","            os.rename(f1, f2)  # rename\n","            ispt = f2.endswith('.pt')  # is *.pt\n","            strip_optimizer(f2) if ispt else None  # strip optimizer\n","            os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n","\n","    # Finish\n","    if not opt.evolve:\n","        plot_results()  # save as results.png\n","    print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n","    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\n","    torch.cuda.empty_cache()\n","    return results"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9csFml3nfP7H","colab_type":"text"},"source":["# **Train Option**"]},{"cell_type":"code","metadata":{"id":"h0uu9IpAyJC1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594360860557,"user_tz":-540,"elapsed":24087190,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"862def9b-db7c-4b8d-f65e-173901b7e916"},"source":["check_git_status()\n","class opt:\n","    epochs=25                #parser.add_argument('--epochs', type=int, default=300)\n","    batch_size=2            #parser.add_argument('--batch-size', type=int, default=16)\n","    cfg=cfg_input           #parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='*.cfg path')\n","    data=data_input         #parser.add_argument('--data', type=str, default='data/coco128.yaml', help='*.data path')\n","    img_size=[1024, 1024]   #parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')\n","    rect=False              #parser.add_argument('--rect', action='store_true', help='rectangular training')\n","    resume=False            #parser.add_argument('--resume', action='store_true', help='resume training from last.pt')\n","    nosave=False            #parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n","    notest=True            #parser.add_argument('--notest', action='store_true', help='only test final epoch')\n","    noautoanchor=False      #parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n","    evolve=False            #parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\n","    bucket=''               #parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n","    cache_images=False      #parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n","    weights=weights_input   #parser.add_argument('--weights', type=str, default='', help='initial weights path')\n","    name=name_input         #parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')\n","    device=''               #parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    adam=False              #parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n","    multi_scale=False       #parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%')\n","    single_cls=False        #parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\n","\n","#parser = argparse.ArgumentParser()\n","#opt = parser.parse_args()\n","\n","opt.weights = last if opt.resume and not opt.weights else opt.weights\n","opt.cfg = check_file(opt.cfg)  # check file\n","opt.data = check_file(opt.data)  # check file\n","print(opt)\n","opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\n","device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n","if device.type == 'cpu':\n","    mixed_precision = False\n","\n","# Train\n","if not opt.evolve:\n","    tb_writer = SummaryWriter(comment=opt.name)\n","    print('Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/')\n","    train(hyp)\n","\n","# Evolve hyperparameters (optional)\n","else:\n","    tb_writer = None\n","    opt.notest, opt.nosave = True, True  # only test/save final epoch\n","    if opt.bucket:\n","        os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n","\n","    for _ in range(10):  # generations to evolve\n","        if os.path.exists('evolve.txt'):  # if evolve.txt exists: select best hyps and mutate\n","            # Select parent(s)\n","            parent = 'single'  # parent selection method: 'single' or 'weighted'\n","            x = np.loadtxt('evolve.txt', ndmin=2)\n","            n = min(5, len(x))  # number of previous results to consider\n","            x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n","            w = fitness(x) - fitness(x).min()  # weights\n","            if parent == 'single' or len(x) == 1:\n","                # x = x[random.randint(0, n - 1)]  # random selection\n","                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n","            elif parent == 'weighted':\n","                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n","\n","            # Mutate\n","            mp, s = 0.9, 0.2  # mutation probability, sigma\n","            npr = np.random\n","            npr.seed(int(time.time()))\n","            g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n","            ng = len(g)\n","            v = np.ones(ng)\n","            while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n","                v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n","            for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n","                hyp[k] = x[i + 7] * v[i]  # mutate\n","\n","        # Clip to limits\n","        keys = ['lr0', 'iou_t', 'momentum', 'weight_decay', 'hsv_s', 'hsv_v', 'translate', 'scale', 'fl_gamma']\n","        limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n","        for k, v in zip(keys, limits):\n","            hyp[k] = np.clip(hyp[k], v[0], v[1])\n","\n","        # Train mutation\n","        results = train(hyp.copy())\n","\n","        # Write mutation results\n","        print_mutation(hyp, results, opt.bucket)\n","\n","        # Plot results\n","        # plot_evolution_results(hyp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class '__main__.opt'>\n","Using CUDA Apex device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB)\n","\n","Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n","  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n","  2                -1  1    315680  models.common.BottleneckCSP             [160, 160, 4]                 \n","  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n","  4                -1  1   3311680  models.common.BottleneckCSP             [320, 320, 12]                \n","  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n","  6                -1  1  13228160  models.common.BottleneckCSP             [640, 640, 12]                \n","  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n","  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n","  9                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1   5435520  models.common.BottleneckCSP             [1280, 640, 4, False]         \n"," 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1   1360960  models.common.BottleneckCSP             [640, 320, 4, False]          \n"," 18                -1  1      5778  torch.nn.modules.conv.Conv2d            [320, 18, 1, 1]               \n"," 19                -2  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n"," 20          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 21                -1  1   5025920  models.common.BottleneckCSP             [640, 640, 4, False]          \n"," 22                -1  1     11538  torch.nn.modules.conv.Conv2d            [640, 18, 1, 1]               \n"," 23                -2  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n"," 24          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 25                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 26                -1  1     23058  torch.nn.modules.conv.Conv2d            [1280, 18, 1, 1]              \n"," 27      [-1, 22, 18]  1         0  models.yolo.Detect                      [1, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n","Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n","\n","Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n"],"name":"stdout"},{"output_type":"stream","text":["Reading image shapes: 100%|██████████| 3373/3373 [00:00<00:00, 12864.15it/s]\n","Caching labels /content/labels/train (3373 found, 0 missing, 0 empty, 0 duplicate, for 3373 images): 100%|██████████| 3373/3373 [00:00<00:00, 4576.56it/s]\n","Caching labels /content/labels/train.npy (891 found, 0 missing, 0 empty, 0 duplicate, for 3373 images):  26%|██▋       | 891/3373 [00:00<00:00, 8905.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saving labels to /content/labels/train.npy for faster future loading\n"],"name":"stdout"},{"output_type":"stream","text":["Caching labels /content/labels/train.npy (3373 found, 0 missing, 0 empty, 0 duplicate, for 3373 images): 100%|██████████| 3373/3373 [00:00<00:00, 9424.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Analyzing anchors... Best Possible Recall (BPR) = 0.9992\n","Image sizes 1024 train, 1024 test\n","Using 2 dataloader workers\n","Starting training for 25 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      0/24     4.95G   0.06972    0.1818         0    0.2516       132      1024: 100%|██████████| 1687/1687 [16:00<00:00,  1.76it/s]\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      1/24     5.34G   0.05272    0.1534         0    0.2061        38      1024: 100%|██████████| 1687/1687 [15:27<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      2/24     5.34G   0.05074    0.1526         0    0.2033        21      1024: 100%|██████████| 1687/1687 [15:23<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      3/24     5.34G   0.04453    0.1484         0    0.1929        45      1024: 100%|██████████| 1687/1687 [15:22<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      4/24     5.34G   0.04076    0.1434         0    0.1841        64      1024: 100%|██████████| 1687/1687 [15:21<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      5/24     5.34G   0.03888    0.1426         0    0.1814        76      1024: 100%|██████████| 1687/1687 [15:23<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      6/24     5.34G   0.03853    0.1412         0    0.1798        81      1024: 100%|██████████| 1687/1687 [15:22<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      7/24     5.34G   0.03716    0.1415         0    0.1787        83      1024: 100%|██████████| 1687/1687 [15:22<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      8/24     5.34G   0.03666      0.14         0    0.1767        37      1024: 100%|██████████| 1687/1687 [15:23<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      9/24     5.34G   0.03622    0.1389         0    0.1751        31      1024: 100%|██████████| 1687/1687 [15:22<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     10/24     5.34G   0.03551     0.137         0    0.1725        45      1024: 100%|██████████| 1687/1687 [15:24<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     11/24     5.34G   0.03497    0.1353         0    0.1702        40      1024: 100%|██████████| 1687/1687 [15:26<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     12/24     5.34G     0.035    0.1373         0    0.1723       125      1024: 100%|██████████| 1687/1687 [15:25<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     13/24     5.34G    0.0345    0.1358         0    0.1702        84      1024: 100%|██████████| 1687/1687 [15:26<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     14/24     5.34G   0.03415    0.1334         0    0.1676        84      1024: 100%|██████████| 1687/1687 [15:25<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     15/24     5.34G   0.03403    0.1336         0    0.1676        54      1024: 100%|██████████| 1687/1687 [15:25<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     16/24     5.34G   0.03388     0.133         0    0.1669        85      1024: 100%|██████████| 1687/1687 [15:24<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     17/24     5.34G   0.03357    0.1344         0    0.1679        46      1024: 100%|██████████| 1687/1687 [15:24<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     18/24     5.34G   0.03342     0.132         0    0.1654        37      1024: 100%|██████████| 1687/1687 [15:23<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     19/24     5.34G   0.03335    0.1317         0    0.1651        41      1024: 100%|██████████| 1687/1687 [15:23<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     20/24     5.34G   0.03333    0.1317         0     0.165        30      1024: 100%|██████████| 1687/1687 [15:24<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     21/24     5.34G   0.03319    0.1316         0    0.1648        48      1024: 100%|██████████| 1687/1687 [15:22<00:00,  1.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     22/24     5.34G   0.03305    0.1325         0    0.1655        28      1024: 100%|██████████| 1687/1687 [15:26<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     23/24     5.34G   0.03313    0.1322         0    0.1653       120      1024: 100%|██████████| 1687/1687 [15:28<00:00,  1.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     24/24     5.34G   0.03299    0.1292         0    0.1622        95      1024: 100%|██████████| 1687/1687 [15:26<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 1687/1687 [04:49<00:00,  5.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all    3.37e+03    1.48e+05      0.0266      0.0324     0.00208    0.000354\n","Optimizer stripped from weights/last_x-b2-e25-coco.pt\n","Optimizer stripped from weights/best_x-b2-e25-coco.pt\n","25 epochs completed in 6.551 hours.\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 10 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"HXFQcoG0kWE7","colab_type":"code","colab":{}},"source":["import shutil\n","new_weight = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + name_input\n","if os.path.exists(new_weight):\n","  shutil.rmtree(new_weight)\n","os.mkdir(new_weight)\n","\n","weight_last = '/content/yolov5/weights/last_' + name_input + '.pt'\n","weight_best = '/content/yolov5/weights/best_' + name_input + '.pt'\n","\n","!cp '{weight_last}' '{new_weight}'\n","!cp '{weight_best}' '{new_weight}'\n","!cp 'results.png' '{new_weight}'\n","!cp 'labels.png' '{new_weight}'\n","!cp 'test_batch0_gt.jpg' '{new_weight}'\n","!cp 'test_batch0_pred.jpg' '{new_weight}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtigXSkIl15i","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}