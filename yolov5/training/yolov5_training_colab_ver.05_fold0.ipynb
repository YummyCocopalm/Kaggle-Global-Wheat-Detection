{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yolov5_training_colab_ver.05_fold0.ipynb의 사본","provenance":[{"file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","timestamp":1594457871481}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1cszNqQoiyO24fzqqlqCnZbQDLYdjznfN","authorship_tag":"ABX9TyN9hDPTWKotn+6OdO0FNEGK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AiQPodddRTaD","colab_type":"code","colab":{}},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yf22UTuLusak","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1594406477460,"user_tz":-540,"elapsed":3726,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"7051a5ed-1745-4db8-d80e-f77b8140f78a"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Fri Jul 10 18:41:14 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OoBHduIadAI7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594406484196,"user_tz":-540,"elapsed":1116,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["import numpy as np\n","import pandas as pd\n","import os"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlvKV6g3Kcdn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"executionInfo":{"status":"ok","timestamp":1594406493171,"user_tz":-540,"elapsed":8339,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"820c3335-b597-49d7-ff5f-f0ad6f1fa5b5"},"source":["!pip install -U PyYAML"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting PyYAML\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\r\u001b[K     |█▏                              | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 2.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=50280378af4abf03fe4109fd7683109c76c546f563eb979378daba6a947a23c3\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built PyYAML\n","Installing collected packages: PyYAML\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SDekTiLTg1WA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594406493175,"user_tz":-540,"elapsed":7183,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"2fa68738-1267-4e5f-c6a1-552d38be2e08"},"source":["%%writefile setup.sh\n","\n","export CUDA_HOME=/usr/local/cuda-10.1\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Writing setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wPuaO9L9g2GF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594407001916,"user_tz":-540,"elapsed":515274,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"523ed42b-b265-4273-a499-9019f912a74d"},"source":["!sh setup.sh"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Cloning into 'apex'...\n","remote: Enumerating objects: 80, done.\u001b[K\n","remote: Counting objects:   1% (1/80)\u001b[K\rremote: Counting objects:   2% (2/80)\u001b[K\rremote: Counting objects:   3% (3/80)\u001b[K\rremote: Counting objects:   5% (4/80)\u001b[K\rremote: Counting objects:   6% (5/80)\u001b[K\rremote: Counting objects:   7% (6/80)\u001b[K\rremote: Counting objects:   8% (7/80)\u001b[K\rremote: Counting objects:  10% (8/80)\u001b[K\rremote: Counting objects:  11% (9/80)\u001b[K\rremote: Counting objects:  12% (10/80)\u001b[K\rremote: Counting objects:  13% (11/80)\u001b[K\rremote: Counting objects:  15% (12/80)\u001b[K\rremote: Counting objects:  16% (13/80)\u001b[K\rremote: Counting objects:  17% (14/80)\u001b[K\rremote: Counting objects:  18% (15/80)\u001b[K\rremote: Counting objects:  20% (16/80)\u001b[K\rremote: Counting objects:  21% (17/80)\u001b[K\rremote: Counting objects:  22% (18/80)\u001b[K\rremote: Counting objects:  23% (19/80)\u001b[K\rremote: Counting objects:  25% (20/80)\u001b[K\rremote: Counting objects:  26% (21/80)\u001b[K\rremote: Counting objects:  27% (22/80)\u001b[K\rremote: Counting objects:  28% (23/80)\u001b[K\rremote: Counting objects:  30% (24/80)\u001b[K\rremote: Counting objects:  31% (25/80)\u001b[K\rremote: Counting objects:  32% (26/80)\u001b[K\rremote: Counting objects:  33% (27/80)\u001b[K\rremote: Counting objects:  35% (28/80)\u001b[K\rremote: Counting objects:  36% (29/80)\u001b[K\rremote: Counting objects:  37% (30/80)\u001b[K\rremote: Counting objects:  38% (31/80)\u001b[K\rremote: Counting objects:  40% (32/80)\u001b[K\rremote: Counting objects:  41% (33/80)\u001b[K\rremote: Counting objects:  42% (34/80)\u001b[K\rremote: Counting objects:  43% (35/80)\u001b[K\rremote: Counting objects:  45% (36/80)\u001b[K\rremote: Counting objects:  46% (37/80)\u001b[K\rremote: Counting objects:  47% (38/80)\u001b[K\rremote: Counting objects:  48% (39/80)\u001b[K\rremote: Counting objects:  50% (40/80)\u001b[K\rremote: Counting objects:  51% (41/80)\u001b[K\rremote: Counting objects:  52% (42/80)\u001b[K\rremote: Counting objects:  53% (43/80)\u001b[K\rremote: Counting objects:  55% (44/80)\u001b[K\rremote: Counting objects:  56% (45/80)\u001b[K\rremote: Counting objects:  57% (46/80)\u001b[K\rremote: Counting objects:  58% (47/80)\u001b[K\rremote: Counting objects:  60% (48/80)\u001b[K\rremote: Counting objects:  61% (49/80)\u001b[K\rremote: Counting objects:  62% (50/80)\u001b[K\rremote: Counting objects:  63% (51/80)\u001b[K\rremote: Counting objects:  65% (52/80)\u001b[K\rremote: Counting objects:  66% (53/80)\u001b[K\rremote: Counting objects:  67% (54/80)\u001b[K\rremote: Counting objects:  68% (55/80)\u001b[K\rremote: Counting objects:  70% (56/80)\u001b[K\rremote: Counting objects:  71% (57/80)\u001b[K\rremote: Counting objects:  72% (58/80)\u001b[K\rremote: Counting objects:  73% (59/80)\u001b[K\rremote: Counting objects:  75% (60/80)\u001b[K\rremote: Counting objects:  76% (61/80)\u001b[K\rremote: Counting objects:  77% (62/80)\u001b[K\rremote: Counting objects:  78% (63/80)\u001b[K\rremote: Counting objects:  80% (64/80)\u001b[K\rremote: Counting objects:  81% (65/80)\u001b[K\rremote: Counting objects:  82% (66/80)\u001b[K\rremote: Counting objects:  83% (67/80)\u001b[K\rremote: Counting objects:  85% (68/80)\u001b[K\rremote: Counting objects:  86% (69/80)\u001b[K\rremote: Counting objects:  87% (70/80)\u001b[K\rremote: Counting objects:  88% (71/80)\u001b[K\rremote: Counting objects:  90% (72/80)\u001b[K\rremote: Counting objects:  91% (73/80)\u001b[K\rremote: Counting objects:  92% (74/80)\u001b[K\rremote: Counting objects:  93% (75/80)\u001b[K\rremote: Counting objects:  95% (76/80)\u001b[K\rremote: Counting objects:  96% (77/80)\u001b[K\rremote: Counting objects:  97% (78/80)\u001b[K\rremote: Counting objects:  98% (79/80)\u001b[K\rremote: Counting objects: 100% (80/80)\u001b[K\rremote: Counting objects: 100% (80/80), done.\u001b[K\n","remote: Compressing objects:   1% (1/61)\u001b[K\rremote: Compressing objects:   3% (2/61)\u001b[K\rremote: Compressing objects:   4% (3/61)\u001b[K\rremote: Compressing objects:   6% (4/61)\u001b[K\rremote: Compressing objects:   8% (5/61)\u001b[K\rremote: Compressing objects:   9% (6/61)\u001b[K\rremote: Compressing objects:  11% (7/61)\u001b[K\rremote: Compressing objects:  13% (8/61)\u001b[K\rremote: Compressing objects:  14% (9/61)\u001b[K\rremote: Compressing objects:  16% (10/61)\u001b[K\rremote: Compressing objects:  18% (11/61)\u001b[K\rremote: Compressing objects:  19% (12/61)\u001b[K\rremote: Compressing objects:  21% (13/61)\u001b[K\rremote: Compressing objects:  22% (14/61)\u001b[K\rremote: Compressing objects:  24% (15/61)\u001b[K\rremote: Compressing objects:  26% (16/61)\u001b[K\rremote: Compressing objects:  27% (17/61)\u001b[K\rremote: Compressing objects:  29% (18/61)\u001b[K\rremote: Compressing objects:  31% (19/61)\u001b[K\rremote: Compressing objects:  32% (20/61)\u001b[K\rremote: Compressing objects:  34% (21/61)\u001b[K\rremote: Compressing objects:  36% (22/61)\u001b[K\rremote: Compressing objects:  37% (23/61)\u001b[K\rremote: Compressing objects:  39% (24/61)\u001b[K\rremote: Compressing objects:  40% (25/61)\u001b[K\rremote: Compressing objects:  42% (26/61)\u001b[K\rremote: Compressing objects:  44% (27/61)\u001b[K\rremote: Compressing objects:  45% (28/61)\u001b[K\rremote: Compressing objects:  47% (29/61)\u001b[K\rremote: Compressing objects:  49% (30/61)\u001b[K\rremote: Compressing objects:  50% (31/61)\u001b[K\rremote: Compressing objects:  52% (32/61)\u001b[K\rremote: Compressing objects:  54% (33/61)\u001b[K\rremote: Compressing objects:  55% (34/61)\u001b[K\rremote: Compressing objects:  57% (35/61)\u001b[K\rremote: Compressing objects:  59% (36/61)\u001b[K\rremote: Compressing objects:  60% (37/61)\u001b[K\rremote: Compressing objects:  62% (38/61)\u001b[K\rremote: Compressing objects:  63% (39/61)\u001b[K\rremote: Compressing objects:  65% (40/61)\u001b[K\rremote: Compressing objects:  67% (41/61)\u001b[K\rremote: Compressing objects:  68% (42/61)\u001b[K\rremote: Compressing objects:  70% (43/61)\u001b[K\rremote: Compressing objects:  72% (44/61)\u001b[K\rremote: Compressing objects:  73% (45/61)\u001b[K\rremote: Compressing objects:  75% (46/61)\u001b[K\rremote: Compressing objects:  77% (47/61)\u001b[K\rremote: Compressing objects:  78% (48/61)\u001b[K\rremote: Compressing objects:  80% (49/61)\u001b[K\rremote: Compressing objects:  81% (50/61)\u001b[K\rremote: Compressing objects:  83% (51/61)\u001b[K\rremote: Compressing objects:  85% (52/61)\u001b[K\rremote: Compressing objects:  86% (53/61)\u001b[K\rremote: Compressing objects:  88% (54/61)\u001b[K\rremote: Compressing objects:  90% (55/61)\u001b[K\rremote: Compressing objects:  91% (56/61)\u001b[K\rremote: Compressing objects:  93% (57/61)\u001b[K\rremote: Compressing objects:  95% (58/61)\u001b[K\rremote: Compressing objects:  96% (59/61)\u001b[K\rremote: Compressing objects:  98% (60/61)\u001b[K\rremote: Compressing objects: 100% (61/61)\u001b[K\rremote: Compressing objects: 100% (61/61), done.\u001b[K\n","Receiving objects:   0% (1/7335)   \rReceiving objects:   1% (74/7335)   \rReceiving objects:   2% (147/7335)   \rReceiving objects:   3% (221/7335)   \rReceiving objects:   4% (294/7335)   \rReceiving objects:   5% (367/7335)   \rReceiving objects:   6% (441/7335)   \rReceiving objects:   7% (514/7335)   \rReceiving objects:   8% (587/7335)   \rReceiving objects:   9% (661/7335)   \rReceiving objects:  10% (734/7335)   \rReceiving objects:  11% (807/7335)   \rReceiving objects:  12% (881/7335)   \rReceiving objects:  13% (954/7335)   \rReceiving objects:  14% (1027/7335)   \rReceiving objects:  15% (1101/7335)   \rReceiving objects:  16% (1174/7335)   \rReceiving objects:  17% (1247/7335)   \rReceiving objects:  18% (1321/7335)   \rReceiving objects:  19% (1394/7335)   \rReceiving objects:  20% (1467/7335)   \rReceiving objects:  21% (1541/7335)   \rReceiving objects:  22% (1614/7335)   \rReceiving objects:  23% (1688/7335)   \rReceiving objects:  24% (1761/7335)   \rReceiving objects:  25% (1834/7335)   \rReceiving objects:  26% (1908/7335)   \rReceiving objects:  27% (1981/7335)   \rReceiving objects:  28% (2054/7335)   \rReceiving objects:  29% (2128/7335)   \rReceiving objects:  30% (2201/7335)   \rReceiving objects:  31% (2274/7335)   \rReceiving objects:  32% (2348/7335)   \rReceiving objects:  33% (2421/7335)   \rReceiving objects:  34% (2494/7335)   \rReceiving objects:  35% (2568/7335)   \rReceiving objects:  36% (2641/7335)   \rReceiving objects:  37% (2714/7335)   \rReceiving objects:  38% (2788/7335)   \rReceiving objects:  39% (2861/7335)   \rReceiving objects:  40% (2934/7335)   \rReceiving objects:  41% (3008/7335)   \rReceiving objects:  42% (3081/7335)   \rReceiving objects:  43% (3155/7335)   \rReceiving objects:  44% (3228/7335)   \rReceiving objects:  45% (3301/7335)   \rReceiving objects:  46% (3375/7335)   \rReceiving objects:  47% (3448/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  48% (3521/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  49% (3595/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  50% (3668/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  51% (3741/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  52% (3815/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  53% (3888/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  54% (3961/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  55% (4035/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  56% (4108/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  57% (4181/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  58% (4255/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  59% (4328/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  60% (4401/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  61% (4475/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  62% (4548/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  63% (4622/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  64% (4695/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  65% (4768/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  66% (4842/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  67% (4915/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  68% (4988/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  69% (5062/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  70% (5135/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  71% (5208/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  72% (5282/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  73% (5355/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  74% (5428/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  75% (5502/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  76% (5575/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  77% (5648/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  78% (5722/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  79% (5795/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  80% (5868/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  81% (5942/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  82% (6015/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  83% (6089/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  84% (6162/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  85% (6235/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  86% (6309/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  87% (6382/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  88% (6455/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  89% (6529/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  90% (6602/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  91% (6675/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  92% (6749/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  93% (6822/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  94% (6895/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  95% (6969/7335), 9.68 MiB | 19.36 MiB/s   \rremote: Total 7335 (delta 40), reused 41 (delta 19), pack-reused 7255\u001b[K\n","Receiving objects:  96% (7042/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  97% (7115/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  98% (7189/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects:  99% (7262/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects: 100% (7335/7335), 9.68 MiB | 19.36 MiB/s   \rReceiving objects: 100% (7335/7335), 13.88 MiB | 21.28 MiB/s, done.\n","Resolving deltas:   0% (0/4941)   \rResolving deltas:   1% (75/4941)   \rResolving deltas:   2% (127/4941)   \rResolving deltas:   3% (150/4941)   \rResolving deltas:   4% (205/4941)   \rResolving deltas:   5% (253/4941)   \rResolving deltas:   6% (313/4941)   \rResolving deltas:   7% (353/4941)   \rResolving deltas:   8% (398/4941)   \rResolving deltas:   9% (449/4941)   \rResolving deltas:  10% (518/4941)   \rResolving deltas:  11% (588/4941)   \rResolving deltas:  12% (607/4941)   \rResolving deltas:  13% (654/4941)   \rResolving deltas:  14% (692/4941)   \rResolving deltas:  15% (743/4941)   \rResolving deltas:  16% (792/4941)   \rResolving deltas:  17% (840/4941)   \rResolving deltas:  18% (897/4941)   \rResolving deltas:  19% (942/4941)   \rResolving deltas:  20% (993/4941)   \rResolving deltas:  21% (1041/4941)   \rResolving deltas:  22% (1105/4941)   \rResolving deltas:  23% (1138/4941)   \rResolving deltas:  24% (1190/4941)   \rResolving deltas:  25% (1239/4941)   \rResolving deltas:  28% (1426/4941)   \rResolving deltas:  31% (1555/4941)   \rResolving deltas:  32% (1583/4941)   \rResolving deltas:  34% (1728/4941)   \rResolving deltas:  35% (1740/4941)   \rResolving deltas:  37% (1837/4941)   \rResolving deltas:  38% (1889/4941)   \rResolving deltas:  39% (1930/4941)   \rResolving deltas:  40% (1981/4941)   \rResolving deltas:  41% (2033/4941)   \rResolving deltas:  42% (2076/4941)   \rResolving deltas:  43% (2127/4941)   \rResolving deltas:  44% (2180/4941)   \rResolving deltas:  45% (2224/4941)   \rResolving deltas:  46% (2279/4941)   \rResolving deltas:  47% (2326/4941)   \rResolving deltas:  48% (2377/4941)   \rResolving deltas:  49% (2435/4941)   \rResolving deltas:  50% (2475/4941)   \rResolving deltas:  51% (2527/4941)   \rResolving deltas:  52% (2571/4941)   \rResolving deltas:  53% (2645/4941)   \rResolving deltas:  57% (2832/4941)   \rResolving deltas:  58% (2866/4941)   \rResolving deltas:  59% (2924/4941)   \rResolving deltas:  60% (2965/4941)   \rResolving deltas:  61% (3044/4941)   \rResolving deltas:  62% (3067/4941)   \rResolving deltas:  63% (3118/4941)   \rResolving deltas:  64% (3180/4941)   \rResolving deltas:  65% (3235/4941)   \rResolving deltas:  66% (3264/4941)   \rResolving deltas:  67% (3334/4941)   \rResolving deltas:  68% (3394/4941)   \rResolving deltas:  69% (3450/4941)   \rResolving deltas:  70% (3475/4941)   \rResolving deltas:  71% (3515/4941)   \rResolving deltas:  72% (3587/4941)   \rResolving deltas:  73% (3621/4941)   \rResolving deltas:  74% (3675/4941)   \rResolving deltas:  75% (3737/4941)   \rResolving deltas:  76% (3768/4941)   \rResolving deltas:  77% (3805/4941)   \rResolving deltas:  78% (3870/4941)   \rResolving deltas:  79% (3904/4941)   \rResolving deltas:  80% (3955/4941)   \rResolving deltas:  81% (4009/4941)   \rResolving deltas:  82% (4061/4941)   \rResolving deltas:  83% (4103/4941)   \rResolving deltas:  84% (4165/4941)   \rResolving deltas:  85% (4213/4941)   \rResolving deltas:  86% (4260/4941)   \rResolving deltas:  87% (4329/4941)   \rResolving deltas:  88% (4355/4941)   \rResolving deltas:  89% (4407/4941)   \rResolving deltas:  90% (4478/4941)   \rResolving deltas:  91% (4501/4941)   \rResolving deltas:  92% (4550/4941)   \rResolving deltas:  93% (4598/4941)   \rResolving deltas:  94% (4652/4941)   \rResolving deltas:  95% (4695/4941)   \rResolving deltas:  96% (4744/4941)   \rResolving deltas:  97% (4828/4941)   \rResolving deltas:  98% (4853/4941)   \rResolving deltas:  99% (4893/4941)   \rResolving deltas: 100% (4941/4941)   \rResolving deltas: 100% (4941/4941), done.\n","/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-512_dapv\n","Created temporary directory: /tmp/pip-req-tracker-j_oaxyyr\n","Created requirements tracker '/tmp/pip-req-tracker-j_oaxyyr'\n","Created temporary directory: /tmp/pip-install-m_dkfs3p\n","Processing ./apex\n","  Created temporary directory: /tmp/pip-req-build-90_uw4_j\n","  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-j_oaxyyr'\n","    Running setup.py (path:/tmp/pip-req-build-90_uw4_j/setup.py) egg_info for package from file:///content/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-90_uw4_j/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-90_uw4_j/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-90_uw4_j/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-90_uw4_j/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-90_uw4_j/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-90_uw4_j/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-90_uw4_j/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-90_uw4_j has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n","  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-j_oaxyyr'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-bde8oz1j\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-90_uw4_j/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-90_uw4_j/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-bde8oz1j/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    /tmp/pip-req-build-90_uw4_j/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda-10.1/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    creating /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-bde8oz1j/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-90_uw4_j\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-j_oaxyyr'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"665tCOfRpIIx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594407002325,"user_tz":-540,"elapsed":512369,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["import torch.random\n","import random\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqnYi7N2Kfjy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594407023339,"user_tz":-540,"elapsed":20999,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["zip_name = 'split-fold0.zip'\n","zip_path = '/content/drive/My Drive/Colab Notebooks/gwdsplit/' + zip_name\n","!cp \"{zip_path}\" .\n","!unzip -q '{zip_name}'\n","!rm '{zip_name}'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"978lsjD2vQAF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594407030140,"user_tz":-540,"elapsed":27772,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["yolov5_name = 'yolov5.zip'\n","yolov5_path = '/content/drive/My Drive/Colab Notebooks/' + yolov5_name\n","!cp '{yolov5_path}' .\n","!unzip -q '{yolov5_name}'\n","!rm '{yolov5_name}'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROkeoZzd_ljI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594407032486,"user_tz":-540,"elapsed":30105,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["weight_name = 'yolov5x_coco.pt'\n","weight_path = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + weight_name\n","!cp '{weight_path}' ."],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1l1XG4zejqR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594407032489,"user_tz":-540,"elapsed":30097,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["train_input = '/content/drive/My Drive/Colab Notebooks/yolov5/train.py'\n","data_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/wheat_colab.yaml'\n","cfg_input = '/content/drive/My Drive/Colab Notebooks/yolov5config/yolov5x.yaml'\n","weights_input = '/content/' + weight_name\n","name_input = 'x-b2-e50-fold0'"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-Sb84dpxl6U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594407032491,"user_tz":-540,"elapsed":30085,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"20284bab-2a51-4a1b-e3c9-20e8da520f65"},"source":["%cd /content/yolov5/"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/yolov5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lYOIA2YaeNgC","colab_type":"text"},"source":["# **train.py**"]},{"cell_type":"code","metadata":{"id":"9UCW99OCx4QA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1594407040184,"user_tz":-540,"elapsed":37763,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"034870bb-284b-4e2e-f1eb-3c9e4dd83cdd"},"source":["import argparse\n","\n","import torch.distributed as dist\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.utils.data\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import test  # import test.py to get mAP after each epoch\n","from models.yolo import Model\n","from utils import google_utils\n","from utils.datasets import *\n","from utils.utils import *\n","\n","mixed_precision = True\n","try:  # Mixed precision training https://github.com/NVIDIA/apex\n","    from apex import amp\n","except:\n","    print('Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex')\n","    mixed_precision = False  # not installed\n","\n","wdir = 'weights' + os.sep  # weights dir\n","os.makedirs(wdir, exist_ok=True)\n","last = wdir + 'last.pt'\n","best = wdir + 'best.pt'\n","results_file = 'results.txt'\n","\n","# Hyperparameters\n","hyp = {'lr0': 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","       'momentum': 0.937,  # SGD momentum\n","       'weight_decay': 5e-4,  # optimizer weight decay\n","       'giou': 0.05,  # giou loss gain\n","       'cls': 0.58,  # cls loss gain\n","       'cls_pw': 1.0,  # cls BCELoss positive_weight\n","       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)\n","       'obj_pw': 1.0,  # obj BCELoss positive_weight\n","       'iou_t': 0.20,  # iou training threshold\n","       'anchor_t': 4.0,  # anchor-multiple threshold\n","       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n","       'hsv_h': 0.014,  # image HSV-Hue augmentation (fraction)\n","       'hsv_s': 0.68,  # image HSV-Saturation augmentation (fraction)\n","       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n","       'degrees': 0.0,  # image rotation (+/- deg)\n","       'translate': 0.0,  # image translation (+/- fraction)\n","       'scale': 0.5,  # image scale (+/- gain)\n","       'shear': 0.0}  # image shear (+/- deg)\n","print(hyp)\n","\n","# Overwrite hyp with hyp*.txt (optional)\n","f = glob.glob('hyp*.txt')\n","if f:\n","    print('Using %s' % f[0])\n","    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n","        hyp[k] = v\n","\n","# Print focal loss if gamma > 0\n","if hyp['fl_gamma']:\n","    print('Using FocalLoss(gamma=%g)' % hyp['fl_gamma'])\n","\n","\n","def train(hyp):\n","    epochs = opt.epochs  # 300\n","    batch_size = opt.batch_size  # 64\n","    weights = opt.weights  # initial training weights\n","\n","    # Configure\n","    init_seeds(1)\n","    with open(opt.data) as f:\n","        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n","    train_path = data_dict['train']\n","    test_path = data_dict['val']\n","    nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes\n","\n","    # Remove previous results\n","    for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):\n","        os.remove(f)\n","\n","    # Create model\n","    model = Model(opt.cfg, nc=data_dict['nc']).to(device)\n","\n","    # Image sizes\n","    gs = int(max(model.stride))  # grid size (max stride)\n","    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n","\n","    # Optimizer\n","    nbs = 64  # nominal batch size\n","    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n","    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n","    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n","    for k, v in model.named_parameters():\n","        if v.requires_grad:\n","            if '.bias' in k:\n","                pg2.append(v)  # biases\n","            elif '.weight' in k and '.bn' not in k:\n","                pg1.append(v)  # apply weight decay\n","            else:\n","                pg0.append(v)  # all else\n","\n","    optimizer = optim.Adam(pg0, lr=hyp['lr0']) if opt.adam else \\\n","        optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n","    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n","    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n","    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n","    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n","    print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n","    del pg0, pg1, pg2\n","\n","    # Load Model\n","    google_utils.attempt_download(weights)\n","    start_epoch, best_fitness = 0, 0.0\n","    if weights.endswith('.pt'):  # pytorch format\n","        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","\n","        # load model\n","        try:\n","            ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()\n","                             if model.state_dict()[k].shape == v.shape}  # to FP32, filter\n","            model.load_state_dict(ckpt['model'], strict=False)\n","        except KeyError as e:\n","            s = \"%s is not compatible with %s. This may be due to model differences or %s may be out of date. \" \\\n","                \"Please delete or update %s and try again, or use --weights '' to train from scratch.\" \\\n","                % (opt.weights, opt.cfg, opt.weights, opt.weights)\n","            raise KeyError(s) from e\n","\n","        # load optimizer\n","        if ckpt['optimizer'] is not None:\n","            optimizer.load_state_dict(ckpt['optimizer'])\n","            best_fitness = ckpt['best_fitness']\n","\n","        # load results\n","        if ckpt.get('training_results') is not None:\n","            with open(results_file, 'w') as file:\n","                file.write(ckpt['training_results'])  # write results.txt\n","\n","        # epochs\n","        start_epoch = ckpt['epoch'] + 1\n","        if epochs < start_epoch:\n","            print('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\n","                  (opt.weights, ckpt['epoch'], epochs))\n","            epochs += ckpt['epoch']  # finetune additional epochs\n","\n","        del ckpt\n","\n","    # Mixed precision training https://github.com/NVIDIA/apex\n","    if mixed_precision:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n","\n","\n","    scheduler.last_epoch = start_epoch - 1  # do not move\n","    # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822\n","    # plot_lr_scheduler(optimizer, scheduler, epochs)\n","\n","    # Initialize distributed training\n","    if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n","        dist.init_process_group(backend='nccl',  # distributed backend\n","                                init_method='tcp://127.0.0.1:9999',  # init method\n","                                world_size=1,  # number of nodes\n","                                rank=0)  # node rank\n","        model = torch.nn.parallel.DistributedDataParallel(model)\n","        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","    # Trainloader\n","    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\n","                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect)\n","    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n","    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Correct your labels or your model.' % (mlc, nc, opt.cfg)\n","\n","    # Testloader\n","    testloader = create_dataloader(test_path, imgsz_test, batch_size, gs, opt,\n","                                   hyp=hyp, augment=False, cache=opt.cache_images, rect=True)[0]\n","\n","    # Model parameters\n","    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\n","    model.nc = nc  # attach number of classes to model\n","    model.hyp = hyp  # attach hyperparameters to model\n","    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n","    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n","    model.names = data_dict['names']\n","\n","    # Class frequency\n","    labels = np.concatenate(dataset.labels, 0)\n","    c = torch.tensor(labels[:, 0])  # classes\n","    # cf = torch.bincount(c.long(), minlength=nc) + 1.\n","    # model._initialize_biases(cf.to(device))\n","    if tb_writer:\n","        plot_labels(labels)\n","        tb_writer.add_histogram('classes', c, 0)\n","\n","    # Check anchors\n","    if not opt.noautoanchor:\n","        check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n","\n","    # Exponential moving average\n","    ema = torch_utils.ModelEMA(model)\n","\n","    # Start training\n","    t0 = time.time()\n","    nb = len(dataloader)  # number of batches\n","    n_burn = max(3 * nb, 1e3)  # burn-in iterations, max(3 epochs, 1k iterations)\n","    maps = np.zeros(nc)  # mAP per class\n","    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n","    print('Image sizes %g train, %g test' % (imgsz, imgsz_test))\n","    print('Using %g dataloader workers' % dataloader.num_workers)\n","    print('Starting training for %g epochs...' % epochs)\n","    # torch.autograd.set_detect_anomaly(True)\n","    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n","        model.train()\n","\n","        # Update image weights (optional)\n","        if dataset.image_weights:\n","            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n","            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n","            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n","\n","        # Update mosaic border\n","        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n","        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n","\n","        mloss = torch.zeros(4, device=device)  # mean losses\n","        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n","        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n","        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n","            ni = i + nb * epoch  # number integrated batches (since train start)\n","            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n","\n","            # Burn-in\n","            if ni <= n_burn:\n","                xi = [0, n_burn]  # x interp\n","                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n","                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n","                for j, x in enumerate(optimizer.param_groups):\n","                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n","                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n","                    if 'momentum' in x:\n","                        x['momentum'] = np.interp(ni, xi, [0.9, hyp['momentum']])\n","\n","            # Multi-scale\n","            if opt.multi_scale:\n","                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n","                sf = sz / max(imgs.shape[2:])  # scale factor\n","                if sf != 1:\n","                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n","                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n","\n","            # Forward\n","            pred = model(imgs)\n","\n","            # Loss\n","            loss, loss_items = compute_loss(pred, targets.to(device), model)\n","            if not torch.isfinite(loss):\n","                print('WARNING: non-finite loss, ending training ', loss_items)\n","                return results\n","\n","            # Backward\n","            if mixed_precision:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            # Optimize\n","            if ni % accumulate == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                ema.update(model)\n","\n","            # Print\n","            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n","            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n","            s = ('%10s' * 2 + '%10.4g' * 6) % (\n","                '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n","            pbar.set_description(s)\n","\n","            # Plot\n","            if ni < 3:\n","                f = 'train_batch%g.jpg' % ni  # filename\n","                result = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n","                if tb_writer and result is not None:\n","                    tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n","                    # tb_writer.add_graph(model, imgs)  # add model to tensorboard\n","\n","            # end batch ------------------------------------------------------------------------------------------------\n","\n","        # Scheduler\n","        scheduler.step()\n","\n","        # mAP\n","        ema.update_attr(model)\n","        final_epoch = epoch + 1 == epochs\n","        if not opt.notest or final_epoch:  # Calculate mAP\n","            results, maps, times = test.test(opt.data,\n","                                             batch_size=batch_size,\n","                                             imgsz=imgsz_test,\n","                                             save_json=final_epoch and opt.data.endswith(os.sep + 'coco.yaml'),\n","                                             model=ema.ema,\n","                                             single_cls=opt.single_cls,\n","                                             dataloader=testloader)\n","\n","        # Write\n","        with open(results_file, 'a') as f:\n","            f.write(s + '%10.4g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n","        if len(opt.name) and opt.bucket:\n","            os.system('gsutil cp results.txt gs://%s/results/results%s.txt' % (opt.bucket, opt.name))\n","\n","        # Tensorboard\n","        if tb_writer:\n","            tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n","                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/F1',\n","                    'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n","            for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n","                tb_writer.add_scalar(tag, x, epoch)\n","\n","        # Update best mAP\n","        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n","        if fi > best_fitness:\n","            best_fitness = fi\n","\n","        # Save model\n","        save = (not opt.nosave) or (final_epoch and not opt.evolve)\n","        if save:\n","            with open(results_file, 'r') as f:  # create checkpoint\n","                ckpt = {'epoch': epoch,\n","                        'best_fitness': best_fitness,\n","                        'training_results': f.read(),\n","                        'model': ema.ema,\n","                        'optimizer': None if final_epoch else optimizer.state_dict()}\n","\n","            # Save last, best and delete\n","            torch.save(ckpt, last)\n","            if (best_fitness == fi) and not final_epoch:\n","                torch.save(ckpt, best)\n","            del ckpt\n","\n","        # end epoch ----------------------------------------------------------------------------------------------------\n","    # end training\n","\n","    # Strip optimizers\n","    n = ('_' if len(opt.name) and not opt.name.isnumeric() else '') + opt.name\n","    fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n","    for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n","        if os.path.exists(f1):\n","            os.rename(f1, f2)  # rename\n","            ispt = f2.endswith('.pt')  # is *.pt\n","            strip_optimizer(f2) if ispt else None  # strip optimizer\n","            os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n","\n","    # Finish\n","    if not opt.evolve:\n","        plot_results()  # save as results.png\n","    print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n","    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\n","    torch.cuda.empty_cache()\n","    return results"],"execution_count":12,"outputs":[{"output_type":"stream","text":["{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9csFml3nfP7H","colab_type":"text"},"source":["# **Train Option**"]},{"cell_type":"code","metadata":{"id":"h0uu9IpAyJC1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594450215918,"user_tz":-540,"elapsed":43213492,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}},"outputId":"ce4cbf0b-b3e2-4401-8411-e4b053ad7e7a"},"source":["check_git_status()\n","class opt:\n","    epochs=50                #parser.add_argument('--epochs', type=int, default=300)\n","    batch_size=2            #parser.add_argument('--batch-size', type=int, default=16)\n","    cfg=cfg_input           #parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='*.cfg path')\n","    data=data_input         #parser.add_argument('--data', type=str, default='data/coco128.yaml', help='*.data path')\n","    img_size=[1024, 1024]   #parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')\n","    rect=False              #parser.add_argument('--rect', action='store_true', help='rectangular training')\n","    resume=False            #parser.add_argument('--resume', action='store_true', help='resume training from last.pt')\n","    nosave=False            #parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n","    notest=False            #parser.add_argument('--notest', action='store_true', help='only test final epoch')\n","    noautoanchor=False      #parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n","    evolve=False            #parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\n","    bucket=''               #parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n","    cache_images=False      #parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n","    weights=weights_input   #parser.add_argument('--weights', type=str, default='', help='initial weights path')\n","    name=name_input         #parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')\n","    device=''               #parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    adam=False              #parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n","    multi_scale=False       #parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%')\n","    single_cls=False        #parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\n","\n","#parser = argparse.ArgumentParser()\n","#opt = parser.parse_args()\n","\n","opt.weights = last if opt.resume and not opt.weights else opt.weights\n","opt.cfg = check_file(opt.cfg)  # check file\n","opt.data = check_file(opt.data)  # check file\n","print(opt)\n","opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\n","device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n","if device.type == 'cpu':\n","    mixed_precision = False\n","\n","# Train\n","if not opt.evolve:\n","    tb_writer = SummaryWriter(comment=opt.name)\n","    print('Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/')\n","    train(hyp)\n","\n","# Evolve hyperparameters (optional)\n","else:\n","    tb_writer = None\n","    opt.notest, opt.nosave = True, True  # only test/save final epoch\n","    if opt.bucket:\n","        os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n","\n","    for _ in range(10):  # generations to evolve\n","        if os.path.exists('evolve.txt'):  # if evolve.txt exists: select best hyps and mutate\n","            # Select parent(s)\n","            parent = 'single'  # parent selection method: 'single' or 'weighted'\n","            x = np.loadtxt('evolve.txt', ndmin=2)\n","            n = min(5, len(x))  # number of previous results to consider\n","            x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n","            w = fitness(x) - fitness(x).min()  # weights\n","            if parent == 'single' or len(x) == 1:\n","                # x = x[random.randint(0, n - 1)]  # random selection\n","                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n","            elif parent == 'weighted':\n","                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n","\n","            # Mutate\n","            mp, s = 0.9, 0.2  # mutation probability, sigma\n","            npr = np.random\n","            npr.seed(int(time.time()))\n","            g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n","            ng = len(g)\n","            v = np.ones(ng)\n","            while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n","                v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n","            for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n","                hyp[k] = x[i + 7] * v[i]  # mutate\n","\n","        # Clip to limits\n","        keys = ['lr0', 'iou_t', 'momentum', 'weight_decay', 'hsv_s', 'hsv_v', 'translate', 'scale', 'fl_gamma']\n","        limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n","        for k, v in zip(keys, limits):\n","            hyp[k] = np.clip(hyp[k], v[0], v[1])\n","\n","        # Train mutation\n","        results = train(hyp.copy())\n","\n","        # Write mutation results\n","        print_mutation(hyp, results, opt.bucket)\n","\n","        # Plot results\n","        # plot_evolution_results(hyp)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["<class '__main__.opt'>\n","Using CUDA Apex device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB)\n","\n","Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n","  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n","  2                -1  1    315680  models.common.BottleneckCSP             [160, 160, 4]                 \n","  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n","  4                -1  1   3311680  models.common.BottleneckCSP             [320, 320, 12]                \n","  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n","  6                -1  1  13228160  models.common.BottleneckCSP             [640, 640, 12]                \n","  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n","  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n","  9                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1   5435520  models.common.BottleneckCSP             [1280, 640, 4, False]         \n"," 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1   1360960  models.common.BottleneckCSP             [640, 320, 4, False]          \n"," 18                -1  1      5778  torch.nn.modules.conv.Conv2d            [320, 18, 1, 1]               \n"," 19                -2  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n"," 20          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 21                -1  1   5025920  models.common.BottleneckCSP             [640, 640, 4, False]          \n"," 22                -1  1     11538  torch.nn.modules.conv.Conv2d            [640, 18, 1, 1]               \n"," 23                -2  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n"," 24          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 25                -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n"," 26                -1  1     23058  torch.nn.modules.conv.Conv2d            [1280, 18, 1, 1]              \n"," 27      [-1, 22, 18]  1         0  models.yolo.Detect                      [1, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n","Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n","\n","Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n"],"name":"stdout"},{"output_type":"stream","text":["Reading image shapes: 100%|██████████| 2696/2696 [00:00<00:00, 11476.14it/s]\n","Caching labels /content/labels/train (2696 found, 0 missing, 0 empty, 0 duplicate, for 2696 images): 100%|██████████| 2696/2696 [00:00<00:00, 3883.77it/s]\n","Reading image shapes: 100%|██████████| 677/677 [00:00<00:00, 12486.40it/s]\n","Caching labels /content/labels/valid (421 found, 0 missing, 0 empty, 0 duplicate, for 677 images):  62%|██████▏   | 421/677 [00:00<00:00, 4203.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saving labels to /content/labels/train.npy for faster future loading\n"],"name":"stdout"},{"output_type":"stream","text":["\rCaching labels /content/labels/valid (677 found, 0 missing, 0 empty, 0 duplicate, for 677 images): 100%|██████████| 677/677 [00:00<00:00, 4096.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Analyzing anchors... Best Possible Recall (BPR) = 0.9991\n","Image sizes 1024 train, 1024 test\n","Using 2 dataloader workers\n","Starting training for 50 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      0/49     5.24G   0.07351    0.1863         0    0.2598        77      1024: 100%|██████████| 1348/1348 [12:56<00:00,  1.74it/s]\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [02:12<00:00,  2.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.309       0.946       0.825       0.308\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      1/49     5.28G   0.05607    0.1585         0    0.2146       194      1024: 100%|██████████| 1348/1348 [12:31<00:00,  1.79it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [02:05<00:00,  2.70it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.443       0.946         0.9       0.437\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      2/49     5.28G   0.05318    0.1553         0    0.2085       272      1024: 100%|██████████| 1348/1348 [12:28<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [02:07<00:00,  2.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.532       0.941       0.916         0.4\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      3/49     5.28G   0.04701      0.15         0     0.197        77      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:58<00:00,  2.85it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04         0.7       0.943       0.937       0.486\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      4/49     5.28G   0.04215     0.146         0    0.1882        54      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.675        0.95       0.939       0.495\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      5/49     5.28G   0.04085    0.1435         0    0.1843       306      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.717       0.947       0.941       0.505\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      6/49     5.28G   0.03936    0.1431         0    0.1824       165      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.731        0.95       0.945       0.518\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      7/49     5.28G   0.03851    0.1426         0    0.1811       129      1024: 100%|██████████| 1348/1348 [12:28<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:58<00:00,  2.87it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.728       0.951       0.945       0.526\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      8/49     5.28G   0.03786    0.1409         0    0.1787       116      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.74       0.951       0.948       0.527\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["      9/49     5.28G   0.03746     0.139         0    0.1764       169      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.736        0.95       0.946       0.521\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     10/49     5.28G   0.03716    0.1406         0    0.1778        77      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.723       0.951       0.945       0.519\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     11/49     5.28G   0.03639    0.1397         0     0.176       113      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.739       0.951       0.947       0.532\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     12/49     5.28G   0.03596    0.1371         0    0.1731       131      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.745        0.95       0.947       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     13/49     5.28G   0.03637     0.139         0    0.1754        97      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.752       0.949       0.947       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     14/49     5.28G   0.03554    0.1366         0    0.1722       141      1024: 100%|██████████| 1348/1348 [12:30<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:58<00:00,  2.85it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.756        0.95       0.948       0.536\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     15/49     5.28G   0.03552    0.1373         0    0.1728        84      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.743       0.952       0.947       0.534\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     16/49     5.28G   0.03495    0.1347         0    0.1696       188      1024: 100%|██████████| 1348/1348 [12:28<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.758        0.95       0.948       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     17/49     5.28G    0.0348    0.1365         0    0.1713       100      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.75       0.952        0.95       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     18/49     5.28G    0.0346    0.1328         0    0.1674        96      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.762       0.951       0.949       0.539\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     19/49     5.28G   0.03463    0.1346         0    0.1692       151      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.757        0.95       0.948       0.538\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     20/49     5.28G   0.03425    0.1348         0    0.1691       100      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:58<00:00,  2.87it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.77       0.949       0.948        0.54\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     21/49     5.28G   0.03416    0.1354         0    0.1696        94      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.761       0.953       0.951       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     22/49     5.28G   0.03393    0.1336         0    0.1675       162      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.757       0.953       0.949       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     23/49     5.28G   0.03381    0.1337         0    0.1675       134      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.763       0.951       0.948       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     24/49     5.28G   0.03375    0.1329         0    0.1667        32      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:56<00:00,  2.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.765       0.949       0.947       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     25/49     5.28G    0.0337    0.1327         0    0.1664        75      1024: 100%|██████████| 1348/1348 [12:26<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:55<00:00,  2.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.762       0.951       0.948       0.541\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     26/49     5.28G   0.03334    0.1327         0     0.166       119      1024: 100%|██████████| 1348/1348 [12:25<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.77        0.95       0.949       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     27/49     5.28G   0.03323    0.1319         0    0.1651       157      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.767        0.95       0.948       0.541\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     28/49     5.28G   0.03305    0.1319         0     0.165        56      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.785       0.949       0.949       0.544\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     29/49     5.28G   0.03315     0.131         0    0.1642        63      1024: 100%|██████████| 1348/1348 [12:27<00:00,  1.80it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:57<00:00,  2.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.766        0.95       0.949       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     30/49     5.28G   0.03268    0.1283         0     0.161       208      1024: 100%|██████████| 1348/1348 [12:22<00:00,  1.81it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:50<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.776        0.95       0.949       0.545\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     31/49     5.28G   0.03277    0.1298         0    0.1625        93      1024: 100%|██████████| 1348/1348 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:47<00:00,  3.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.778       0.949       0.948       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     32/49     5.28G   0.03274    0.1287         0    0.1615        91      1024: 100%|██████████| 1348/1348 [12:14<00:00,  1.84it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:47<00:00,  3.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.782       0.948       0.948       0.541\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     33/49     5.28G   0.03256    0.1288         0    0.1614       123      1024: 100%|██████████| 1348/1348 [12:14<00:00,  1.84it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:46<00:00,  3.19it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.773       0.948       0.947       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     34/49     5.28G   0.03248    0.1285         0     0.161       245      1024: 100%|██████████| 1348/1348 [12:14<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:46<00:00,  3.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.775        0.95       0.949       0.545\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     35/49     5.28G   0.03218    0.1259         0    0.1581       246      1024: 100%|██████████| 1348/1348 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:48<00:00,  3.13it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.777       0.949       0.948       0.544\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     36/49     5.28G    0.0322    0.1281         0    0.1603       210      1024: 100%|██████████| 1348/1348 [12:16<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:49<00:00,  3.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.785       0.948       0.948       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     37/49     5.28G   0.03215     0.128         0    0.1601        68      1024: 100%|██████████| 1348/1348 [12:18<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:50<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.78       0.947       0.948       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     38/49     5.28G   0.03205    0.1256         0    0.1576        85      1024: 100%|██████████| 1348/1348 [12:19<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:51<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.779       0.949       0.948       0.544\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     39/49     5.28G   0.03214    0.1263         0    0.1584        72      1024: 100%|██████████| 1348/1348 [12:19<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:50<00:00,  3.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.779       0.948       0.947       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     40/49     5.28G   0.03195    0.1255         0    0.1574       251      1024: 100%|██████████| 1348/1348 [12:19<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:51<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.792       0.948       0.947       0.541\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     41/49     5.28G   0.03193    0.1257         0    0.1577       278      1024: 100%|██████████| 1348/1348 [12:19<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:50<00:00,  3.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.787       0.946       0.947       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     42/49     5.28G   0.03183     0.127         0    0.1588        45      1024: 100%|██████████| 1348/1348 [12:21<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:52<00:00,  3.03it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.782       0.948       0.948       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     43/49     5.28G   0.03172    0.1251         0    0.1569       159      1024: 100%|██████████| 1348/1348 [12:20<00:00,  1.82it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:48<00:00,  3.12it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.793       0.947       0.947       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     44/49     5.28G   0.03176    0.1243         0     0.156       149      1024: 100%|██████████| 1348/1348 [12:14<00:00,  1.84it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:46<00:00,  3.19it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.786       0.949       0.948       0.544\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     45/49     5.28G   0.03151    0.1243         0    0.1558        80      1024: 100%|██████████| 1348/1348 [12:13<00:00,  1.84it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:46<00:00,  3.19it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.785       0.947       0.947       0.542\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     46/49     5.28G   0.03151    0.1239         0    0.1555       149      1024: 100%|██████████| 1348/1348 [12:13<00:00,  1.84it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:47<00:00,  3.16it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.786       0.947       0.947       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     47/49     5.28G   0.03158    0.1243         0    0.1559       132      1024: 100%|██████████| 1348/1348 [12:14<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:47<00:00,  3.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.79       0.946       0.946       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     48/49     5.28G   0.03152    0.1251         0    0.1566        73      1024: 100%|██████████| 1348/1348 [12:15<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:47<00:00,  3.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04       0.794       0.946       0.947       0.543\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n"],"name":"stdout"},{"output_type":"stream","text":["     49/49     5.28G    0.0316    0.1258         0    0.1574       104      1024: 100%|██████████| 1348/1348 [12:15<00:00,  1.83it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 339/339 [01:46<00:00,  3.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["                 all         677    2.95e+04        0.79       0.946       0.946       0.543\n","Optimizer stripped from weights/last_x-b2-e50-fold0.pt\n","Optimizer stripped from weights/best_x-b2-e50-fold0.pt\n","50 epochs completed in 11.983 hours.\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 10 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"HXFQcoG0kWE7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594450223865,"user_tz":-540,"elapsed":43221424,"user":{"displayName":"노수철","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJOIZCchrOEMg4QcU2w6H3DmKa3eEnmQMEQl1V=s64","userId":"08629297047198293656"}}},"source":["import shutil\n","new_weight = '/content/drive/My Drive/Colab Notebooks/yolov5weights/' + name_input\n","if os.path.exists(new_weight):\n","  shutil.rmtree(new_weight)\n","os.mkdir(new_weight)\n","\n","weight_last = '/content/yolov5/weights/last_' + name_input + '.pt'\n","weight_best = '/content/yolov5/weights/best_' + name_input + '.pt'\n","\n","!cp '{weight_last}' '{new_weight}'\n","!cp '{weight_best}' '{new_weight}'\n","!cp 'results.png' '{new_weight}'\n","!cp 'labels.png' '{new_weight}'\n","!cp 'test_batch0_gt.jpg' '{new_weight}'\n","!cp 'test_batch0_pred.jpg' '{new_weight}'"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtigXSkIl15i","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}